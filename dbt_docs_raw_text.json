[
  {
    "url": "https://docs.getdbt.com/reference/references-overview",
    "text": "About References\nThe References section contains reference materials for developing with dbt, which includes dbt and dbt Core .\nLearn how to add more configurations to your dbt project or adapter, use properties for extra ability, refer to dbt commands, use powerful Jinja functions to streamline your dbt project, and understand how to use dbt artifacts.\nProject configurations\nPlatform-specific configurations\nResource configurations and properties\ndbt Commands\ndbt Jinja functions\ndbt Artifacts\nSnowflake permissions artifacts\nDatabricks permissions artifacts\nRedshift permissions artifacts\nPostgres permissions artifacts"
  },
  {
    "url": "https://docs.getdbt.com/docs/introduction",
    "text": "What is dbt?\ndbt is the industry standard for data transformation. Learn how it can help you transform data and deploy analytics code following software engineering best practices like version control, modularity, portability, CI/CD, and documentation. dbt is a transformation workflow that helps you get more work done while producing higher quality results. You can use dbt to modularize and centralize your analytics code, while also providing your data team with guardrails typically found in software engineering workflows. Collaborate on data models, version them, and test and document your queries before safely deploying them to production, with monitoring and visibility. dbt compiles and runs your analytics code against your data platform, enabling you and your team to collaborate on a single source of truth for metrics, insights, and business definitions. This single source of truth, combined with the ability to define tests for your data, reduces errors when logic changes, and alerts you when issues arise. dbt works alongside your ingestion, visualization, and other data tools, so you can transform data directly in your cloud data platform. Read more about why we want to enable analysts to work more like software engineers in The dbt Viewpoint . Learn how other data practitioners around the world are using dbt by joining the dbt Community . dbt \u200b Use dbt to quickly and collaboratively transform data and deploy analytics code following software engineering best practices like version control, modularity, portability, CI/CD, and documentation. This means anyone on the data team comfortable with SQL can safely contribute to production-grade data pipelines. The dbt platform \u200b The dbt platform offers the fastest, most reliable, and scalable way to deploy dbt. Allowing data teams to optimize their data transformation by developing, testing, scheduling, and investigating data models using a single, fully managed service through a web-based user interface (UI). You can learn about plans and pricing on www.getdbt.com . Learn more about the dbt platform features and try one of the dbt quickstarts . The dbt Fusion engine \u200b The dbt Fusion Engine is the next-generation dbt engine, designed to deliver data teams a lightning-fast development experience, intelligent cost savings, and improved governance. For more information, refer to About the dbt Fusion Engine , supported features , and the installation instructions . dbt Core \u200b dbt Core is an open-source tool that enables data practitioners to transform data and is suitable for users who prefer to manually set up dbt and locally maintain it. You can install dbt Core through the command line. Learn more with the quickstart for dbt Core . dbt optimizes your workflow \u200b Avoid writing boilerplate DML and DDL by managing transactions, dropping tables, and managing schema changes. Write business logic with just a SQL select statement, or a Python DataFrame, that returns the dataset you need, and dbt takes care of materialization . Build up reusable, or modular, data models that can be referenced in subsequent work instead of starting at the raw data with every analysis. Dramatically reduce the time your queries take to run: Leverage metadata to find long-running models that you want to optimize and use incremental models which dbt makes easy to configure and use. Write DRY er code by leveraging macros , hooks , and package management . dbt provides more reliable analysis \u200b No longer copy and paste SQL, which can lead to errors when logic changes. Instead, build reusable data models that get pulled into subsequent models and analysis. Change a model once and that change will propagate to all its dependencies. Publish the canonical version of a particular data model, encapsulating all complex business logic. All analysis on top of this model will incorporate the same business logic without needing to reimplement it. Use mature source control processes like branching, pull requests, and code reviews. Write data quality tests quickly and easily on the underlying data. Many analytic errors are caused by edge cases in the data: testing helps analysts find and handle those edge cases. The power of dbt \u200b As a dbt user, your main focus will be on writing models (select queries) that reflect core business logic \u2013 there\u2019s no need to write boilerplate code to create tables and views, or to define the order of execution of your models. Instead, dbt handles turning these models into objects in your warehouse for you. Feature Description Handle boilerplate code to materialize queries as relations For each model you create, you can easily configure a materialization . A materialization represents a build strategy for your select query \u2013 the code behind a materialization is robust, boilerplate SQL that wraps your select query in a statement to create a new, or update an existing, relation. Read more about Materializations . Use a code compiler SQL files can contain Jinja, a lightweight templating language. Using Jinja in SQL provides a way to use control structures in your queries. For example, if statements and for loops. It also enables repeated SQL to be shared through macros . Read more about Macros . Determine the order of model execution Often, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots . Related docs \u200b Quickstarts for dbt Best practice guides What is a dbt Project? dbt run\ndbt is a transformation workflow that helps you get more work done while producing higher quality results. You can use dbt to modularize and centralize your analytics code, while also providing your data team with guardrails typically found in software engineering workflows. Collaborate on data models, version them, and test and document your queries before safely deploying them to production, with monitoring and visibility.\ndbt compiles and runs your analytics code against your data platform, enabling you and your team to collaborate on a single source of truth for metrics, insights, and business definitions. This single source of truth, combined with the ability to define tests for your data, reduces errors when logic changes, and alerts you when issues arise.\nRead more about why we want to enable analysts to work more like software engineers in The dbt Viewpoint . Learn how other data practitioners around the world are using dbt by joining the dbt Community .\ndbt \u200b\nUse dbt to quickly and collaboratively transform data and deploy analytics code following software engineering best practices like version control, modularity, portability, CI/CD, and documentation. This means anyone on the data team comfortable with SQL can safely contribute to production-grade data pipelines.\nThe dbt platform \u200b\nThe dbt platform offers the fastest, most reliable, and scalable way to deploy dbt. Allowing data teams to optimize their data transformation by developing, testing, scheduling, and investigating data models using a single, fully managed service through a web-based user interface (UI).\nYou can learn about plans and pricing on www.getdbt.com . Learn more about the dbt platform features and try one of the dbt quickstarts .\nThe dbt Fusion engine \u200b\nThe dbt Fusion Engine is the next-generation dbt engine, designed to deliver data teams a lightning-fast development experience, intelligent cost savings, and improved governance.\nFor more information, refer to About the dbt Fusion Engine , supported features , and the installation instructions .\ndbt Core \u200b\ndbt Core is an open-source tool that enables data practitioners to transform data and is suitable for users who prefer to manually set up dbt and locally maintain it. You can install dbt Core through the command line. Learn more with the quickstart for dbt Core .\ndbt optimizes your workflow \u200b\nAvoid writing boilerplate DML and DDL by managing transactions, dropping tables, and managing schema changes. Write business logic with just a SQL select statement, or a Python DataFrame, that returns the dataset you need, and dbt takes care of materialization .\nselect\nBuild up reusable, or modular, data models that can be referenced in subsequent work instead of starting at the raw data with every analysis.\nDramatically reduce the time your queries take to run: Leverage metadata to find long-running models that you want to optimize and use incremental models which dbt makes easy to configure and use.\nWrite DRY er code by leveraging macros , hooks , and package management .\ndbt provides more reliable analysis \u200b\nNo longer copy and paste SQL, which can lead to errors when logic changes. Instead, build reusable data models that get pulled into subsequent models and analysis. Change a model once and that change will propagate to all its dependencies.\nPublish the canonical version of a particular data model, encapsulating all complex business logic. All analysis on top of this model will incorporate the same business logic without needing to reimplement it.\nUse mature source control processes like branching, pull requests, and code reviews.\nWrite data quality tests quickly and easily on the underlying data. Many analytic errors are caused by edge cases in the data: testing helps analysts find and handle those edge cases.\nThe power of dbt \u200b\nAs a dbt user, your main focus will be on writing models (select queries) that reflect core business logic \u2013 there\u2019s no need to write boilerplate code to create tables and views, or to define the order of execution of your models. Instead, dbt handles turning these models into objects in your warehouse for you.\nFeature Description Handle boilerplate code to materialize queries as relations For each model you create, you can easily configure a materialization . A materialization represents a build strategy for your select query \u2013 the code behind a materialization is robust, boilerplate SQL that wraps your select query in a statement to create a new, or update an existing, relation. Read more about Materializations . Use a code compiler SQL files can contain Jinja, a lightweight templating language. Using Jinja in SQL provides a way to use control structures in your queries. For example, if statements and for loops. It also enables repeated SQL to be shared through macros . Read more about Macros . Determine the order of model execution Often, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nDescription Handle boilerplate code to materialize queries as relations For each model you create, you can easily configure a materialization . A materialization represents a build strategy for your select query \u2013 the code behind a materialization is robust, boilerplate SQL that wraps your select query in a statement to create a new, or update an existing, relation. Read more about Materializations . Use a code compiler SQL files can contain Jinja, a lightweight templating language. Using Jinja in SQL provides a way to use control structures in your queries. For example, if statements and for loops. It also enables repeated SQL to be shared through macros . Read more about Macros . Determine the order of model execution Often, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nHandle boilerplate code to materialize queries as relations For each model you create, you can easily configure a materialization . A materialization represents a build strategy for your select query \u2013 the code behind a materialization is robust, boilerplate SQL that wraps your select query in a statement to create a new, or update an existing, relation. Read more about Materializations . Use a code compiler SQL files can contain Jinja, a lightweight templating language. Using Jinja in SQL provides a way to use control structures in your queries. For example, if statements and for loops. It also enables repeated SQL to be shared through macros . Read more about Macros . Determine the order of model execution Often, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nFor each model you create, you can easily configure a materialization . A materialization represents a build strategy for your select query \u2013 the code behind a materialization is robust, boilerplate SQL that wraps your select query in a statement to create a new, or update an existing, relation. Read more about Materializations . Use a code compiler SQL files can contain Jinja, a lightweight templating language. Using Jinja in SQL provides a way to use control structures in your queries. For example, if statements and for loops. It also enables repeated SQL to be shared through macros . Read more about Macros . Determine the order of model execution Often, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nUse a code compiler SQL files can contain Jinja, a lightweight templating language. Using Jinja in SQL provides a way to use control structures in your queries. For example, if statements and for loops. It also enables repeated SQL to be shared through macros . Read more about Macros . Determine the order of model execution Often, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nSQL files can contain Jinja, a lightweight templating language. Using Jinja in SQL provides a way to use control structures in your queries. For example, if statements and for loops. It also enables repeated SQL to be shared through macros . Read more about Macros . Determine the order of model execution Often, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nif\nfor\nmacros\nDetermine the order of model execution Often, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nOften, when transforming data, it makes sense to do so in a staged approach. dbt provides a mechanism to implement transformations in stages through the ref function . Rather than selecting from existing tables and views in your warehouse, you can select from another model. Document your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nDocument your dbt project In the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nIn the dbt platform , you can auto-generate the documentation when your dbt project runs. dbt provides a mechanism to write, version-control, and share documentation for your dbt models. You can write descriptions (in plain text or markdown) for each model and field. Read more about the Documentation . Test your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nTest your models Tests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nTests provide a way to improve the integrity of the SQL in each model by making assertions about the results generated by a model. Build, test, and run your project with a button click or by using the Studio IDE command bar. Read more about writing tests for your models Testing Manage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nManage packages dbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\ndbt ships with a package manager, which allows analysts to use and publish both public and private repositories of dbt code which can then be referenced by others. Read more about Package Management . Load seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nLoad seed files Often in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nOften in analytics, raw values need to be mapped to a more readable value (for example, converting a country-code to a country name) or enriched with static or infrequently changing data. These data sources, known as seed files, can be saved as a CSV file in your project and loaded into your data warehouse using the seed command. Read more about Seeds . Snapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nproject\nseed\nSnapshot data Often, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nOften, records in a data source are mutable, in that they change over time. This can be difficult to handle in analytics if you want to reconstruct historic values. dbt provides a mechanism to snapshot raw data for a point in time, through use of snapshots .\nRelated docs \u200b\nQuickstarts for dbt\nBest practice guides\nWhat is a dbt Project?\ndbt run\ndbt The dbt platform The dbt Fusion engine dbt Core dbt optimizes your workflow dbt provides more reliable analysis The power of dbt Related docs\nThe dbt platform The dbt Fusion engine dbt Core\nThe dbt Fusion engine dbt Core\ndbt Core\ndbt optimizes your workflow dbt provides more reliable analysis The power of dbt Related docs\ndbt provides more reliable analysis The power of dbt Related docs\nThe power of dbt Related docs\nRelated docs"
  },
  {
    "url": "https://docs.getdbt.com/docs/dbt-cloud-apis/overview",
    "text": "APIs overview Enterprise Enterprise +\nAccounts on the Enterprise and Enterprise+ plans can query the dbt APIs.\ndbt provides the following APIs:\nThe dbt Administrative API can be used to administrate a dbt account. It can be called manually or with the dbt Terraform provider .\nThe dbt Discovery API can be used to fetch metadata related to the state and health of your dbt project.\nThe Semantic Layer APIs provides multiple API options which allow you to query your metrics defined in the Semantic Layer .\nIf you want to learn more about webhooks, refer to Webhooks for your jobs .\nHow to Access the APIs \u200b\ndbt supports two types of API Tokens: personal access tokens and service account tokens . Requests to the dbt APIs can be authorized using these tokens.\nHow to Access the APIs"
  },
  {
    "url": "https://docs.getdbt.com/best-practices",
    "text": "Best practice guides\nLearn how dbt Labs approaches building projects through our current viewpoints on structure, style, and setup.\n\ud83d\uddc3\ufe0f How we structure our dbt projects\n4 items\n\ud83d\uddc3\ufe0f How we style our dbt projects\n6 items\n\ud83d\uddc3\ufe0f How we build our metrics\n9 items\n\ud83d\uddc3\ufe0f How we build our dbt Mesh projects\n4 items\n\ud83d\uddc3\ufe0f Materialization best practices\n6 items\n\ud83d\udcc4\ufe0f Don't nest your curlies\nPoetry\n\ud83d\udcc4\ufe0f Clone incremental models as the first step of your CI job\nLearn how to define clone incremental models as the first step of your CI job.\n\ud83d\udcc4\ufe0f Writing custom generic data tests\nLearn how to define your own custom generic data tests.\n\ud83d\udcc4\ufe0f Best practices for workflows\nThis page contains the collective wisdom of experienced users of dbt on how to best use it in your analytics work. Observing these best practices will help your analytics team work as effectively as possible, while implementing the pro-tips will add some polish to your dbt projects!\n\ud83d\udcc4\ufe0f Best practices for dbt and Unity Catalog\nLearn how to configure your."
  },
  {
    "url": "https://docs.getdbt.com/docs/dbt-versions/dbt-cloud-release-notes",
    "text": "dbt release notes\ndbt release notes for recent and historical changes. Release notes fall into one of the following categories:\nNew: New products and features\nEnhancement: Performance improvements and feature enhancements\nFix: Bug and security fixes\nBehavior change: A change to existing behavior that doesn't fit into the other categories, such as feature deprecations or changes to default settings\nRelease notes are grouped by month for both multi-tenant and virtual private cloud (VPC) environments.\nJune 2025 \u200b\nNew : The Semantic Layer now supports Trino as a data platform. For more details, see Set up the Semantic Layer .\nNew : The dbt Fusion engine supports Databricks in beta.\nEnhancement : Group owners can now specify multiple email addresses for model-level notifications, enabling broader team alerts. Previously, only a single email address was supported. Check out the Configure groups section to learn more.\nNew : The Semantic Layer GraphQL API now has a List a saved query endpoint.\nList a saved query\nMay 2025 \u200b\n2025 dbt Launch Showcase \u200b\nThe following features are new or enhanced as part of our dbt Launch Showcase on May 28th, 2025:\nNew : The dbt Fusion engine is the brand new dbt engine re-written from the ground up to provide incredible speed, cost-savings tools, and comprehensive SQL language tools. The dbt Fusion engine is now available in beta for Snowflake users. Read more about Fusion . Understand what actions you need to take to get your projects Fusion-ready with the upgrade guide . Begin testing today with the quickstart guide . Know where we're headed with the dbt Fusion engine .\nRead more about Fusion .\nUnderstand what actions you need to take to get your projects Fusion-ready with the upgrade guide .\nBegin testing today with the quickstart guide .\nKnow where we're headed with the dbt Fusion engine .\nNew : The dbt VS Code extension is a powerful new tool that brings the speed and productivity of the dbt Fusion engine into your Visual Studio Code editor. This is a free download that will forever change your dbt development workflows. The dbt VS Code extension is now available as beta alongside Fusion . Check out the installation instructions and read more about the features to get started enhancing your dbt workflows today!\nNew : dbt Explorer is now dbt Catalog! Learn more about the change here . dbt's Catalog, global navigation provides a search experience that lets you find dbt resources across all your projects, as well as non-dbt resources in Snowflake. External metadata ingestion allows you to connect directly to your data warehouse, giving you visibility into tables, views, and other resources that aren't defined in dbt.\ndbt's Catalog, global navigation provides a search experience that lets you find dbt resources across all your projects, as well as non-dbt resources in Snowflake.\nExternal metadata ingestion allows you to connect directly to your data warehouse, giving you visibility into tables, views, and other resources that aren't defined in dbt.\nNew : dbt Canvas is now generally available (GA). Canvas is the intuitive visual editing tool that enables anyone to create dbt models with an easy to understand drag-and-drop interface. Read more about Canvas to begin empowering your teams to build more, faster!\nNew : State-aware orchestration is now in beta! Every time a new job in Fusion runs, state-aware orchestration automatically determines which models to build by detecting changes in code or data.\nNew : With Hybrid projects, your organization can adopt complementary dbt Core and dbt Cloud workflows and seamlessly integrate these workflows by automatically uploading dbt Core artifacts into dbt Cloud. Hybrid projects are now generally available.\nNew : System for Cross-Domain Identity Management (SCIM) through Okta is now GA.\nNew : dbt now acts as a Model Context Protocol (MCP) server, allowing seamless integration of AI tools with data warehouses through a standardized framework.\nNew : The quickstart guide for data analysts is now available. With dbt, data analysts can use built-in, AI-powered tools to build governed data models, explore how they\u2019re built, and run their own analysis.\nNew : You can view your usage metering and limiting in dbt Copilot on the billing page of your dbt Cloud account.\nNew : You can use Copilot to create a dbt-styleguide.md for dbt projects. The generated style guide template includes SQL style guidelines, model organization and naming conventions, model configurations and testing practices, and recommendations to enforce style rules. For more information, see Copilot style guide .\ndbt-styleguide.md\nNew : Copilot chat is an interactive interface within the Studio IDE where you can generate SQL code from natural language prompts and ask analytics-related questions. It integrates contextual understanding of your dbt project and assists in streamlining SQL development. For more information, see Copilot chat .\nNew : Leverage dbt Copilot to generate SQL queries in Insights from natural language prompts, enabling efficient data exploration within a context-aware interface.\nNew : The dbt platform Cost management dashboard is now available as a preview for Snowflake users on Enterprise and Enteprise Plus plans. Gain valuable insights into your warehouse spend with the comprehensive and interactive dashboard. Read more about it to get started with your cost savings analysis today!\nNew : Apache Iceberg and support for external Iceberg catalogs is now available! External catalogs are a vital part of dbt Mesh and a critical component for supporting large data sets across a variety of warehouses. Read more about Iceberg and external catalog support to begin enhancing your dbt Mesh configurations.\nNew : Preview support for Power BI integration with Semantic Layer is now available. This integration provides a live connection to the Semantic Layer through Power BI Desktop or Power BI Service.\nUpdate : Product renaming and other changes. For more information, refer to Updated names for dbt platform and features . Product names key Canvas (previously Visual Editor) Catalog (previously Explorer) Copilot Cost Management dbt Fusion engine Insights Mesh Orchestrator Studio IDE (previously Cloud IDE) Semantic Layer Pricing plan changes. For more information, refer to One dbt .\nCanvas (previously Visual Editor)\nCatalog (previously Explorer)\nCopilot\nCost Management\ndbt Fusion engine\nInsights\nMesh\nOrchestrator\nStudio IDE (previously Cloud IDE)\nSemantic Layer\nPricing plan changes. For more information, refer to One dbt .\nApril 2025 \u200b\nEnhancement : The Python SDK now supports lazy loading for large fields for dimensions , entities , and measures on Metric objects. For more information, see Lazy loading for large fields .\ndimensions\nentities\nmeasures\nMetric\nEnhancement : The Semantic Layer now supports SSH tunneling for Postgres or Redshift connections. Refer to Set up the Semantic Layer for more information.\nBehavior change : Users assigned the job admin permission set now have access to set up integrations for projects, including the Tableau integration to populate downstream exposures.\njob admin\nMarch 2025 \u200b\nBehavior change : As of March 31st, 2025, dbt Core versions 1.0, 1.1, and 1.2 have been deprecated from dbt . They are no longer available to select as versions for dbt projects. Workloads currently on these versions will be automatically upgraded to v1.3, which may cause new failures.\nEnhancement : Semantic Layer users on single-tenant configurations no longer need to contact their account representative to enable this feature. Setup is now self-service and available across all tenant configurations.\nNew : The Semantic Layer now supports Postgres as a data platform. For more details on how to set up the Semantic Layer for Postgres, see Set up the Semantic Layer .\nNew : New environment variable default DBT_CLOUD_INVOCATION_CONTEXT .\nDBT_CLOUD_INVOCATION_CONTEXT\nEnhancement : Users assigned read-only licenses are now able to view the Deploy section of their dbt account and click into the individual sections but not edit or otherwise make any changes.\ndbt Developer day \u200b\nThe following features are new or enhanced as part of our dbt Developer day on March 19th and 20th, 2025:\nNew : The --sample flag , now available for the run and build commands, helps reduce build times and warehouse costs by running dbt in sample mode. It generates filtered refs and sources using time-based sampling, allowing developers to validate outputs without building entire models.\n--sample\nrun\nbuild\nNew : Copilot , an AI-powered assistant, is now generally available in the Studio IDE for all dbt Enterprise accounts. Check out Copilot for more information.\nAlso available this month \u200b\nNew : Bringing your own Azure OpenAI key for Copilot is now generally available. Your organization can configure Copilot to use your own Azure OpenAI keys, giving you more control over data governance and billing.\nNew : The Semantic Layer supports Power BI as a partner integration , available in private beta. To join the private beta, please reach out to your account representative. Check out the Power BI integration for more information.\nNew : dbt release tracks are Generally Available. Depending on their plan, customers may select among the Latest, Compatible, or Extended tracks to manage the update cadences for development and deployment environments.\nNew: The dbt -native integration with Azure DevOps now supports Entra ID service principals . Unlike a services user, which represents a real user object in Entra ID, the service principal is a secure identity associated with your dbt app to access resources in Azure unattended. Please migrate your service user to a service principal for Azure DevOps  as soon as possible.\nFebruary 2025 \u200b\nEnhancement : The Python SDK added a new timeout parameter to Semantic Layer client and to underlying GraphQL clients to specify timeouts. Set a timeout number or use the total_timeout parameter in the global TimeoutOptions to control connect, execute and close timeouts granularly. ExponentialBackoff.timeout_ms is now deprecated.\ntotal_timeout\nTimeoutOptions\nExponentialBackoff.timeout_ms\nNew : The Azure DevOps integration for Git now supports Entra service principal apps on dbt Enterprise accounts. Microsoft is enforcing MFA across user accounts, including service users, which will impact existing app integrations. This is a phased rollout, and dbt Labs recommends migrating to a service principal on existing integrations once the option becomes available in your account.\nNew : Added the dbt invocation command to the dbt CLI . This command allows you to view and manage active invocations, which are long-running sessions in the dbt CLI. For more information, see dbt invocation .\ndbt invocation\nNew : Users can now switch themes directly from the user menu, available in Preview . We have added support for Light mode (default), Dark mode , and automatic theme switching based on system preferences. The selected theme is stored in the user profile and will follow users across all devices. Dark mode is currently available on the Developer plan and will be available for all plans in the future. We\u2019ll be rolling it out gradually, so stay tuned for updates. For more information, refer to Change your dbt theme .\nDark mode is currently available on the Developer plan and will be available for all plans in the future. We\u2019ll be rolling it out gradually, so stay tuned for updates. For more information, refer to Change your dbt theme .\nFix : Semantic Layer errors in the Cloud Studio IDE are now displayed with proper formatting, fixing an issue where newlines appeared broken or difficult to read. This fix ensures error messages are more user-friendly and easier to parse.\nFix : Fixed an issue where saved queries with no exports would fail with an UnboundLocalError . Previously, attempting to process a saved query without any exports would cause an error due to an undefined relation variable. Exports are optional, and this fix ensures saved queries without exports don't fail.\nUnboundLocalError\nNew : You can now query metric alias in Semantic Layer GraphQL and JDBC APIs. For the JDBC API, refer to Query metric alias for more information. For the GraphQL API, refer to Query metric alias for more information.\nFor the JDBC API, refer to Query metric alias for more information.\nFor the GraphQL API, refer to Query metric alias for more information.\nEnhancement : Added support to automatically refresh access tokens when Snowflake's SSO connection expires. Previously, users would get the following error: Connection is not available, request timed out after 30000ms and would have to wait 10 minutes to try again.\nConnection is not available, request timed out after 30000ms\nEnhancement : The dbt_version format in dbt Cloud now better aligns with semantic versioning rules . Leading zeroes have been removed from the month and day ( YYYY.M.D+<suffix> ). For example: New format: 2024.10.8+996c6a8 Previous format: 2024.10.08+996c6a8\ndbt_version\nYYYY.M.D+<suffix>\nNew format: 2024.10.8+996c6a8\n2024.10.8+996c6a8\nPrevious format: 2024.10.08+996c6a8\n2024.10.08+996c6a8\nJune 2025 May 2025 2025 dbt Launch Showcase April 2025 March 2025 February 2025\nMay 2025 2025 dbt Launch Showcase April 2025 March 2025 February 2025\n2025 dbt Launch Showcase\nApril 2025 March 2025 February 2025\nMarch 2025 February 2025\nFebruary 2025"
  },
  {
    "url": "https://docs.getdbt.com/blog",
    "text": "The Components of the dbt Fusion engine and how they fit together\nToday, we announced the dbt Fusion engine .\nFusion isn't just one thing \u2014 it's a set of interconnected components working together to power the next generation of analytics engineering.\nThis post maps out each piece of the Fusion architecture, explains how they fit together, and clarifies what's available to you whether you're compiling from source, using our pre-built binaries, or developing within a dbt Fusion powered product experience.\nFrom the Rust engine to the VS Code extension, through to new Arrow-based adapters and Apache-licensed foundational technologies, we'll break down exactly what each component does, how each component is licensed (for why, see Tristan's accompanying post ), and how you can start using it and get involved today.\nPath to GA: How the dbt Fusion engine rolls out from beta to production\nToday, we announced that the dbt Fusion engine is available in beta .\nIf Fusion works with your project today, great! You're in for a treat \ud83d\ude04\nIf it's your first day using dbt, welcome! You should start on Fusion \u2014 you're in for a treat too.\nToday is Launch Day \u2014\u00a0the first day of a new era: the Age of Fusion. We expect many teams with existing projects will encounter at least one issue that will prevent them from adopting the dbt Fusion engine in production environments. That's ok!\nWe're moving quickly to unblock more teams, and we are committing that by the time Fusion reaches General Availability:\nWe will support Snowflake, Databricks, BigQuery, Redshift\u00a0\u2014\u00a0and likely also Athena, Postgres, Spark, and Trino \u2014 with the new Fusion Adapter pattern .\nWe will have coverage for (basically) all dbt Core functionality. Some things are impractical to replicate outside of Python, or so seldom-used that we'll be more reactive than proactive. On the other hand, many existing dbt Core behaviours will be improved by the unique capabilities of the dbt Fusion engine, such as speed and SQL comprehension. You'll see us talk about this in relevant GitHub issues, many of which we've linked below.\nThe source-available dbt-fusion repository will contain more total functionality than what is available in dbt Core today. ( Read more about this here .)\ndbt-fusion\nThe developer experience will be even speedier and more intuitive.\nThese statements aren't true yet \u2014\u00a0but you can see where we're headed. That's what betas are for, that's the journey we're going on together, and that's why we want to have you all involved.\nMeet the\u202fdbt\u202fFusion Engine: the new Rust-based, industrial-grade engine for dbt\nTL;DR: What You Need to Know \u200b\ndbt\u2019s familiar authoring layer remains unchanged, but the execution engine beneath it is completely new.\nThe new engine is called the dbt Fusion engine \u2014 rewritten from the ground up in Rust based on technology from SDF .  The dbt Fusion engine is substantially faster than dbt Core and has built in SQL comprehension technology to power the next generation of analytics engineering workflows.\nThe dbt Fusion engine is currently in beta. You can try it today if you use Snowflake \u2014 with additional adapters coming starting in early June. Review our path to general availability (GA) and try the quickstart .\nYou do not need to be a dbt Labs customer to use Fusion - dbt Core users can adopt the dbt Fusion engine today for free in your local environment.\nYou can use Fusion with the new dbt VS Code extension , directly via the CLI , or via dbt Studio .\nThis is the beginning of a new era for analytics engineering. For a glimpse into what the Fusion engine is going to enable over the next 1 to 2 years, read this post .\nAI Evaluation in dbt\nThe AI revolution is here\u2014but are we ready? Across the world, the excitement around AI is undeniable.  Discussions on large language models, agentic workflows, and how AI is set to transform every industry abound, yet real-world use cases of AI in production remain few and far between.\nA common issue blocking people from moving AI use cases to production is an ability to evaluate the validity of AI responses in a systematic and well governed way.\nMoving AI workflows from prototype to production requires rigorous evaluation, and most organizations do not have a framework to ensure AI outputs remain high-quality, trustworthy, and actionable.\nScaling Data Pipelines for a Growth-Stage Fintech with Incremental Models\nIntroduction \u200b\nBuilding scalable data pipelines in a fast-growing fintech can feel like fixing a bike while riding it. You must keep insights flowing even as data volumes explode. At Kuda (a Nigerian neo-bank), we faced this problem as our user base surged. Traditional batch ETL (rebuilding entire tables each run) started to buckle; pipelines took hours, and costs ballooned. We needed to keep data fresh without reprocessing everything. Our solution was to leverage dbt\u2019s incremental models , which process only new or changed records. This dramatically cut run times and curbed our BigQuery costs, letting us scale efficiently.\nIntroducing the dbt MCP Server \u2013 Bringing Structured Data to AI Workflows and Agents\ndbt is the standard for creating governed, trustworthy datasets on top of your structured data. MCP is showing increasing promise as the standard for providing context to LLMs to allow them to function at a high level in real world, operational scenarios.\nToday, we are open sourcing an experimental version of the dbt MCP server . We expect that over the coming years, structured data is going to become heavily integrated into AI workflows and that dbt will play a key role in building and provisioning this data.\nEstablishing dbt Cloud: Securing your account through SSO & RBAC\nAs a dbt Cloud admin, you\u2019ve just upgraded to dbt Cloud on the Enterprise plan - congrats ! dbt Cloud has a lot to offer such as CI/CD , Orchestration , dbt Explorer , dbt Semantic Layer , dbt Mesh , Visual Editor , dbt Copilot , and so much more. But where should you begin?\nWe strongly recommend as you start adopting dbt Cloud functionality to make it a priority to set up Single-Sign On (SSO) and Role-Based Access Control (RBAC). This foundational step enables your organization to keep your data pipelines secure, onboard users into dbt Cloud with ease, and optimize cost savings for the long term.\nGetting Started with git Branching Strategies and dbt\nHi! We\u2019re Christine and Carol, Resident Architects at dbt Labs. Our day-to-day\nwork is all about helping teams reach their technical and business-driven goals.\nCollaborating with a broad spectrum of customers ranging from scrappy startups\nto massive enterprises, we\u2019ve gained valuable experience guiding teams to\nimplement architecture which addresses their major pain points.\nThe information we\u2019re about to share isn't just from our experiences - we\nfrequently collaborate with other experts like Taylor Dunlap and Steve Dowling\nwho have greatly contributed to the amalgamation of this guidance. Their work\nlies in being the critical bridge for teams between\nimplementation and business outcomes, ultimately leading teams to align on a\ncomprehensive technical vision through identification of problems and solutions.\nWhy are we here? We help teams with dbt architecture, which encompasses the tools, processes and\nconfigurations used to start developing and deploying with dbt. There\u2019s a lot of\ndecision making that happens behind the scenes to standardize on these pieces -\nmuch of which is informed by understanding what we want the development workflow\nto look like. The focus on having the perfect workflow often gets teams\nstuck in heaps of planning and endless conversations, which slows down or even\nstops momentum on development. If you feel this, we\u2019re hoping our guidance will\ngive you a great sense of comfort in taking steps to unblock development - even\nwhen you don\u2019t have everything figured out yet!\nParser, Better, Faster, Stronger: A peek at the new dbt engine\nRemember how dbt felt when you had a small project? You pressed enter and stuff just happened immediately? We're bringing that back.\nAfter a series of deep dives into the guts of SQL comprehension , let's talk about speed a little bit. Specifically, I want to talk about one of the most annoying slowdowns as your project grows: project parsing.\nWhen you're waiting a few seconds or a few minutes for things to start happening after you invoke dbt, it's because parsing isn't finished yet. But Lukas' SDF demo at last month's webinar didn't have a big wait, so why not?\nThe key technologies behind SQL Comprehension\nYou ever wonder what\u2019s really going on in your database when you fire off a (perfect, efficient, full-of-insight) SQL query to your database?\nOK, probably not \ud83d\ude05. Your personal tastes aside, we\u2019ve been talking a lot about SQL Comprehension tools at dbt Labs in the wake of our acquisition of SDF Labs, and think that the community would benefit if we included them in the conversation too! We recently published a blog that talked about the different levels of SQL Comprehension tools . If you read that, you may have encountered a few new terms you weren\u2019t super familiar with.\nIn this post, we\u2019ll talk about the technologies that underpin SQL Comprehension tools in more detail. Hopefully, you come away with a deeper understanding of and appreciation for the hard work that your computer does to turn your SQL queries into actionable business insights!\nThe Three Levels of SQL Comprehension: What they are and why you need to know about them\nEver since dbt Labs acquired SDF Labs last week , I've been head-down diving into their technology and making sense of it all. The main thing I knew going in was \"SDF understands SQL\". It's a nice pithy quote, but the specifics are fascinating.\nFor the next era of Analytics Engineering to be as transformative as the last, dbt needs to move beyond being a string preprocessor and into fully comprehending SQL. For the first time, SDF provides the technology necessary to make this possible. Today we're going to dig into what SQL comprehension actually means, since it's so critical to what comes next.\nWhy I wish I had a control plane for my renovation\nWhen my wife and I renovated our home, we chose to take on the role of owner-builder. It was a bold (and mostly naive) decision, but we wanted control over every aspect of the project. What we didn\u2019t realize was just how complex and exhausting managing so many moving parts would be.\nWe had to coordinate multiple elements:\nThe architects , who designed the layout, interior, and exterior.\nThe architectural plans , which outlined what the house should look like.\nThe builders , who executed those plans.\nThe inspectors , councils , and energy raters , who checked whether everything met the required standards.\nTest smarter not harder: Where should tests go in your pipeline?\n\ud83d\udc4b\u00a0Greetings, dbt\u2019ers! It\u2019s Faith & Jerrie, back again to offer tactical advice on where to put tests in your pipeline.\nIn our first post on refining testing best practices, we developed a prioritized list of data quality concerns. We also documented first steps for debugging each concern. This post will guide you on where specific tests should go in your data pipeline.\nNote that we are constructing this guidance based on how we structure data at dbt Labs. You may use a different modeling approach\u2014that\u2019s okay! Translate our guidance to your data\u2019s shape, and let us know in the comments section what modifications you made.\nFirst, here\u2019s our opinions on where specific tests should go:\nSource tests should be fixable data quality concerns. See the callout box below for what we mean by \u201cfixable\u201d.\nStaging tests should be business-focused anomalies specific to individual tables, such as accepted ranges or ensuring sequential values. In addition to these tests, your staging layer should clean up any nulls, duplicates, or outliers that you can\u2019t fix in your source system. You generally don\u2019t need to test your cleanup efforts.\nIntermediate and marts layer tests should be business-focused anomalies resulting specifically from joins or calculations.  You also may consider adding additional primary key and not null tests on columns where it\u2019s especially important to protect the grain.\nTest smarter not harder: add the right tests to your dbt project\nThe Analytics Development Lifecycle (ADLC) is a workflow for improving data maturity and velocity. Testing is a key phase here. Many dbt developers tend to focus on primary keys and source freshness. We think there is a more holistic and in-depth path to tread. Testing is a key piece of the ADLC, and it should drive data quality.\nIn this blog, we\u2019ll walk through a plan to define data quality. This will look like:\nidentifying data hygiene issues\nidentifying business-focused anomaly issues\nidentifying stats-focused anomaly issues\nOnce we have defined data quality, we\u2019ll move on to prioritize those concerns. We will:\nthink through each concern in terms of the breadth of impact\ndecide if each concern should be at error or warning severity\nSnowflake feature store and dbt: A bridge between data pipelines and ML\nFlying home into Detroit this past week working on this blog post on a plane and saw for the first time, the newly connected deck of the Gordie Howe International bridge spanning the Detroit River and connecting the U.S. and Canada. The image stuck out because, in one sense, a feature store is a bridge between the clean, consistent datasets and the machine learning models that rely upon this data. But, more interesting than the bridge itself is the massive process of coordination needed to build it. This construction effort \u2014 I think \u2014 can teach us more about processes and the need for feature stores in machine learning (ML).\nThink of the manufacturing materials needed as our data and the building of the bridge as the building of our ML models. There are thousands of engineers and construction workers taking materials from all over the world, pulling only the specific pieces needed for each part of the project. However, to make this project truly work at this scale, we need the warehousing and logistics to ensure that each load of concrete rebar and steel meets the standards for quality and safety needed and is available to the right people at the right time \u2014 as even a single fault can have catastrophic consequences or cause serious delays in project success. This warehouse and the associated logistics play the role of the feature store, ensuring that data is delivered consistently where and when it is needed to train and run ML models.\nIceberg Is An Implementation Detail\nIf you haven\u2019t paid attention to the data industry news cycle, you might have missed the recent excitement centered around an open table format called Apache Iceberg\u2122. It\u2019s one of many open table formats like Delta Lake, Hudi, and Hive. These formats are changing the way data is stored and metadata accessed. They are groundbreaking in many ways.\nBut I have to be honest: I don\u2019t care . But not for the reasons you think.\nHow Hybrid Mesh unlocks dbt collaboration at scale\nOne of the most important things that dbt does is unlock the ability for teams to collaborate on creating and disseminating organizational knowledge.\nIn the past, this primarily looked like a team working in one dbt Project to create a set of transformed objects in their data platform.\nAs dbt was adopted by larger organizations and began to drive workloads at a global scale, it became clear that we needed mechanisms to allow teams to operate independently from each other, creating and sharing data models across teams \u2014 dbt Mesh .\nHow to build a Semantic Layer in pieces: step-by-step for busy analytics engineers\nThe dbt Semantic Layer is founded on the idea that data transformation should be both flexible , allowing for on-the-fly aggregations grouped and filtered by definable dimensions and version-controlled and tested . Like any other codebase, you should have confidence that your transformations express your organization\u2019s business logic correctly. Historically, you had to choose between these options, but the dbt Semantic Layer brings them together. This has required new paradigms for how you express your transformations though.\nPutting Your DAG on the internet\nNew in dbt: allow Snowflake Python models to access the internet \u200b\nWith dbt 1.8, dbt released support for Snowflake\u2019s external access integrations further enabling the use of dbt + AI to enrich your data. This allows querying of external APIs within dbt Python models, a functionality that was required for dbt Cloud customer, EQT AB . Learn about why they needed it and how they helped build the feature and get it shipped!\nUp and Running with Azure Synapse on dbt Cloud\nAt dbt Labs, we\u2019ve always believed in meeting analytics engineers where they are. That\u2019s why we\u2019re so excited to announce that today, analytics engineers within the Microsoft Ecosystem can use dbt Cloud with not only Microsoft Fabric but also Azure Synapse Analytics Dedicated SQL Pools (ASADSP).\nSince the early days of dbt, folks have been interested having MSFT data platforms. Huge shoutout to Mikael Ene and Jacob Mastel for their efforts back in 2019 on the original SQL Server adapters ( dbt-sqlserver and dbt-mssql , respectively)\nThe journey for the Azure Synapse dbt adapter, dbt-synapse, is closely tied to my journey with dbt. I was the one who forked dbt-sqlserver into dbt-synapse in April of 2020. I had first learned of dbt only a month earlier and knew immediately that my team needed the tool. With a great deal of assistance from Jeremy and experts at Microsoft, my team and I got it off the ground and started using it. When I left my team at Avanade in early 2022 to join dbt Labs, I joked that I wasn\u2019t actually leaving the team; I was just temporarily embedding at dbt Labs to expedite dbt Labs getting into Cloud. Two years later, I can tell my team that the mission has been accomplished! Kudos to all the folks who have contributed to the TSQL adapters either directly in GitHub or in the community Slack channels. The integration would not exist if not for you!"
  },
  {
    "url": "https://docs.getdbt.com/community/join",
    "text": "Join the Community\nWant to learn how organizations around the world are tackling the biggest challenges in data while making new friends from the best analytics teams? Join the dbt Community \u2014 data practitioners\u2019 favorite place to learn new skills, keep on top of industry trends, and forge connections. Join us on Slack Follow the pulse of the dbt Community! Chat with other practitioners in your city, country or worldwide about data work, tech stacks, or simply share a killer meme. Community Forum Have a question about how to do something in dbt? Hop into the Community Forum and work with others to create long lived community knowledge. How to contribute Want to get involved? This is the place! Learn how to contribute to our public repositories, write for the blog, speak at a meetup and more. Code of Conduct We are committed to creating a space where everyone can feel welcome and safe. Our Code of Conduct reflects the agreement that all Community members make to uphold these ideals. Upcoming events Whether it's in-person Meetups in your local area, Coalesce \u2013 the annual Analytics Engineering Conference \u2013 or online Office Hours there are options for everyone. Watch past events Get a taste for the energy of our live events, get inspired, or prepare for an upcoming event by watching recordings from our YouTube archives.\nJoin us on Slack\nCommunity Forum\nHow to contribute\nCode of Conduct\nUpcoming events\nWatch past events"
  },
  {
    "url": "https://docs.getdbt.com/community/contribute",
    "text": "Become a contributor\nWant to get involved? Start here \u200b\nThe dbt Community predates dbt Labs as an organization and harkens back to the days when a scrappy analytics consultancy of a few pissed off data analysts started hacking together an open source project around which gathered a community that would change how the world uses data. The dbt Community exists to allow analytics practitioners to share their knowledge, help others and collectively to drive forward the discipline of analytics engineering. This is something that can\u2019t be done by any one individual or any one organization - to create a new discipline is necessarily a community effort. The only reason that dbt has become as widespread as it has is because people like you choose to get involved and share your knowledge. Contributing to the community can also be a great way to learn new skills, build up a public portfolio and make friends with other practitioners. There are opportunities here for everyone to get involved, whether you are just beginning your analytics engineering journey or you are a seasoned data professional. Contributing isn\u2019t about knowing all of the answers, it\u2019s about learning things together. Below you\u2019ll find a sampling of the ways to get involved. There are a lot of options but these are ultimately just variations on the theme of sharing knowledge with the broader community. Writing contributions Learn how to share and grow the collective knowledge of the dbt Community through blogs, guides, and documentation. Coding contributions The dbt Community supports a wide variety of open source and source-available projects, and this software is at the heart of everything we do. Learn how to get involved with projects in the dbt ecosystem. Online community building Getting involved in the dbt Community  Forum or Slack is one of the best entry points for contributing. Share your knowledge and learn from others. Realtime event participation Want to speak at a Meetup or conference? Learn how to get involved and check out best practices for crafting a talk that everyone will remember.\nThe dbt Community exists to allow analytics practitioners to share their knowledge, help others and collectively to drive forward the discipline of analytics engineering. This is something that can\u2019t be done by any one individual or any one organization - to create a new discipline is necessarily a community effort. The only reason that dbt has become as widespread as it has is because people like you choose to get involved and share your knowledge. Contributing to the community can also be a great way to learn new skills, build up a public portfolio and make friends with other practitioners. There are opportunities here for everyone to get involved, whether you are just beginning your analytics engineering journey or you are a seasoned data professional. Contributing isn\u2019t about knowing all of the answers, it\u2019s about learning things together. Below you\u2019ll find a sampling of the ways to get involved. There are a lot of options but these are ultimately just variations on the theme of sharing knowledge with the broader community. Writing contributions Learn how to share and grow the collective knowledge of the dbt Community through blogs, guides, and documentation. Coding contributions The dbt Community supports a wide variety of open source and source-available projects, and this software is at the heart of everything we do. Learn how to get involved with projects in the dbt ecosystem. Online community building Getting involved in the dbt Community  Forum or Slack is one of the best entry points for contributing. Share your knowledge and learn from others. Realtime event participation Want to speak at a Meetup or conference? Learn how to get involved and check out best practices for crafting a talk that everyone will remember.\nThere are opportunities here for everyone to get involved, whether you are just beginning your analytics engineering journey or you are a seasoned data professional. Contributing isn\u2019t about knowing all of the answers, it\u2019s about learning things together. Below you\u2019ll find a sampling of the ways to get involved. There are a lot of options but these are ultimately just variations on the theme of sharing knowledge with the broader community. Writing contributions Learn how to share and grow the collective knowledge of the dbt Community through blogs, guides, and documentation. Coding contributions The dbt Community supports a wide variety of open source and source-available projects, and this software is at the heart of everything we do. Learn how to get involved with projects in the dbt ecosystem. Online community building Getting involved in the dbt Community  Forum or Slack is one of the best entry points for contributing. Share your knowledge and learn from others. Realtime event participation Want to speak at a Meetup or conference? Learn how to get involved and check out best practices for crafting a talk that everyone will remember.\nBelow you\u2019ll find a sampling of the ways to get involved. There are a lot of options but these are ultimately just variations on the theme of sharing knowledge with the broader community. Writing contributions Learn how to share and grow the collective knowledge of the dbt Community through blogs, guides, and documentation. Coding contributions The dbt Community supports a wide variety of open source and source-available projects, and this software is at the heart of everything we do. Learn how to get involved with projects in the dbt ecosystem. Online community building Getting involved in the dbt Community  Forum or Slack is one of the best entry points for contributing. Share your knowledge and learn from others. Realtime event participation Want to speak at a Meetup or conference? Learn how to get involved and check out best practices for crafting a talk that everyone will remember.\nWriting contributions\nCoding contributions\nOnline community building\nRealtime event participation\nWant to get involved? Start here"
  },
  {
    "url": "https://docs.getdbt.com/category/project-configs",
    "text": "Project configs\nThe list of project configs available in dbt.\n\ud83d\udcc4\ufe0f dbt_project.yml\nReference guide for configuring the dbt_project.yml file.\n\ud83d\udcc4\ufe0f .dbtignore\nYou can create a .dbtignore file in the root of your dbt project to specify files that should be entirely ignored by dbt. The file behaves like a .gitignore file, using the same syntax. Files and subdirectories matching the pattern will not be read, parsed, or otherwise detected by dbt\u2014as if they didn't exist.\n\ud83d\udcc4\ufe0f analysis-paths\nRead this guide to understand the analysis-paths configuration in dbt.\n\ud83d\udcc4\ufe0f asset-paths\nRead this guide to understand the asset-paths configuration in dbt.\n\ud83d\udcc4\ufe0f clean-targets\nDefinition\n\ud83d\udcc4\ufe0f config-version\nRead this guide to understand the config-version configuration in dbt.\n\ud83d\udcc4\ufe0f dispatch (config)\nRead this guide to understand the dispatch configuration in dbt.\n\ud83d\udcc4\ufe0f docs-paths\nRead this guide to understand the docs-paths configuration in dbt.\n\ud83d\udcc4\ufe0f macro-paths\nRead this guide to understand the macro-paths configuration in dbt.\n\ud83d\udcc4\ufe0f name\nRead this guide to understand the name configuration in dbt.\n\ud83d\udcc4\ufe0f on-run-start & on-run-end\nRead this guide to understand the on-run-start and on-run-end configurations in dbt.\n\ud83d\udcc4\ufe0f packages-install-path\nDefinition\n\ud83d\udcc4\ufe0f profile\nRead this guide to understand the profile configuration in dbt.\n\ud83d\udcc4\ufe0f query-comment\nThe query-comment configuration also accepts a dictionary input, like so:\n\ud83d\udcc4\ufe0f quoting\nRead this guide to understand the quoting configuration in dbt.\n\ud83d\udcc4\ufe0f require-dbt-version\nRead this guide to understand the require-dbt-version configuration in dbt.\n\ud83d\udcc4\ufe0f snapshot-paths\nRead this guide to understand the snapshot-paths configuration in dbt.\n\ud83d\udcc4\ufe0f seed-paths\nDefinition\n\ud83d\udcc4\ufe0f model-paths\nDefinition\n\ud83d\udcc4\ufe0f test-paths\nDefinition\n\ud83d\udcc4\ufe0f version\ndbt projects have two distinct types of version tags. This field has a different meaning depending on its location."
  },
  {
    "url": "https://docs.getdbt.com/reference/resource-configs/resource-configs",
    "text": "Platform-specific configs\nPlatform-specific configs are used to configure the dbt project for a specific database platform.\n\ud83d\udcc4\ufe0f Microsoft Azure Synapse DWH configurations\nAll configuration options for the Microsoft SQL Server adapter also apply to this adapter.\n\ud83d\udcc4\ufe0f Amazon Athena configurations\nReference article for the Amazon Athena adapter for dbt Core and the dbt platform.\n\ud83d\udcc4\ufe0f Apache Impala configurations\nImpala Configs - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f Apache Spark configurations\nApache Spark Configurations - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f BigQuery configurations\nReference guide for Big Query configurations in dbt.\n\ud83d\udcc4\ufe0f ClickHouse configurations\nRead this guide to understand ClickHouse configurations in dbt.\n\ud83d\udcc4\ufe0f Databricks configurations\nConfiguring tables\n\ud83d\udcc4\ufe0f Doris/SelectDB configurations\nDoris/SelectDB Configurations - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f DuckDB configurations\nProfile\n\ud83d\udcc4\ufe0f Microsoft Fabric Data Warehouse configurations\nMaterializations\n\ud83d\udcc4\ufe0f Microsoft Fabric Spark configurations\nMicrosoft Fabric Spark Configurations - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f Firebolt configurations\nSetting quote_columns\n\ud83d\udcc4\ufe0f Greenplum configurations\nGreenplum Configurations - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f Infer configurations\nRead this guide to understand how to configure Infer with dbt.\n\ud83d\udcc4\ufe0f IBM Netezza configurations\nInstance requirements\n\ud83d\udcc4\ufe0f Materialize configurations\nMaterialize Configurations- Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f Microsoft SQL Server configurations\nMaterializations\n\ud83d\udcc4\ufe0f MindsDB configurations\nAuthentication\n\ud83d\udcc4\ufe0f Oracle configurations\nUse parallel hint\n\ud83d\udcc4\ufe0f Postgres configurations\nPostgres Configurations - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f Redshift configurations\nRedshift Configurations - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f SingleStore configurations\nIncremental materialization strategies\n\ud83d\udcc4\ufe0f Snowflake configurations\nSnowflake Configurations - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f Starburst/Trino configurations\nCluster requirements\n\ud83d\udcc4\ufe0f Starrocks configurations\nStarrocks Configurations - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f Teradata configurations\nGeneral\n\ud83d\udcc4\ufe0f Upsolver configurations\nUpsolver Configurations - Read this in-depth guide to learn about configurations in dbt.\n\ud83d\udcc4\ufe0f Vertica configurations\nConfiguration of Incremental Models\n\ud83d\udcc4\ufe0f IBM watsonx.data Presto configurations\nInstance requirements\n\ud83d\udcc4\ufe0f IBM watsonx.data Spark configurations\nInstance requirements\n\ud83d\udcc4\ufe0f Yellowbrick configurations\nYellowbrick Configurations: Read this in-depth guide to learn about configurations in dbt."
  },
  {
    "url": "https://docs.getdbt.com/reference/resource-configs/resource-path",
    "text": "Resource path\nThe <resource-path> nomenclature is used in this documentation when documenting how to configure resource types like models, seeds, snapshots, tests, sources, and others, from your dbt_project.yml file.\n<resource-path>\ndbt_project.yml\nIt represents the nested dictionary keys that provide the path to a directory of that resource type, or a single instance of that resource type by name.\nresource_type : project_name : directory_name : subdirectory_name : instance_of_resource_type (by name) : ...\nresource_type : project_name : directory_name : subdirectory_name : instance_of_resource_type (by name) : ...\nExample \u200b\nThe following examples are mostly for models and a source, but the same concepts apply for seeds, snapshots, tests, sources, and other resource types.\nApply config to all models \u200b\nTo apply a configuration to all models, do not use a <resource-path> :\n<resource-path>\nmodels : +enabled : false # this will disable all models (not a thing you probably want to do)\nmodels : +enabled : false # this will disable all models (not a thing you probably want to do)\nApply config to all models in your project \u200b\nTo apply a configuration to all models in your project only, use your project name as the <resource-path> :\n<resource-path>\nname : jaffle_shop models : jaffle_shop : +enabled : false # this will apply to all models in your project, but not any installed packages\nname : jaffle_shop models : jaffle_shop : +enabled : false # this will apply to all models in your project, but not any installed packages\nApply config to all models in a subdirectory \u200b\nTo apply a configuration to all models in a subdirectory of your project, e.g. staging , nest the directory under the project name:\nstaging\nname : jaffle_shop models : jaffle_shop : staging : +enabled : false # this will apply to all models in the `staging/` directory of your project\nname : jaffle_shop models : jaffle_shop : staging : +enabled : false # this will apply to all models in the `staging/` directory of your project\nIn the following project, this would apply to models in the staging/ directory, but not the marts/ directory:\nstaging/\nmarts/\n. \u251c\u2500\u2500 dbt_project.yml \u2514\u2500\u2500 models \u251c\u2500\u2500 marts \u2514\u2500\u2500 staging\n. \u251c\u2500\u2500 dbt_project.yml \u2514\u2500\u2500 models \u251c\u2500\u2500 marts \u2514\u2500\u2500 staging\nApply config to a specific model \u200b\nTo apply a configuration to a specific model, nest the full path under the project name. For a model at /staging/stripe/payments.sql , this would look like:\n/staging/stripe/payments.sql\nname : jaffle_shop models : jaffle_shop : staging : stripe : payments : +enabled : false # this will apply to only one model\nname : jaffle_shop models : jaffle_shop : staging : stripe : payments : +enabled : false # this will apply to only one model\nIn the following project, this would only apply to the payments model:\npayments\n. \u251c\u2500\u2500 dbt_project.yml \u2514\u2500\u2500 models \u251c\u2500\u2500 marts \u2502\u00a0\u00a0 \u2514\u2500\u2500 core \u2502\u00a0\u00a0     \u251c\u2500\u2500 dim_customers.sql \u2502\u00a0\u00a0     \u2514\u2500\u2500 fct_orders.sql \u2514\u2500\u2500 staging \u251c\u2500\u2500 jaffle_shop \u2502\u00a0\u00a0 \u251c\u2500\u2500 customers.sql \u2502\u00a0\u00a0 \u2514\u2500\u2500 orders.sql \u2514\u2500\u2500 stripe \u2514\u2500\u2500 payments.sql\n. \u251c\u2500\u2500 dbt_project.yml \u2514\u2500\u2500 models \u251c\u2500\u2500 marts \u2502\u00a0\u00a0 \u2514\u2500\u2500 core \u2502\u00a0\u00a0     \u251c\u2500\u2500 dim_customers.sql \u2502\u00a0\u00a0     \u2514\u2500\u2500 fct_orders.sql \u2514\u2500\u2500 staging \u251c\u2500\u2500 jaffle_shop \u2502\u00a0\u00a0 \u251c\u2500\u2500 customers.sql \u2502\u00a0\u00a0 \u2514\u2500\u2500 orders.sql \u2514\u2500\u2500 stripe \u2514\u2500\u2500 payments.sql\nApply config to a source nested in a subfolder \u200b\nTo disable a source table nested in a YAML file in a subfolder, you will need to supply the subfolder(s) within the path to that YAML file, as well as the source name and the table name in the dbt_project.yml file. The following example shows how to disable a source table nested in a YAML file in a subfolder:\ndbt_project.yml\nsources : your_project_name : subdirectory_name : source_name : source_table_name : +enabled : false\nsources : your_project_name : subdirectory_name : source_name : source_table_name : +enabled : false\nExample Apply config to all models Apply config to all models in your project Apply config to all models in a subdirectory Apply config to a specific model Apply config to a source nested in a subfolder\nApply config to all models Apply config to all models in your project Apply config to all models in a subdirectory Apply config to a specific model Apply config to a source nested in a subfolder\nApply config to all models in your project Apply config to all models in a subdirectory Apply config to a specific model Apply config to a source nested in a subfolder\nApply config to all models in a subdirectory Apply config to a specific model Apply config to a source nested in a subfolder\nApply config to a specific model Apply config to a source nested in a subfolder\nApply config to a source nested in a subfolder"
  },
  {
    "url": "https://docs.getdbt.com/reference/dbt-commands",
    "text": "dbt Command reference\nYou can run dbt using the following tools:\nIn your browser with the Studio IDE\nOn the command line interface using the Cloud CLI or open-source dbt Core .\nA key distinction with the tools mentioned, is that Cloud CLI and Studio IDE are designed to support safe parallel execution of dbt commands, leveraging dbt 's infrastructure and its comprehensive features . In contrast, dbt Core doesn't support safe parallel execution for multiple invocations in the same process. Learn more in the parallel execution section.\nParallel execution \u200b\ndbt allows for concurrent execution of commands, enhancing efficiency without compromising data integrity. This enables you to run multiple commands at the same time. However, it's important to understand which commands can be run in parallel and which can't.\nIn contrast, dbt-core doesn't support safe parallel execution for multiple invocations in the same process, and requires users to manage concurrency manually to ensure data integrity and system stability.\ndbt-core\nTo ensure your dbt workflows are both efficient and safe, you can run different types of dbt commands at the same time (in parallel) \u2014 for example, dbt build (write operation) can safely run alongside dbt parse (read operation) at the same time. However, you can't run dbt build and dbt run (both write operations) at the same time.\ndbt build\ndbt parse\ndbt build\ndbt run\ndbt commands can be read or write commands:\nread\nwrite\nCommand type Description Example Write These commands perform actions that change data or metadata in your data platform. Limited to one invocation at any given time, which prevents any potential conflicts, such as overwriting the same table in your data platform at the same time. dbt build dbt run Read These commands involve operations that fetch or read data without making any changes to your data platform. Can have multiple invocations in parallel and aren't limited to one invocation at any given time. This means read commands can run in parallel with other read commands and a single write command. dbt parse dbt compile\nDescription Example Write These commands perform actions that change data or metadata in your data platform. Limited to one invocation at any given time, which prevents any potential conflicts, such as overwriting the same table in your data platform at the same time. dbt build dbt run Read These commands involve operations that fetch or read data without making any changes to your data platform. Can have multiple invocations in parallel and aren't limited to one invocation at any given time. This means read commands can run in parallel with other read commands and a single write command. dbt parse dbt compile\nExample Write These commands perform actions that change data or metadata in your data platform. Limited to one invocation at any given time, which prevents any potential conflicts, such as overwriting the same table in your data platform at the same time. dbt build dbt run Read These commands involve operations that fetch or read data without making any changes to your data platform. Can have multiple invocations in parallel and aren't limited to one invocation at any given time. This means read commands can run in parallel with other read commands and a single write command. dbt parse dbt compile\nWrite These commands perform actions that change data or metadata in your data platform. Limited to one invocation at any given time, which prevents any potential conflicts, such as overwriting the same table in your data platform at the same time. dbt build dbt run Read These commands involve operations that fetch or read data without making any changes to your data platform. Can have multiple invocations in parallel and aren't limited to one invocation at any given time. This means read commands can run in parallel with other read commands and a single write command. dbt parse dbt compile\nThese commands perform actions that change data or metadata in your data platform. Limited to one invocation at any given time, which prevents any potential conflicts, such as overwriting the same table in your data platform at the same time. dbt build dbt run Read These commands involve operations that fetch or read data without making any changes to your data platform. Can have multiple invocations in parallel and aren't limited to one invocation at any given time. This means read commands can run in parallel with other read commands and a single write command. dbt parse dbt compile\ndbt build dbt run Read These commands involve operations that fetch or read data without making any changes to your data platform. Can have multiple invocations in parallel and aren't limited to one invocation at any given time. This means read commands can run in parallel with other read commands and a single write command. dbt parse dbt compile\ndbt build\ndbt run\nRead These commands involve operations that fetch or read data without making any changes to your data platform. Can have multiple invocations in parallel and aren't limited to one invocation at any given time. This means read commands can run in parallel with other read commands and a single write command. dbt parse dbt compile\nThese commands involve operations that fetch or read data without making any changes to your data platform. Can have multiple invocations in parallel and aren't limited to one invocation at any given time. This means read commands can run in parallel with other read commands and a single write command. dbt parse dbt compile\ndbt parse dbt compile\ndbt parse\ndbt compile\nAvailable commands \u200b\nThe following sections outline the commands supported by dbt and their relevant flags. They are available in all tools and all supported versions unless noted otherwise. You can run these commands in your specific tool by prefixing them with dbt \u2014 for example, to run the test command, type dbt test .\ndbt\ntest\ndbt test\nFor information about selecting models on the command line, refer to Model selection syntax .\nCommands with a ('\u274c') indicate write commands, commands with a ('\u2705') indicate read commands, and commands with a (N/A) indicate it's not relevant to the parallelization of dbt commands.\nCommand Description Parallel execution Caveats build Builds and tests all selected resources (models, seeds, snapshots, tests) \u274c All tools All supported versions cancel Cancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nDescription Parallel execution Caveats build Builds and tests all selected resources (models, seeds, snapshots, tests) \u274c All tools All supported versions cancel Cancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nParallel execution Caveats build Builds and tests all selected resources (models, seeds, snapshots, tests) \u274c All tools All supported versions cancel Cancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nCaveats build Builds and tests all selected resources (models, seeds, snapshots, tests) \u274c All tools All supported versions cancel Cancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nbuild Builds and tests all selected resources (models, seeds, snapshots, tests) \u274c All tools All supported versions cancel Cancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nBuilds and tests all selected resources (models, seeds, snapshots, tests) \u274c All tools All supported versions cancel Cancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u274c All tools All supported versions cancel Cancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions cancel Cancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ncancel Cancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nCancels the most recent invocation. N/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nN/A Cloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nCloud CLI Requires dbt v1.6 or higher clean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nclean Deletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nDeletes artifacts present in the dbt project \u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 All tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions clone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nclone Clones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nClones selected models from the specified state \u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u274c All tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools Requires dbt v1.6 or higher compile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ncompile Compiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nCompiles (but does not run) the models in a project \u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 All tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions debug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ndebug Debugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nDebugs dbt connections and projects \u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 Studio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nStudio IDE , Cloud CLI , dbt Core All supported versions deps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ndeps Downloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nDownloads dependencies for a project \u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 All tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions docs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ndocs Generates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nGenerates documentation for a project \u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 All tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions environment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nenvironment Enables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nEnables you to interact with your dbt environment. N/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nN/A Cloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nCloud CLI Requires dbt v1.5 or higher help Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nhelp Displays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nDisplays help information for any command N/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nN/A dbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ndbt Core , Cloud CLI All supported versions init Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ninit Initializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nInitializes a new dbt project \u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 dbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ndbt Core All supported versions invocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ninvocation Enables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nEnables users to debug long-running sessions by interacting with active invocations. N/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nN/A Cloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nCloud CLI Requires dbt v1.5 or higher list Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nlist Lists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nLists resources defined in a dbt project \u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 All tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions parse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nparse Parses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nParses a project and writes detailed timing info \u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 All tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions reattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nreattach Reattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nReattaches to the most recent invocation to retrieve logs and artifacts. N/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nN/A Cloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nCloud CLI Requires dbt v1.6 or higher retry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nretry Retry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nRetry the last run dbt command from the point of failure \u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ndbt\n\u274c All tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools Requires dbt v1.6 or higher run Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nrun Runs the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nRuns the models in a project \u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u274c All tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions run-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nrun-operation Invokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nInvokes a macro, including running arbitrary maintenance SQL against the database \u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u274c All tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions seed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nseed Loads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nLoads CSV files into the database \u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u274c All tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions show Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nshow Previews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nPreviews table rows post-transformation \u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 All tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions snapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nsnapshot Executes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nExecutes \"snapshot\" jobs defined in a project \u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u274c All tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions source Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nsource Provides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nProvides tools for working with source data (including validating that sources are \"fresh\") \u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\n\u2705 All tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\nAll tools All supported versions test Executes tests defined in a project \u2705 All tools All supported versions\ntest Executes tests defined in a project \u2705 All tools All supported versions\nExecutes tests defined in a project \u2705 All tools All supported versions\n\u2705 All tools All supported versions\nAll tools All supported versions\nNote, use the --version flag to display the installed dbt Core or Cloud CLI version. (Not applicable for the Studio IDE ). Available on all supported versions .\n--version\nParallel execution Available commands\nAvailable commands"
  },
  {
    "url": "https://docs.getdbt.com/category/jinja-reference",
    "text": "Jinja reference\nThe list of Jinja functions available in dbt.\n\ud83d\uddc3\ufe0f dbt Jinja functions\n46 items\n\ud83d\udcc4\ufe0f dbt Classes\ndbt has a number of classes it uses to represent objects in a , parts of a dbt project, and the results of a command."
  },
  {
    "url": "https://docs.getdbt.com/reference/artifacts/dbt-artifacts",
    "text": "About dbt artifacts\nWith every invocation, dbt generates and saves one or more artifacts . Several of these are JSON files ( semantic_manifest.json , manifest.json , catalog.json , run_results.json , and sources.json ) that are used to power:\nsemantic_manifest.json\nmanifest.json\ncatalog.json\nrun_results.json\nsources.json\ndocumentation\nstate\nvisualizing source freshness\nThey could also be used to:\ngain insights into your Semantic Layer\ncalculate project-level test coverage\nperform longitudinal analysis of run timing\nidentify historical changes in table structure\ndo much, much more\nWhen are artifacts produced? Starter Enterprise \u200b\nMost dbt commands (and corresponding RPC methods) produce artifacts:\nsemantic manifest : produced whenever your dbt project is parsed\nmanifest : produced by commands that read and understand your project\nrun results : produced by commands that run, compile, or catalog nodes in your DAG\ncatalog : produced by docs generate\ndocs generate\nsources : produced by source freshness\nsource freshness\nWhen running commands from the dbt CLI , all artifacts are downloaded by default. If you want to change this behavior, refer to How to skip artifacts from being downloaded .\nWhere are artifacts produced? \u200b\nBy default, artifacts are written to the /target directory of your dbt project. You can configure the location using the target-path flag .\n/target\ntarget-path\nCommon metadata \u200b\nAll artifacts produced by dbt include a metadata dictionary with these properties:\nmetadata\ndbt_version : Version of dbt that produced this artifact. For details about release versioning, refer to Versioning .\ndbt_version\ndbt_schema_version : URL of this artifact's schema. See notes below.\ndbt_schema_version\ngenerated_at : Timestamp in UTC when this artifact was produced.\ngenerated_at\nadapter_type : The adapter (database), e.g. postgres , spark , etc.\nadapter_type\npostgres\nspark\nenv : Any environment variables prefixed with DBT_ENV_CUSTOM_ENV_ will be included in a dictionary, with the prefix-stripped variable name as its key.\nenv\nDBT_ENV_CUSTOM_ENV_\ninvocation_id : Unique identifier for this dbt invocation\ninvocation_id\nIn the manifest, the metadata may also include:\nmetadata\nsend_anonymous_usage_stats : Whether this invocation sent anonymous usage statistics while executing.\nsend_anonymous_usage_stats\nproject_name : The name defined in the root project's dbt_project.yml . (Added in manifest v10 / dbt Core v1.6)\nproject_name\nname\ndbt_project.yml\nproject_id : Project identifier, hashed from project_name , sent with anonymous usage stats if enabled.\nproject_id\nproject_name\nuser_id : User identifier, stored by default in ~/dbt/.user.yml , sent with anonymous usage stats if enabled.\nuser_id\n~/dbt/.user.yml\nNotes: \u200b\nThe structure of dbt artifacts is canonized by JSON schemas , which are hosted at schemas.getdbt.com .\nArtifact versions may change in any minor version of dbt ( v1.x.0 ). Each artifact is versioned independently.\nv1.x.0\nRelated docs \u200b\nOther artifacts files such as index.html or graph_summary.json .\nindex.html\ngraph_summary.json\nWhen are artifacts produced? Where are artifacts produced? Common metadata Related docs\nWhere are artifacts produced? Common metadata Related docs\nCommon metadata Related docs\nRelated docs"
  },
  {
    "url": "https://docs.getdbt.com/reference/database-permissions/about-database-permissions",
    "text": "Database permissions\nDatabase permissions are access rights and privileges granted to users or roles within a database or data platform. They help you specify what actions users or roles can perform on various database objects, like tables, views, schemas, or even the entire database.\nWhy are they useful \u200b\nDatabase permissions are essential for security and data access control.\nThey ensure that only authorized users can perform specific actions.\nThey help maintain data integrity, prevent unauthorized changes, and limit exposure to sensitive data.\nPermissions also support compliance with data privacy regulations and auditing.\nHow to use them \u200b\nUsers and administrators can grant and manage permissions at various levels (such as table, schema, and so on) using SQL statements or through the database system's interface.\nAssign permissions to individual users or roles (groups of users) based on their responsibilities. Typical permissions include \"SELECT\" (read), \"INSERT\" (add data), \"UPDATE\" (modify data), \"DELETE\" (remove data), and administrative rights like \"CREATE\" and \"DROP.\"\nTypical permissions include \"SELECT\" (read), \"INSERT\" (add data), \"UPDATE\" (modify data), \"DELETE\" (remove data), and administrative rights like \"CREATE\" and \"DROP.\"\nUsers should be assigned permissions that ensure they have the necessary access to perform their tasks without overextending privileges.\nSomething to note is that each data platform provider might have different approaches and names for privileges. Refer to their documentation for more details.\nExamples \u200b\nRefer to the following database permission pages for more info on examples and how to set up database permissions:\nDatabricks\nPostgres\nRedshift\nSnowflake\nWhy are they useful How to use them Examples\nHow to use them Examples\nExamples"
  },
  {
    "url": "https://docs.getdbt.com/reference/dbt_project.yml",
    "text": "dbt_project.yml\nThe dbt_project.yml file is a required file for all dbt projects. It contains important information that tells dbt how to operate your project. Every dbt project needs a dbt_project.yml file \u2014 this is how dbt knows a directory is a dbt project. It also contains important information that tells dbt how to operate your project. It works as follows: dbt uses YAML in a few different places. If you're new to YAML, it would be worth learning how arrays, dictionaries, and strings are represented. By default, dbt looks for the dbt_project.yml in your current working directory and its parents, but you can set a different directory using the --project-dir flag or the DBT_PROJECT_DIR environment variable. Specify your dbt project ID in the dbt_project.yml file using project-id under the dbt-cloud config. Find your project ID in your dbt project URL: For example, in https://YOUR_ACCESS_URL/11/projects/123456 , the project ID is 123456 . Note, you can't set up a \"property\" in the dbt_project.yml file if it's not a config (an example is macros ). This applies to all types of resources. Refer to Configs and properties for more detail. Example \u200b The following example is a list of all available configurations in the dbt_project.yml file: dbt_project.yml name : string config-version : 2 version : version profile : profilename model-paths : [directorypath] seed-paths : [directorypath] test-paths : [directorypath] analysis-paths : [directorypath] macro-paths : [directorypath] snapshot-paths : [directorypath] docs-paths : [directorypath] asset-paths : [directorypath] packages-install-path : directorypath clean-targets : [directorypath] query-comment : string require-dbt-version : version-range | [version-range] flags : <global-configs> dbt-cloud : project-id : project_id # Required defer-env-id : environment_id # Optional exposures : + enabled : true | false quoting : database : true | false schema : true | false identifier : true | false metrics : <metric-configs> models : <model-configs> seeds : <seed-configs> semantic-models : <semantic-model-configs> saved-queries : <saved-queries-configs> snapshots : <snapshot-configs> sources : <source-configs> tests : <test-configs> vars : <variables> on-run-start : sql-statement | [sql-statement] on-run-end : sql-statement | [sql-statement] dispatch : - macro_namespace : packagename search_order : [ packagename ] restrict-access : true | false The + prefix \u200b dbt demarcates between a folder name and a configuration by using a + prefix before the configuration name. The + prefix is used for configs only and applies to dbt_project.yml under the corresponding resource key. It doesn't apply to: config() Jinja macro within a resource file config property in a .yml file. For more info, see the Using the + prefix . Naming convention \u200b It's important to follow the correct YAML naming conventions for the configs in your dbt_project.yml file to ensure dbt can process them properly. This is especially true for resource types with more than one word. Use dashes ( - ) when configuring resource types with multiple words in your dbt_project.yml file. Here's an example for saved queries : dbt_project.yml saved-queries : # Use dashes for resource types in the dbt_project.yml file. my_saved_query : +cache : enabled : true Use underscore ( _ ) when configuring resource types with multiple words for YAML files other than the dbt_project.yml file. For example, here's the same saved queries resource in the semantic_models.yml file: models/semantic_models.yml saved_queries : # Use underscores everywhere outside the dbt_project.yml file. - name : saved_query_name ... # Rest of the saved queries configuration. config : cache : enabled : true\nEvery dbt project needs a dbt_project.yml file \u2014 this is how dbt knows a directory is a dbt project. It also contains important information that tells dbt how to operate your project. It works as follows:\ndbt_project.yml\ndbt uses YAML in a few different places. If you're new to YAML, it would be worth learning how arrays, dictionaries, and strings are represented.\ndbt uses YAML in a few different places. If you're new to YAML, it would be worth learning how arrays, dictionaries, and strings are represented.\nBy default, dbt looks for the dbt_project.yml in your current working directory and its parents, but you can set a different directory using the --project-dir flag or the DBT_PROJECT_DIR environment variable.\nBy default, dbt looks for the dbt_project.yml in your current working directory and its parents, but you can set a different directory using the --project-dir flag or the DBT_PROJECT_DIR environment variable.\ndbt_project.yml\n--project-dir\nDBT_PROJECT_DIR\nSpecify your dbt project ID in the dbt_project.yml file using project-id under the dbt-cloud config. Find your project ID in your dbt project URL: For example, in https://YOUR_ACCESS_URL/11/projects/123456 , the project ID is 123456 .\nSpecify your dbt project ID in the dbt_project.yml file using project-id under the dbt-cloud config. Find your project ID in your dbt project URL: For example, in https://YOUR_ACCESS_URL/11/projects/123456 , the project ID is 123456 .\ndbt_project.yml\nproject-id\ndbt-cloud\nhttps://YOUR_ACCESS_URL/11/projects/123456\n123456\nNote, you can't set up a \"property\" in the dbt_project.yml file if it's not a config (an example is macros ). This applies to all types of resources. Refer to Configs and properties for more detail.\nNote, you can't set up a \"property\" in the dbt_project.yml file if it's not a config (an example is macros ). This applies to all types of resources. Refer to Configs and properties for more detail.\ndbt_project.yml\nExample \u200b\nThe following example is a list of all available configurations in the dbt_project.yml file:\ndbt_project.yml\nname : string config-version : 2 version : version profile : profilename model-paths : [directorypath] seed-paths : [directorypath] test-paths : [directorypath] analysis-paths : [directorypath] macro-paths : [directorypath] snapshot-paths : [directorypath] docs-paths : [directorypath] asset-paths : [directorypath] packages-install-path : directorypath clean-targets : [directorypath] query-comment : string require-dbt-version : version-range | [version-range] flags : <global-configs> dbt-cloud : project-id : project_id # Required defer-env-id : environment_id # Optional exposures : + enabled : true | false quoting : database : true | false schema : true | false identifier : true | false metrics : <metric-configs> models : <model-configs> seeds : <seed-configs> semantic-models : <semantic-model-configs> saved-queries : <saved-queries-configs> snapshots : <snapshot-configs> sources : <source-configs> tests : <test-configs> vars : <variables> on-run-start : sql-statement | [sql-statement] on-run-end : sql-statement | [sql-statement] dispatch : - macro_namespace : packagename search_order : [ packagename ] restrict-access : true | false\nname : string config-version : 2 version : version profile : profilename model-paths : [directorypath] seed-paths : [directorypath] test-paths : [directorypath] analysis-paths : [directorypath] macro-paths : [directorypath] snapshot-paths : [directorypath] docs-paths : [directorypath] asset-paths : [directorypath] packages-install-path : directorypath clean-targets : [directorypath] query-comment : string require-dbt-version : version-range | [version-range] flags : <global-configs> dbt-cloud : project-id : project_id # Required defer-env-id : environment_id # Optional exposures : + enabled : true | false quoting : database : true | false schema : true | false identifier : true | false metrics : <metric-configs> models : <model-configs> seeds : <seed-configs> semantic-models : <semantic-model-configs> saved-queries : <saved-queries-configs> snapshots : <snapshot-configs> sources : <source-configs> tests : <test-configs> vars : <variables> on-run-start : sql-statement | [sql-statement] on-run-end : sql-statement | [sql-statement] dispatch : - macro_namespace : packagename search_order : [ packagename ] restrict-access : true | false\nThe + prefix \u200b\n+\ndbt demarcates between a folder name and a configuration by using a + prefix before the configuration name. The + prefix is used for configs only and applies to dbt_project.yml under the corresponding resource key. It doesn't apply to:\n+\n+\ndbt_project.yml\nconfig() Jinja macro within a resource file\nconfig()\nconfig property in a .yml file.\n.yml\nFor more info, see the Using the + prefix .\n+\nNaming convention \u200b\nIt's important to follow the correct YAML naming conventions for the configs in your dbt_project.yml file to ensure dbt can process them properly. This is especially true for resource types with more than one word.\ndbt_project.yml\nUse dashes ( - ) when configuring resource types with multiple words in your dbt_project.yml file. Here's an example for saved queries : dbt_project.yml saved-queries : # Use dashes for resource types in the dbt_project.yml file. my_saved_query : +cache : enabled : true\nUse dashes ( - ) when configuring resource types with multiple words in your dbt_project.yml file. Here's an example for saved queries :\n-\ndbt_project.yml\nsaved-queries : # Use dashes for resource types in the dbt_project.yml file. my_saved_query : +cache : enabled : true\nsaved-queries : # Use dashes for resource types in the dbt_project.yml file. my_saved_query : +cache : enabled : true\nUse underscore ( _ ) when configuring resource types with multiple words for YAML files other than the dbt_project.yml file. For example, here's the same saved queries resource in the semantic_models.yml file: models/semantic_models.yml saved_queries : # Use underscores everywhere outside the dbt_project.yml file. - name : saved_query_name ... # Rest of the saved queries configuration. config : cache : enabled : true\nUse underscore ( _ ) when configuring resource types with multiple words for YAML files other than the dbt_project.yml file. For example, here's the same saved queries resource in the semantic_models.yml file:\n_\ndbt_project.yml\nsemantic_models.yml\nsaved_queries : # Use underscores everywhere outside the dbt_project.yml file. - name : saved_query_name ... # Rest of the saved queries configuration. config : cache : enabled : true\nsaved_queries : # Use underscores everywhere outside the dbt_project.yml file. - name : saved_query_name ... # Rest of the saved queries configuration. config : cache : enabled : true\nExample The + prefix Naming convention\nThe + prefix Naming convention\n+\nNaming convention"
  },
  {
    "url": "https://docs.getdbt.com/reference/resource-configs/postgres-configs",
    "text": "Postgres configurations\nIncremental materialization strategies \u200b\nIn dbt-postgres, the following incremental materialization strategies are supported:\nappend (default when unique_key is not defined)\nappend\nunique_key\nmerge\nmerge\ndelete+insert (default when unique_key is defined)\ndelete+insert\nunique_key\nmicrobatch\nmicrobatch\nPerformance optimizations \u200b\nUnlogged \u200b\n\"Unlogged\" tables can be considerably faster than ordinary tables, as they are not written to the write-ahead log nor replicated to read replicas. They are also considerably less safe than ordinary tables. See Postgres docs for details.\n{{ config ( materialized = 'table' , unlogged = True ) }} select . . .\n{{ config ( materialized = 'table' , unlogged = True ) }} select . . .\nmodels : +unlogged : true\nmodels : +unlogged : true\nIndexes \u200b\nWhile Postgres works reasonably well for datasets smaller than about 10m rows, database tuning is sometimes required. It's important to create indexes for columns that are commonly used in joins or where clauses.\nTable models, incremental models, seeds, snapshots, and materialized views may have a list of indexes defined. Each Postgres index can have three components:\nindexes\ncolumns (list, required): one or more columns on which the index is defined\ncolumns\nunique (boolean, optional): whether the index should be declared unique\nunique\ntype (string, optional): a supported index type (B-tree, Hash, GIN, etc)\ntype\n{{ config ( materialized = 'table' , indexes = [ { 'columns' : [ 'column_a' ] , 'type' : 'hash' } , { 'columns' : [ 'column_a' , 'column_b' ] , 'unique' : True } , ] ) }} select . . .\n{{ config ( materialized = 'table' , indexes = [ { 'columns' : [ 'column_a' ] , 'type' : 'hash' } , { 'columns' : [ 'column_a' , 'column_b' ] , 'unique' : True } , ] ) }} select . . .\nIf one or more indexes are configured on a resource, dbt will run create index DDL statement(s) as part of that resource's materialization , within the same transaction as its main create statement. For the index's name, dbt uses a hash of its properties and the current timestamp, in order to guarantee uniqueness and avoid namespace conflict with other indexes.\ncreate index\ncreate\ncreate index if not exists \"3695050e025a7173586579da5b27d275\" on \"my_target_database\" . \"my_target_schema\" . \"indexed_model\" using hash ( column_a ) ; create unique index if not exists \"1bf5f4a6b48d2fd1a9b0470f754c1b0d\" on \"my_target_database\" . \"my_target_schema\" . \"indexed_model\" ( column_a , column_b ) ;\ncreate index if not exists \"3695050e025a7173586579da5b27d275\" on \"my_target_database\" . \"my_target_schema\" . \"indexed_model\" using hash ( column_a ) ; create unique index if not exists \"1bf5f4a6b48d2fd1a9b0470f754c1b0d\" on \"my_target_database\" . \"my_target_schema\" . \"indexed_model\" ( column_a , column_b ) ;\nYou can also configure indexes for a number of resources at once:\nmodels : project_name : subdirectory : +indexes : - columns : [ 'column_a' ] type : hash\nmodels : project_name : subdirectory : +indexes : - columns : [ 'column_a' ] type : hash\nMaterialized views \u200b\nThe Postgres adapter supports materialized views with the following configuration parameters:\nParameter Type Required Default Change Monitoring Support on_configuration_change <string> no apply n/a indexes [{<dictionary>}] no none alter\nType Required Default Change Monitoring Support on_configuration_change <string> no apply n/a indexes [{<dictionary>}] no none alter\nRequired Default Change Monitoring Support on_configuration_change <string> no apply n/a indexes [{<dictionary>}] no none alter\nDefault Change Monitoring Support on_configuration_change <string> no apply n/a indexes [{<dictionary>}] no none alter\nChange Monitoring Support on_configuration_change <string> no apply n/a indexes [{<dictionary>}] no none alter\non_configuration_change <string> no apply n/a indexes [{<dictionary>}] no none alter\non_configuration_change\n<string> no apply n/a indexes [{<dictionary>}] no none alter\n<string>\nno apply n/a indexes [{<dictionary>}] no none alter\napply n/a indexes [{<dictionary>}] no none alter\napply\nn/a indexes [{<dictionary>}] no none alter\nindexes [{<dictionary>}] no none alter\nindexes\n[{<dictionary>}] no none alter\n[{<dictionary>}]\nno none alter\nnone alter\nnone\nalter\nProject file Property file Config block\nProperty file Config block\nConfig block\nmodels : <resource-path> : + materialized : materialized_view + on_configuration_change : apply | continue | fail + indexes : - columns : [ <column - name > ] unique : true | false type : hash | btree\nmodels : <resource-path> : + materialized : materialized_view + on_configuration_change : apply | continue | fail + indexes : - columns : [ <column - name > ] unique : true | false type : hash | btree\nversion : 2 models : - name : [ <model - name > ] config : materialized : materialized_view on_configuration_change : apply | continue | fail indexes : - columns : [ <column - name > ] unique : true | false type : hash | btree\nversion : 2 models : - name : [ <model - name > ] config : materialized : materialized_view on_configuration_change : apply | continue | fail indexes : - columns : [ <column - name > ] unique : true | false type : hash | btree\n{{ config( materialized =\"materialized_view\", on_configuration_change =\"apply\" | \"continue\" | \"fail\", indexes =[ { \"columns\": [\"<column-name>\"], \"unique\": true | false, \"type\": \"hash\" | \"btree\", } ] ) }}\n{{ config( materialized =\"materialized_view\", on_configuration_change =\"apply\" | \"continue\" | \"fail\", indexes =[ { \"columns\": [\"<column-name>\"], \"unique\": true | false, \"type\": \"hash\" | \"btree\", } ] ) }}\nThe indexes parameter corresponds to that of a table, as explained above.\nIt's worth noting that, unlike tables, dbt monitors this parameter for changes and applies the changes without dropping the materialized view.\nThis happens via a DROP/CREATE of the indexes, which can be thought of as an ALTER of the materialized view.\nindexes\nDROP/CREATE\nALTER\nLearn more about these parameters in Postgres's docs .\nIncremental materialization strategies Performance optimizations Unlogged Indexes Materialized views\nPerformance optimizations Unlogged Indexes Materialized views\nUnlogged Indexes\nIndexes\nMaterialized views"
  },
  {
    "url": "https://docs.getdbt.com/reference/configs-and-properties",
    "text": "Configurations and properties, what are they?\nUnderstand the difference between properties and configurations in dbt: properties describe resources, while configurations control how dbt builds them in the warehouse. Resources in your project\u2014models, snapshots, seeds, tests, and the rest\u2014can have a number of declared properties . Resources can also define configurations (configs), which are a special kind of property that bring extra abilities. What's the distinction? Properties are declared for resources one-by-one in properties.yml files. Configs can be defined there, nested under a config property. They can also be set one-by-one via a config() macro (right within .sql files), and for many resources at once in dbt_project.yml . Because configs can be set in multiple places, they are also applied hierarchically. An individual resource might inherit or override configs set elsewhere. You can select resources based on their config values using the config: selection method, but not the values of non-config properties. There are slightly different naming conventions for properties and configs depending on the file type. Refer to naming convention for more details. A rule of thumb: properties declare things about your project resources; configs go the extra step of telling dbt how to build those resources in your warehouse. This is generally true, but not always, so it's always good to check! For example, you can use resource properties to: Describe models, snapshots, seed files, and their columns Assert \"truths\" about a model, in the form of data tests , e.g. \"this id column is unique\" Define pointers to existing tables that contain raw data, in the form of sources , and assert the expected \"freshness\" of this raw data Define official downstream uses of your data models, in the form of exposures Whereas you can use configurations to: Change how a model will be materialized ( table , view , incremental, etc) Declare where a seed will be created in the database ( <database>.<schema>.<alias> ) Declare whether a resource should persist its descriptions as comments in the database Apply tags and meta to a resource\nResources in your project\u2014models, snapshots, seeds, tests, and the rest\u2014can have a number of declared properties . Resources can also define configurations (configs), which are a special kind of property that bring extra abilities. What's the distinction?\nProperties are declared for resources one-by-one in properties.yml files. Configs can be defined there, nested under a config property. They can also be set one-by-one via a config() macro (right within .sql files), and for many resources at once in dbt_project.yml .\nproperties.yml\nconfig\nconfig()\n.sql\ndbt_project.yml\nBecause configs can be set in multiple places, they are also applied hierarchically. An individual resource might inherit or override configs set elsewhere.\nYou can select resources based on their config values using the config: selection method, but not the values of non-config properties.\nconfig:\nThere are slightly different naming conventions for properties and configs depending on the file type. Refer to naming convention for more details.\nA rule of thumb: properties declare things about your project resources; configs go the extra step of telling dbt how to build those resources in your warehouse. This is generally true, but not always, so it's always good to check!\nFor example, you can use resource properties to:\nDescribe models, snapshots, seed files, and their columns\nAssert \"truths\" about a model, in the form of data tests , e.g. \"this id column is unique\"\nid\nDefine pointers to existing tables that contain raw data, in the form of sources , and assert the expected \"freshness\" of this raw data\nDefine official downstream uses of your data models, in the form of exposures\nWhereas you can use configurations to:\nChange how a model will be materialized ( table , view , incremental, etc)\nDeclare where a seed will be created in the database ( <database>.<schema>.<alias> )\n<database>.<schema>.<alias>\nDeclare whether a resource should persist its descriptions as comments in the database\nApply tags and meta to a resource"
  },
  {
    "url": "https://docs.getdbt.com/reference/dbt-jinja-functions",
    "text": "dbt Jinja functions\nIn addition to the standard Jinja library, we've added additional functions and variables to the Jinja context that are useful when working with a dbt project.\n\ud83d\udcc4\ufe0f adapter\nWrap the internal database adapter with the Jinja object `adapter`.\n\ud83d\udcc4\ufe0f as_bool\nUse this filter to coerce a Jinja output into boolean value.\n\ud83d\udcc4\ufe0f as_native\nUse this filter to coerce Jinja-compiled output into its native python.\n\ud83d\udcc4\ufe0f as_number\nUse this filter to convert Jinja-compiled output to a numeric value..\n\ud83d\udcc4\ufe0f builtins\nRead this guide to understand the builtins Jinja variable in dbt.\n\ud83d\udcc4\ufe0f config\nRead this guide to understand the config Jinja function in dbt.\n\ud83d\udcc4\ufe0f cross-database macros\nRead this guide to understand cross-database macros in dbt.\n\ud83d\udcc4\ufe0f dbt_project.yml context\nThe context methods and variables available when configuring resources in the dbt_project.yml file.\n\ud83d\udcc4\ufe0f dbt_version\nRead this guide to understand the dbt_version Jinja function in dbt.\n\ud83d\udcc4\ufe0f debug\nThe `{{ debug() }}` macro will open an iPython debugger.\n\ud83d\udcc4\ufe0f dispatch\ndbt extends functionality across data platforms using multiple dispatch.\n\ud83d\udcc4\ufe0f doc\nUse the `doc` to reference docs blocks in description fields.\n\ud83d\udcc4\ufe0f env_var\nIncorporate environment variables using `en_var` function.\n\ud83d\udcc4\ufe0f exceptions\nRaise warnings/errors with the `exceptions` namespace.\n\ud83d\udcc4\ufe0f execute\nUse `execute` to return True when dbt is in 'execute' mode.\n\ud83d\udcc4\ufe0f flags\nThe `flags` variable contains values of flags provided on the cli.\n\ud83d\udcc4\ufe0f fromjson\nDeserialize a JSON string into python with `fromjson` context method.\n\ud83d\udcc4\ufe0f fromyaml\nDeserialize a YAML string into python with `fromyaml` context method.\n\ud83d\udcc4\ufe0f graph\nThe `graph` context variable contains info about nodes in your project.\n\ud83d\udcc4\ufe0f invocation_id\nThe `invocation_id` outputs a UUID generated for this dbt command.\n\ud83d\udcc4\ufe0f local_md5\nCalculate an MD5 hash of a string with `local_md5` context variable.\n\ud83d\udcc4\ufe0f log\nLearn more about the log Jinja function in dbt.\n\ud83d\udcc4\ufe0f model\n`model` is the dbt graph object (or node) for the current model.\n\ud83d\udcc4\ufe0f modules\n`modules` Jinja variables has useful Python modules to operate data.\n\ud83d\udcc4\ufe0f on-run-end context\nUse these variables in the context for `on-run-end` hooks.\n\ud83d\udcc4\ufe0f print\nUse the `print()` to print messages to the log file and stdout.\n\ud83d\udcc4\ufe0f profiles.yml context\nUse these context methods to configure resources in `profiles.yml` file.\n\ud83d\udcc4\ufe0f project_name\nRead this guide to understand the project_name Jinja function in dbt.\n\ud83d\udcc4\ufe0f properties.yml context\nThe context methods and variables available when configuring resources in a properties.yml file.\n\ud83d\udcc4\ufe0f ref\nRead this guide to understand the ref Jinja function in dbt.\n\ud83d\udcc4\ufe0f return\nRead this guide to understand the return Jinja function in dbt.\n\ud83d\udcc4\ufe0f run_query\nUse `run_query` macro to run queries and fetch results.\n\ud83d\udcc4\ufe0f run_started_at\nUse `run_started_at` to output the timestamp the run started.\n\ud83d\udcc4\ufe0f schema\nThe schema that the model is configured to be materialized in.\n\ud83d\udcc4\ufe0f schemas\nA list of schemas where dbt built objects during the current run.\n\ud83d\udcc4\ufe0f selected_resources\nContains a list of all the nodes selected by current dbt command.\n\ud83d\udcc4\ufe0f set\nConverts any iterable to a sequence of iterable and unique elements.\n\ud83d\udcc4\ufe0f source\nRead this guide to understand the source Jinja function in dbt.\n\ud83d\udcc4\ufe0f statement blocks\nSQL queries that hit database and return results to your Jinja context.\n\ud83d\udcc4\ufe0f target\nThe `target` variable contains information about your connection to the warehouse.\n\ud83d\udcc4\ufe0f this\nRepresents the current model in the database.\n\ud83d\udcc4\ufe0f thread_id\nThe `thread_id` outputs an identifier for the current Python thread.\n\ud83d\udcc4\ufe0f tojson\nUse this context method to serialize a Python object primitive.\n\ud83d\udcc4\ufe0f toyaml\nUsed to serialize a Python object primitive.\n\ud83d\udcc4\ufe0f var\nPass variables from `dbt_project.yml` file into models.\n\ud83d\udcc4\ufe0f zip\nUse this context method to return an iterator of tuples."
  },
  {
    "url": "https://docs.getdbt.com/reference/database-permissions/snowflake-permissions",
    "text": "Snowflake permissions\nIn Snowflake, permissions are used to control who can perform certain actions on different database objects. Use SQL statements to manage permissions in a Snowflake database.\nSet up Snowflake account \u200b\nThis section explains how to set up permissions and roles within Snowflake. In Snowflake, you would perform these actions using SQL commands and set up your data warehouse and access control within Snowflake's ecosystem.\nSet up databases\nuse role sysadmin; create database raw; create database analytics;\nuse role sysadmin; create database raw; create database analytics;\nSet up warehouses\ncreate warehouse loading warehouse_size = xsmall auto_suspend = 3600 auto_resume = false initially_suspended = true; create warehouse transforming warehouse_size = xsmall auto_suspend = 60 auto_resume = true initially_suspended = true; create warehouse reporting warehouse_size = xsmall auto_suspend = 60 auto_resume = true initially_suspended = true;\ncreate warehouse loading warehouse_size = xsmall auto_suspend = 3600 auto_resume = false initially_suspended = true; create warehouse transforming warehouse_size = xsmall auto_suspend = 60 auto_resume = true initially_suspended = true; create warehouse reporting warehouse_size = xsmall auto_suspend = 60 auto_resume = true initially_suspended = true;\nSet up roles and warehouse permissions\nuse role securityadmin; create role loader; grant all on warehouse loading to role loader; create role transformer; grant all on warehouse transforming to role transformer; create role reporter; grant all on warehouse reporting to role reporter;\nuse role securityadmin; create role loader; grant all on warehouse loading to role loader; create role transformer; grant all on warehouse transforming to role transformer; create role reporter; grant all on warehouse reporting to role reporter;\nCreate users, assigning them to their roles\nEvery person and application gets a separate user and is assigned to the correct role.\ncreate user stitch_user -- or fivetran_user password = '_generate_this_' default_warehouse = loading default_role = loader; create user claire -- or amy, jeremy, etc. password = '_generate_this_' default_warehouse = transforming default_role = transformer must_change_password = true; create user dbt_cloud_user password = '_generate_this_' default_warehouse = transforming default_role = transformer; create user looker_user -- or mode_user etc. password = '_generate_this_' default_warehouse = reporting default_role = reporter; -- then grant these roles to each user grant role loader to user stitch_user; -- or fivetran_user grant role transformer to user dbt_cloud_user; grant role transformer to user claire; -- or amy, jeremy grant role reporter to user looker_user; -- or mode_user, periscope_user\ncreate user stitch_user -- or fivetran_user password = '_generate_this_' default_warehouse = loading default_role = loader; create user claire -- or amy, jeremy, etc. password = '_generate_this_' default_warehouse = transforming default_role = transformer must_change_password = true; create user dbt_cloud_user password = '_generate_this_' default_warehouse = transforming default_role = transformer; create user looker_user -- or mode_user etc. password = '_generate_this_' default_warehouse = reporting default_role = reporter; -- then grant these roles to each user grant role loader to user stitch_user; -- or fivetran_user grant role transformer to user dbt_cloud_user; grant role transformer to user claire; -- or amy, jeremy grant role reporter to user looker_user; -- or mode_user, periscope_user\nLet  loader load data\nGive the role unilateral permission to operate on the raw database\nuse role sysadmin; grant all on database raw to role loader;\nuse role sysadmin; grant all on database raw to role loader;\nLet transformer transform data\nThe transformer role needs to be able to read raw data.\nIf you do this before you have any data loaded, you can run:\ngrant usage on database raw to role transformer; grant usage on future schemas in database raw to role transformer; grant select on future tables in database raw to role transformer; grant select on future views in database raw to role transformer;\ngrant usage on database raw to role transformer; grant usage on future schemas in database raw to role transformer; grant select on future tables in database raw to role transformer; grant select on future views in database raw to role transformer;\nIf you already have data loaded in the raw database, make sure also you run the following to update the permissions\ngrant usage on all schemas in database raw to role transformer; grant select on all tables in database raw to role transformer; grant select on all views in database raw to role transformer;\ngrant usage on all schemas in database raw to role transformer; grant select on all tables in database raw to role transformer; grant select on all views in database raw to role transformer;\ntransformer also needs to be able to create in the analytics database:\ngrant all on database analytics to role transformer;\ngrant all on database analytics to role transformer;\nLet reporter read the transformed data\nA previous version of this article recommended this be implemented through hooks in dbt, but this way lets you get away with a one-off statement.\ngrant usage on database analytics to role reporter; grant usage on future schemas in database analytics to role reporter; grant select on future tables in database analytics to role reporter; grant select on future views in database analytics to role reporter;\ngrant usage on database analytics to role reporter; grant usage on future schemas in database analytics to role reporter; grant select on future tables in database analytics to role reporter; grant select on future views in database analytics to role reporter;\nAgain, if you already have data in your analytics database, make sure you run:\ngrant usage on all schemas in database analytics to role reporter; grant select on all tables in database analytics to role reporter; grant select on all views in database analytics to role reporter;\ngrant usage on all schemas in database analytics to role reporter; grant select on all tables in database analytics to role reporter; grant select on all views in database analytics to role reporter;\nMaintain\nWhen new users are added, make sure you add them to the right role! Everything else should be inherited automatically thanks to those future grants.\nfuture\nFor more discussion and legacy information, refer to this Discourse article .\nExample Snowflake permissions \u200b\nThe following example provides you with the SQL statements you can use to manage permissions.\nNote that warehouse_name , database_name , and role_name are placeholders and you can replace them as needed for your organization's naming convention.\nwarehouse_name\ndatabase_name\nrole_name\ngrant all on warehouse warehouse_name to role role_name; grant usage on database database_name to role role_name; grant create schema on database database_name to role role_name; grant usage on schema database.an_existing_schema to role role_name; grant create table on schema database.an_existing_schema to role role_name; grant create view on schema database.an_existing_schema to role role_name; grant usage on future schemas in database database_name to role role_name; grant monitor on future schemas in database database_name to role role_name; grant select on future tables in database database_name to role role_name; grant select on future views in database database_name to role role_name; grant usage on all schemas in database database_name to role role_name; grant monitor on all schemas in database database_name to role role_name; grant select on all tables in database database_name to role role_name; grant select on all views in database database_name to role role_name;\ngrant all on warehouse warehouse_name to role role_name; grant usage on database database_name to role role_name; grant create schema on database database_name to role role_name; grant usage on schema database.an_existing_schema to role role_name; grant create table on schema database.an_existing_schema to role role_name; grant create view on schema database.an_existing_schema to role role_name; grant usage on future schemas in database database_name to role role_name; grant monitor on future schemas in database database_name to role role_name; grant select on future tables in database database_name to role role_name; grant select on future views in database database_name to role role_name; grant usage on all schemas in database database_name to role role_name; grant monitor on all schemas in database database_name to role role_name; grant select on all tables in database database_name to role role_name; grant select on all views in database database_name to role role_name;\nSet up Snowflake account Example Snowflake permissions\nExample Snowflake permissions"
  },
  {
    "url": "https://docs.getdbt.com/reference/database-permissions/databricks-permissions",
    "text": "Databricks permissions\nIn Databricks, permissions are used to control who can perform certain actions on different database objects. Use SQL statements to manage permissions in a Databricks database.\nExample Databricks permissions \u200b\nThe following example provides you with the SQL statements you can use to manage permissions.\nNote that you can grant permissions on securable_objects to principals (This can be user, service principal, or group). For example, grant privilege_type on securable_object to principal .\nsecurable_objects\nprincipals\ngrant privilege_type\nsecurable_object\nprincipal\ngrant all privileges on schema schema_name to principal; grant create table on schema schema_name to principal; grant create view on schema schema_name to principal;\ngrant all privileges on schema schema_name to principal; grant create table on schema schema_name to principal; grant create view on schema schema_name to principal;\nCheck out the official documentation for more information.\nExample Databricks permissions"
  },
  {
    "url": "https://docs.getdbt.com/reference/database-permissions/redshift-permissions",
    "text": "Redshift permissions\nIn Redshift, permissions are used to control who can perform certain actions on different database objects. Use SQL statements to manage permissions in a Redshift database.\nExample Redshift permissions \u200b\nThe following example provides you with the SQL statements you can use to manage permissions.\nNote that database_name , database.schema_name , and user_name are placeholders and you can replace them as needed for your organization's naming convention.\ndatabase_name\ndatabase.schema_name\nuser_name\ngrant create schema on database database_name to user_name; grant usage on schema database.schema_name to user_name; grant create table on schema database.schema_name to user_name; grant create view on schema database.schema_name to user_name; grant usage for schemas in database database_name to role role_name; grant select on all tables in database database_name to user_name; grant select on all views in database database_name to user_name;\ngrant create schema on database database_name to user_name; grant usage on schema database.schema_name to user_name; grant create table on schema database.schema_name to user_name; grant create view on schema database.schema_name to user_name; grant usage for schemas in database database_name to role role_name; grant select on all tables in database database_name to user_name; grant select on all views in database database_name to user_name;\nTo connect to the database, confirm with an admin that your user role or group has been added to the database. Note that Redshift permissions differ from Postgres, and commands like grant connect aren't supported in Redshift.\ngrant connect\nCheck out the official documentation for more information.\nExample Redshift permissions"
  },
  {
    "url": "https://docs.getdbt.com/reference/database-permissions/postgres-permissions",
    "text": "Postgres Permissions\nIn Postgres, permissions are used to control who can perform certain actions on different database objects. Use SQL statements to manage permissions in a Postgres database.\nExample Postgres permissions \u200b\nThe following example provides you with the SQL statements you can use to manage permissions. These examples allow you to run dbt smoothly without encountering permission issues, such as creating schemas, reading existing data, and accessing the information schema.\nNote that database_name , source_schema , destination_schema , and user_name are placeholders and you can replace them as needed for your organization's naming convention.\ndatabase_name\nsource_schema\ndestination_schema\nuser_name\ngrant connect on database database_name to user_name ; -- Grant read permissions on the source schema grant usage on schema source_schema to user_name ; grant select on all tables in schema source_schema to user_name ; alter default privileges in schema source_schema grant select on tables to user_name ; -- Create destination schema and make user_name the owner create schema if not exists destination_schema ; alter schema destination_schema owner to user_name ; -- Grant write permissions on the destination schema grant usage on schema destination_schema to user_name ; grant create on schema destination_schema to user_name ; grant insert , update , delete , truncate on all tables in schema destination_schema to user_name ; alter default privileges in schema destination_schema grant insert , update , delete , truncate on tables to user_name ;\ngrant connect on database database_name to user_name ; -- Grant read permissions on the source schema grant usage on schema source_schema to user_name ; grant select on all tables in schema source_schema to user_name ; alter default privileges in schema source_schema grant select on tables to user_name ; -- Create destination schema and make user_name the owner create schema if not exists destination_schema ; alter schema destination_schema owner to user_name ; -- Grant write permissions on the destination schema grant usage on schema destination_schema to user_name ; grant create on schema destination_schema to user_name ; grant insert , update , delete , truncate on all tables in schema destination_schema to user_name ; alter default privileges in schema destination_schema grant insert , update , delete , truncate on tables to user_name ;\nCheck out the official documentation for more information.\nExample Postgres permissions"
  },
  {
    "url": "https://docs.getdbt.com/blog/dbt-fusion-engine-components",
    "text": "The Components of the dbt Fusion engine and how they fit together\nToday, we announced the dbt Fusion engine .\nFusion isn't just one thing \u2014 it's a set of interconnected components working together to power the next generation of analytics engineering.\nThis post maps out each piece of the Fusion architecture, explains how they fit together, and clarifies what's available to you whether you're compiling from source, using our pre-built binaries, or developing within a dbt Fusion powered product experience.\nFrom the Rust engine to the VS Code extension, through to new Arrow-based adapters and Apache-licensed foundational technologies, we'll break down exactly what each component does, how each component is licensed (for why, see Tristan's accompanying post ), and how you can start using it and get involved today.\nThis post describes the state of the world as it will be when Fusion reaches General Availability. For a look at the path to GA, read this post .\nThere are a number of different ways to access the dbt Fusion engine \u200b\nA big change between the dbt Fusion engine and the dbt Core engine is their language. Core is Python; Fusion is Rust. This is meaningful not just because of the performance benefits, but because it creates a new way for us to distribute functionality to the community.\nTo distribute a Python program, you also have to distribute its underlying source code. But Rust is a compiled language, meaning we can share either the source code or just the compiled binaries derived from that source code.\nThis means that features which would have otherwise had to stay completely proprietary for IP reasons can instead be broadly distributed in binary form. There's also a completely source-available version of dbt Fusion which will exceed dbt Core's capabilities by the time we reach GA.\nWhat variants of the dbt Fusion engine exist? \u200b\nSource-available dbt Fusion engine \u200b\nArtifact type: Code\nAvailable at: https://github.com/dbt-labs/dbt-fusion (Note: this repo currently only contains the code necessary for a dbt parse and dbt deps - more will follow!)\ndbt parse\ndbt deps\nLicense: ELv2\nThis will be the foundation of the Fusion engine - the code that lets you:\nExecute your dbt seed/run/test/build\ndbt seed/run/test/build\nRender your Jinja and create your DAG\nConnect to the adapters that render your dbt project into the DDL and DML that hits your warehouse\nProduce the artifacts in your dbt project\nTo be clear, the self-compiled binary that's available today doesn't do much yet. By the time the new engine enters general availability, its source-available components will exceed the net capabilities of dbt Core . If you are a data team running dbt Core, simply running the self-compiled version of dbt Fusion will be a pure upgrade.\nThis repository will also include the code necessary for Level 1 SQL Comprehension (the ability to parse SQL into a syntax tree).\nAs long as you comply with the three restrictions in ELv2 :\n\u2705\u00a0You can adopt the binary into your data workflows without dbt Labs' involvement\n\u2705\u00a0You\u00a0can see and modify the code\nPrecompiled dbt Fusion engine binary \u200b\nArtifact type: Precompiled binary\nHow to access: download following the instructions here\nLicense: ELv2\nWhen you download the precompiled binary created by dbt Labs, it contains:\nAll of the functionality in the Source Available Fusion\nAll of the functionality in the Source Available Fusion\nAdditional capabilities which are derived from proprietary code (such as the Level 2 SQL Comprehension required to compile and type-check your SQL).\nAdditional capabilities which are derived from proprietary code (such as the Level 2 SQL Comprehension required to compile and type-check your SQL).\nAs long as you comply with the three restrictions in ELv2,\n\u2705\u00a0You can\u00a0adopt the binary into your data workflows without dbt Labs' involvement\n\u274c\u00a0But you cannot see or modify the code itself\nThe vast majority of existing dbt Core users that adopt the freely distributed components of Fusion should use the binary to do so, rather than compiling it from source code. The binary has the same permissions but more capabilities (and it saves you from having to compile it yourself). You can use it internally at your company for free, even if you are not a dbt Labs customer.\nUsing the dbt Fusion engine with a commercial agreement \u200b\nArtifact type: Precompiled binary and managed service\nAvailable at: Download binary and sign up for the service\nLicense: ELv2 (binary) and Proprietary (service)\nOrganizations who do have a commercial agreement will unlock even more capabilities, but they'll use the exact same publicly-released binary discussed above. If you want to start using platform features, such as dbt Mesh , all you need to do is download a configuration file . (Joel commentary - As someone who has been juggling the dbt Cloud CLI alongside dbt Core for the last couple of years, I cannot overstate how thrilled I am by this.)\nObviously there's additional cloud-backed services necessary to deliver platform-specific features, such as State-Aware Orchestration. That code is proprietary and governed by your agreement with dbt Labs.\nOther pieces of the puzzle \u200b\nThe dbt Fusion engine is the headline act, but its underlying technologies can be mixed and matched in a variety of ways.\nThe dbt VS Code Extension and Language Server \u200b\nArtifact type: Precompiled binaries\nHow to access: Install on the VS Code marketplace\nLicense: Proprietary\nThe dbt VS Code extension is one of the first product experiences built on top of Fusion. It is not part of Fusion, it is powered by Fusion and is part of the wider dbt platform's offerings (with a generous free tier). Specifically, the VS Code extension interacts with another brand-new binary, the dbt Language Server .\nThe Language Server is built on top of a subset of the technology powering the extended Fusion engine: as an example, it can quickly compile SQL and interact with databases, but it defers to the dbt binary when it's time to actually run a model.\nThe dbt Authoring Layer \u200b\nArtifact type: JSON Schema definitions\nAvailable at: Git repos for input files and output artifacts\nLicense: Apache 2.0\nWhen you think of dbt, you're probably thinking of a combination of the Engine (described above) and the Authoring Layer.\nThe Authoring Layer is made up of everything necessary to define the what of a dbt project: things like the YAML specs, Artifact specs, CLI commands and flags , and macro signatures . As the user interface to dbt, the authoring layer is standard between Core and Fusion, although the Fusion engine does not include support for various behaviours and functions deprecated in earlier releases of dbt Core.\nFor the first time, we're releasing a series of definitive JSON schemas, backed by the code in dbt Core and Fusion , that encapsulate the acceptable content of dbt's various YAML files. These are Apache 2.0-licensed and will be particularly helpful for other tools integrating with dbt projects.\nThis joins the existing JSON schemas defining the shape of dbt's output artifacts (e.g. manifest.json ). As we stabilize Fusion's metadata output (logging and artifacts) on the path to GA, we will update the published schemas.\nmanifest.json\ndbt Fusion engine adapters \u200b\nArtifact type: Source code\nAvailable at: Initial code in dbt-fusion repo , with more to come\ndbt-fusion\nLicense: Apache 2.0 (later this year)\nAdapters are responsible for two key tasks:\nKnowing how to create the appropriate SQL commands (via macros and materializations) for a data platform\nConnecting to that target data platform and sending it SQL commands\nMuch like Fusion is the next generation engine for dbt, we also needed next-generation adapters for dbt. These adapters are written in Rust and built on the Apache Arrow standard.\nThe templating of SQL commands largely carries over from macros in the dbt Core adapters. Database connectivity is another story, the dbt Fusion engine cannot use the Python classes present in each adapter, for reasons both practical and performance-related.\nEnter the Apache Arrow ecosystem at large, and the new ADBC API in particular. ADBC is a future-looking platform for database connectivity, and we are leaning into it heavily with these Fusion adapters.\nBecause the ADBC standard is extremely new, not all databases are compatible with ADBC yet, and using ADBC in a Rust client isn't easy. To solve both problems, we have created a Rust client library, XDBC that:\nXDBC\nSupports ODBC connections to databases where Arrow is not yet provided as an output\nProvides generic methods for creating and managing connections to databases\nIs useful for anyone who wants to build data tooling in Rust, inside or outside of the dbt ecosystem\nAll of this will be open-sourced under the Apache 2.0 license later this year, namely:\nFusion adapters we have created\nThe XDBC library\nWe'll also continue upstreaming improvements to Apache Arrow's ADBC project\nANTLR Grammars \u200b\nArtifact type: g4 files\nAvailable at: (repo to come, in the meantime you can discuss this in #dbt-fusion-engine in the dbt Slack)\nLicense: Apache 2.0 (later this year)\nANTLR grammars are the formal language specifications that let Fusion parse every SQL statement across multiple dialects. Specifically, ANTLR takes in these declarative, high level grammars and uses them to generate a parser. The grammars have wide utility anywhere it's necessary to parse SQL \u2013 not just in Fusion \u2013 and we're releasing them as Apache 2 to enable the Community and others in the data ecosystem to build on top of them.\nMost ANTLR grammars are only applicable to a single dialect, but the SDF team created a system which makes it possible to define a shared base grammar and generate each warehouse's g4 file from there. This halves the amount of work required to support a new dialect at the level of precision and robustness required.\ndbt-jinja \u200b\nArtifact type: Source code\nAvailable at: A subdirectory of the dbt-fusion repo (but there's still work to do before it's easy to use outside of the Fusion repository)\nLicense: Apache 2.0\nSince Fusion is completely Rust-based, while Jinja is a Python project, we needed a completely new way to render all the Jinja spread through users' projects. We started by switching to minijinja : a Rust port of a subset of the original Jinja project, written by Jinja's original maintainer.\nThis subset of coverage wasn't enough to support existing dbt projects, so we created Rust-native implementations of the majority of these missing features. This achieved the best of both worlds: significant performance improvements while maintaining compatibility with users' existing codebases.\ndbt-jinja is the most feature-complete implementation of Jinja in Rust, and is available with an Apache 2.0 license today, with a more formal release (documentation etc) later this year. It's useful whether you're building tooling to operate on top of dbt projects, or working on something completely different which just needs to render Jinja quickly.\nHow do I engage with these components? \u200b\nOur Contributors' Principles remain: Building dbt is a team sport!\nIf you want to open a PR against publicly-viewable code, you can.\nIf you want to open issues describing bugs during the Fusion engine's beta period, you can. (This is probably one of the highest-leverage things you can do!)\nIf you want to open a discussion and pitch a new way to use dbt more effectively in our new SQL-aware world, you can.\nIf you want to move upstream, and contribute to the standards underlying the dbt Fusion engine like Arrow, ADBC, Iceberg, or DataFusion, you can. You might see some familiar faces while you're there!\nIf you just want to let dbt get better and better in the background, you can do that too.\nWant to get involved in the team building this? If the components here are uniquely interesting to you, email careers.fusion@dbtlabs.com .\nIf you need a hand wrapping your head around any of these new components, drop by #dbt-fusion-engine in the Community Slack - we'd love to chat.\nComments"
  },
  {
    "url": "https://docs.getdbt.com/blog/tags/analytics-craft",
    "text": "Analytics craft\nThe art of being an analytics practitioner.\nThe Components of the dbt Fusion engine and how they fit together\nToday, we announced the dbt Fusion engine .\nFusion isn't just one thing \u2014 it's a set of interconnected components working together to power the next generation of analytics engineering.\nThis post maps out each piece of the Fusion architecture, explains how they fit together, and clarifies what's available to you whether you're compiling from source, using our pre-built binaries, or developing within a dbt Fusion powered product experience.\nFrom the Rust engine to the VS Code extension, through to new Arrow-based adapters and Apache-licensed foundational technologies, we'll break down exactly what each component does, how each component is licensed (for why, see Tristan's accompanying post ), and how you can start using it and get involved today.\nMeet the\u202fdbt\u202fFusion Engine: the new Rust-based, industrial-grade engine for dbt\nTL;DR: What You Need to Know \u200b\ndbt\u2019s familiar authoring layer remains unchanged, but the execution engine beneath it is completely new.\nThe new engine is called the dbt Fusion engine \u2014 rewritten from the ground up in Rust based on technology from SDF .  The dbt Fusion engine is substantially faster than dbt Core and has built in SQL comprehension technology to power the next generation of analytics engineering workflows.\nThe dbt Fusion engine is currently in beta. You can try it today if you use Snowflake \u2014 with additional adapters coming starting in early June. Review our path to general availability (GA) and try the quickstart .\nYou do not need to be a dbt Labs customer to use Fusion - dbt Core users can adopt the dbt Fusion engine today for free in your local environment.\nYou can use Fusion with the new dbt VS Code extension , directly via the CLI , or via dbt Studio .\nThis is the beginning of a new era for analytics engineering. For a glimpse into what the Fusion engine is going to enable over the next 1 to 2 years, read this post .\nAI Evaluation in dbt\nThe AI revolution is here\u2014but are we ready? Across the world, the excitement around AI is undeniable.  Discussions on large language models, agentic workflows, and how AI is set to transform every industry abound, yet real-world use cases of AI in production remain few and far between.\nA common issue blocking people from moving AI use cases to production is an ability to evaluate the validity of AI responses in a systematic and well governed way.\nMoving AI workflows from prototype to production requires rigorous evaluation, and most organizations do not have a framework to ensure AI outputs remain high-quality, trustworthy, and actionable.\nGetting Started with git Branching Strategies and dbt\nHi! We\u2019re Christine and Carol, Resident Architects at dbt Labs. Our day-to-day\nwork is all about helping teams reach their technical and business-driven goals.\nCollaborating with a broad spectrum of customers ranging from scrappy startups\nto massive enterprises, we\u2019ve gained valuable experience guiding teams to\nimplement architecture which addresses their major pain points.\nThe information we\u2019re about to share isn't just from our experiences - we\nfrequently collaborate with other experts like Taylor Dunlap and Steve Dowling\nwho have greatly contributed to the amalgamation of this guidance. Their work\nlies in being the critical bridge for teams between\nimplementation and business outcomes, ultimately leading teams to align on a\ncomprehensive technical vision through identification of problems and solutions.\nWhy are we here? We help teams with dbt architecture, which encompasses the tools, processes and\nconfigurations used to start developing and deploying with dbt. There\u2019s a lot of\ndecision making that happens behind the scenes to standardize on these pieces -\nmuch of which is informed by understanding what we want the development workflow\nto look like. The focus on having the perfect workflow often gets teams\nstuck in heaps of planning and endless conversations, which slows down or even\nstops momentum on development. If you feel this, we\u2019re hoping our guidance will\ngive you a great sense of comfort in taking steps to unblock development - even\nwhen you don\u2019t have everything figured out yet!\nWhy I wish I had a control plane for my renovation\nWhen my wife and I renovated our home, we chose to take on the role of owner-builder. It was a bold (and mostly naive) decision, but we wanted control over every aspect of the project. What we didn\u2019t realize was just how complex and exhausting managing so many moving parts would be.\nWe had to coordinate multiple elements:\nThe architects , who designed the layout, interior, and exterior.\nThe architectural plans , which outlined what the house should look like.\nThe builders , who executed those plans.\nThe inspectors , councils , and energy raters , who checked whether everything met the required standards.\nTest smarter not harder: Where should tests go in your pipeline?\n\ud83d\udc4b\u00a0Greetings, dbt\u2019ers! It\u2019s Faith & Jerrie, back again to offer tactical advice on where to put tests in your pipeline.\nIn our first post on refining testing best practices, we developed a prioritized list of data quality concerns. We also documented first steps for debugging each concern. This post will guide you on where specific tests should go in your data pipeline.\nNote that we are constructing this guidance based on how we structure data at dbt Labs. You may use a different modeling approach\u2014that\u2019s okay! Translate our guidance to your data\u2019s shape, and let us know in the comments section what modifications you made.\nFirst, here\u2019s our opinions on where specific tests should go:\nSource tests should be fixable data quality concerns. See the callout box below for what we mean by \u201cfixable\u201d.\nStaging tests should be business-focused anomalies specific to individual tables, such as accepted ranges or ensuring sequential values. In addition to these tests, your staging layer should clean up any nulls, duplicates, or outliers that you can\u2019t fix in your source system. You generally don\u2019t need to test your cleanup efforts.\nIntermediate and marts layer tests should be business-focused anomalies resulting specifically from joins or calculations.  You also may consider adding additional primary key and not null tests on columns where it\u2019s especially important to protect the grain.\nTest smarter not harder: add the right tests to your dbt project\nThe Analytics Development Lifecycle (ADLC) is a workflow for improving data maturity and velocity. Testing is a key phase here. Many dbt developers tend to focus on primary keys and source freshness. We think there is a more holistic and in-depth path to tread. Testing is a key piece of the ADLC, and it should drive data quality.\nIn this blog, we\u2019ll walk through a plan to define data quality. This will look like:\nidentifying data hygiene issues\nidentifying business-focused anomaly issues\nidentifying stats-focused anomaly issues\nOnce we have defined data quality, we\u2019ll move on to prioritize those concerns. We will:\nthink through each concern in terms of the breadth of impact\ndecide if each concern should be at error or warning severity\nHow Hybrid Mesh unlocks dbt collaboration at scale\nOne of the most important things that dbt does is unlock the ability for teams to collaborate on creating and disseminating organizational knowledge.\nIn the past, this primarily looked like a team working in one dbt Project to create a set of transformed objects in their data platform.\nAs dbt was adopted by larger organizations and began to drive workloads at a global scale, it became clear that we needed mechanisms to allow teams to operate independently from each other, creating and sharing data models across teams \u2014 dbt Mesh .\nHow to build a Semantic Layer in pieces: step-by-step for busy analytics engineers\nThe dbt Semantic Layer is founded on the idea that data transformation should be both flexible , allowing for on-the-fly aggregations grouped and filtered by definable dimensions and version-controlled and tested . Like any other codebase, you should have confidence that your transformations express your organization\u2019s business logic correctly. Historically, you had to choose between these options, but the dbt Semantic Layer brings them together. This has required new paradigms for how you express your transformations though.\nPutting Your DAG on the internet\nNew in dbt: allow Snowflake Python models to access the internet \u200b\nWith dbt 1.8, dbt released support for Snowflake\u2019s external access integrations further enabling the use of dbt + AI to enrich your data. This allows querying of external APIs within dbt Python models, a functionality that was required for dbt Cloud customer, EQT AB . Learn about why they needed it and how they helped build the feature and get it shipped!\nUnit testing in dbt for test-driven development\nDo you ever have \"bad data\" dreams? Or am I the only one that has recurring nightmares? \ud83d\ude31\nHere's the one I had last night:\nIt began with a midnight bug hunt. A menacing insect creature has locked my colleagues in a dungeon, and they are pleading for my help to escape . Finding the key is elusive and always seems just beyond my grasp. The stress is palpable, a physical weight on my chest, as I raced against time to unlock them.\nOf course I wake up without actually having saved them, but I am relieved nonetheless. And I've had similar nightmares involving a heroic code refactor or the launch of a new model or feature.\nGood news: beginning in dbt v1.8, we're introducing a first-class unit testing framework that can handle each of the scenarios from my data nightmares.\nBefore we dive into the details, let's take a quick look at how we got here.\nMaximum override: Configuring unique connections in dbt Cloud\ndbt Cloud now includes a suite of new features that enable configuring precise and unique connections to data platforms at the environment and user level. These enable more sophisticated setups, like connecting a project to multiple warehouse accounts, first-class support for staging environments , and user-level overrides for specific dbt versions . This gives dbt Cloud developers the features they need to tackle more complex tasks, like Write-Audit-Publish (WAP) workflows and safely testing dbt version upgrades. While you still configure a default connection at the project level and per-developer, you now have tools to get more advanced in a secure way. Soon, dbt Cloud will take this even further allowing multiple connections to be set globally and reused with global connections .\nLLM-powered Analytics Engineering: How we're using AI inside of our dbt project, today, with no new tools.\nCloud Data Platforms make new things possible; dbt helps you put them into production \u200b\nThe original paradigm shift that enabled dbt to exist and be useful was databases going to the cloud.\nAll of a sudden it was possible for more people to do better data work as huge blockers became huge opportunities:\nWe could now dynamically scale compute on-demand, without upgrading to a larger on-prem database.\nWe could now store and query enormous datasets like clickstream data, without pre-aggregating and transforming it.\nToday, the next wave of innovation is happening in AI and LLMs, and it's coming to the cloud data platforms dbt practitioners are already using every day. For one example, Snowflake have just released their Cortex functions to access LLM-powered tools tuned for running common tasks against your existing datasets. In doing so, there are a new set of opportunities available to us:\nColumn-Level Lineage, Model Performance, and Recommendations: ship trusted data products with dbt Explorer\nWhat\u2019s in a data platform? \u200b\nRaising a dbt project is hard work. We, as data professionals, have poured ourselves into raising happy healthy data products, and we should be proud of the insights they\u2019ve driven. It certainly wasn\u2019t without its challenges though \u2014 we remember the terrible twos, where we worked hard to just get the platform to walk straight. We remember the angsty teenage years where tests kept failing, seemingly just to spite us. A lot of blood, sweat, and tears are shed in the service of clean data!\nOnce the project could dress and feed itself, we also worked hard to get buy-in from our colleagues who put their trust in our little project. Without deep trust and understanding of what we built, our colleagues who depend on your data (or even those involved in developing it with you \u2014 it takes a village after all!) are more likely to be in your DMs with questions than in their BI tools, generating insights.\nWhen our teammates ask about where the data in their reports come from, how fresh it is, or about the right calculation for a metric, what a joy! This means they want to put what we\u2019ve built to good use \u2014 the challenge is that, historically, it hasn\u2019t been all that easy to answer these questions well. That has often meant a manual, painstaking process of cross checking run logs and your dbt documentation site to get the stakeholder the information they need.\nEnter dbt Explorer ! dbt Explorer centralizes documentation, lineage, and execution metadata to reduce the work required to ship trusted data products faster.\nMore time coding, less time waiting: Mastering defer in dbt\nPicture this \u2014 you\u2019ve got a massive dbt project, thousands of models chugging along, creating actionable insights for your stakeholders. A ticket comes your way \u2014 a model needs to be refactored! \"No problem,\" you think to yourself, \"I will simply make that change and test it locally!\" You look at your lineage, and realize this model is many layers deep, buried underneath a long chain of tables and views.\n\u201cOK,\u201d you think further, \u201cI\u2019ll just run a dbt build -s +my_changed_model to make sure I have everything I need built into my dev schema and I can test my changes\u201d. You run the command. You wait. You wait some more. You get some coffee, and completely take yourself out of your dbt development flow state. A lot of time and money down the drain to get to a point where you can start your work. That\u2019s no good!\ndbt build -s +my_changed_model\nLuckily, dbt\u2019s defer functionality allow you to only build what you care about when you need it, and nothing more. This feature helps developers spend less time and money in development, helping ship trusted data products faster. dbt Cloud offers native support for this workflow in development, so you can start deferring without any additional overhead!\nTo defer or to clone, that is the question\nHi all, I\u2019m Kshitij, a senior software engineer on the Core team at dbt Labs.\nOne of the coolest moments of my career here thus far has been shipping the new dbt clone command as part of the dbt-core v1.6 release.\ndbt clone\nHowever, one of the questions I\u2019ve received most frequently is guidance around \u201cwhen\u201d to clone that goes beyond the documentation on \u201chow\u201d to clone .\nIn this blog post, I\u2019ll attempt to provide this guidance by answering these FAQs:\nWhat is dbt clone ?\ndbt clone\nHow is it different from deferral?\nShould I defer or should I clone?\nOptimizing Materialized Views with dbt\nThis blog post was updated on December 18, 2023 to cover the support of MVs on dbt-bigquery\nand updates on how to test MVs.\nIntroduction \u200b\nThe year was 2020. I was a kitten-only household, and dbt Labs was still Fishtown Analytics. A enterprise customer I was working with, Jetblue, asked me for help running their dbt models every 2 minutes to meet a 5 minute SLA.\nAfter getting over the initial terror, we talked through the use case and soon realized there was a better option. Together with my team, I created lambda views to meet the need.\nFlash forward to 2023. I\u2019m writing this as my giant dog snores next to me (don\u2019t worry the cats have multiplied as well). Jetblue has outgrown lambda views due to performance constraints (a view can only be so performant) and we are at another milestone in dbt\u2019s journey to support streaming. What. a. time.\nToday we are announcing that we now support Materialized Views in dbt. So, what does that mean?\nCreate dbt Documentation and Tests 10x faster with ChatGPT\nWhether you are creating your pipelines into dbt for the first time or just adding a new model once in a while, good documentation and testing should always be a priority for you and your team. Why do we avoid it like the plague then? Because it\u2019s a hassle having to write down each individual field, its description in layman terms and figure out what tests should be performed to ensure the data is fine and dandy. How can we make this process faster and less painful?\nBy now, everyone knows the wonders of the GPT models for code generation and pair programming so this shouldn\u2019t come as a surprise. But ChatGPT really shines at inferring the context of verbosely named fields from database table schemas. So in this post I am going to help you 10x your documentation and testing speed by using ChatGPT to do most of the leg work for you.\nData Vault 2.0 with dbt Cloud\nData Vault 2.0 is a data modeling technique designed to help scale large data warehousing projects. It is a rigid, prescriptive system detailed vigorously in a book that has become the bible for this technique.\nSo why Data Vault? Have you experienced a data warehousing project with 50+ data sources, with 25+ data developers working on the same data platform, or data spanning 5+ years with two or more generations of source systems? If not, it might be hard to initially understand the benefits of Data Vault, and maybe Kimball modelling is better for you. But if you are in any of the situations listed, then this is the article for you!\nBuilding a historical user segmentation model with dbt\nIntroduction \u200b\nMost data modeling approaches for customer segmentation are based on a wide table with user attributes. This table only stores the current attributes for each user, and is then loaded into the various SaaS platforms via Reverse ETL tools.\nTake for example a Customer Experience (CX) team that uses Salesforce as a CRM. The users will create tickets to ask for assistance, and the CX team will start attending them in the order that they are created. This is a good first approach, but not a data driven one.\nAn improvement to this would be to prioritize the tickets based on the customer segment, answering our most valuable customers first. An Analytics Engineer can build a segmentation to identify the power users (for example with an RFM approach) and store it in the data warehouse. The Data Engineering team can then export that user attribute to the CRM, allowing the customer experience team to build rules on top of it."
  },
  {
    "url": "https://docs.getdbt.com/blog/tags/data-ecosystem",
    "text": "Data ecosystem\nWalkthroughs of how top data practitioners use tools in the modern data stack.\nThe Components of the dbt Fusion engine and how they fit together\nToday, we announced the dbt Fusion engine .\nFusion isn't just one thing \u2014 it's a set of interconnected components working together to power the next generation of analytics engineering.\nThis post maps out each piece of the Fusion architecture, explains how they fit together, and clarifies what's available to you whether you're compiling from source, using our pre-built binaries, or developing within a dbt Fusion powered product experience.\nFrom the Rust engine to the VS Code extension, through to new Arrow-based adapters and Apache-licensed foundational technologies, we'll break down exactly what each component does, how each component is licensed (for why, see Tristan's accompanying post ), and how you can start using it and get involved today.\nPath to GA: How the dbt Fusion engine rolls out from beta to production\nToday, we announced that the dbt Fusion engine is available in beta .\nIf Fusion works with your project today, great! You're in for a treat \ud83d\ude04\nIf it's your first day using dbt, welcome! You should start on Fusion \u2014 you're in for a treat too.\nToday is Launch Day \u2014\u00a0the first day of a new era: the Age of Fusion. We expect many teams with existing projects will encounter at least one issue that will prevent them from adopting the dbt Fusion engine in production environments. That's ok!\nWe're moving quickly to unblock more teams, and we are committing that by the time Fusion reaches General Availability:\nWe will support Snowflake, Databricks, BigQuery, Redshift\u00a0\u2014\u00a0and likely also Athena, Postgres, Spark, and Trino \u2014 with the new Fusion Adapter pattern .\nWe will have coverage for (basically) all dbt Core functionality. Some things are impractical to replicate outside of Python, or so seldom-used that we'll be more reactive than proactive. On the other hand, many existing dbt Core behaviours will be improved by the unique capabilities of the dbt Fusion engine, such as speed and SQL comprehension. You'll see us talk about this in relevant GitHub issues, many of which we've linked below.\nThe source-available dbt-fusion repository will contain more total functionality than what is available in dbt Core today. ( Read more about this here .)\ndbt-fusion\nThe developer experience will be even speedier and more intuitive.\nThese statements aren't true yet \u2014\u00a0but you can see where we're headed. That's what betas are for, that's the journey we're going on together, and that's why we want to have you all involved.\nMeet the\u202fdbt\u202fFusion Engine: the new Rust-based, industrial-grade engine for dbt\nTL;DR: What You Need to Know \u200b\ndbt\u2019s familiar authoring layer remains unchanged, but the execution engine beneath it is completely new.\nThe new engine is called the dbt Fusion engine \u2014 rewritten from the ground up in Rust based on technology from SDF .  The dbt Fusion engine is substantially faster than dbt Core and has built in SQL comprehension technology to power the next generation of analytics engineering workflows.\nThe dbt Fusion engine is currently in beta. You can try it today if you use Snowflake \u2014 with additional adapters coming starting in early June. Review our path to general availability (GA) and try the quickstart .\nYou do not need to be a dbt Labs customer to use Fusion - dbt Core users can adopt the dbt Fusion engine today for free in your local environment.\nYou can use Fusion with the new dbt VS Code extension , directly via the CLI , or via dbt Studio .\nThis is the beginning of a new era for analytics engineering. For a glimpse into what the Fusion engine is going to enable over the next 1 to 2 years, read this post .\nIntroducing the dbt MCP Server \u2013 Bringing Structured Data to AI Workflows and Agents\ndbt is the standard for creating governed, trustworthy datasets on top of your structured data. MCP is showing increasing promise as the standard for providing context to LLMs to allow them to function at a high level in real world, operational scenarios.\nToday, we are open sourcing an experimental version of the dbt MCP server . We expect that over the coming years, structured data is going to become heavily integrated into AI workflows and that dbt will play a key role in building and provisioning this data.\nParser, Better, Faster, Stronger: A peek at the new dbt engine\nRemember how dbt felt when you had a small project? You pressed enter and stuff just happened immediately? We're bringing that back.\nAfter a series of deep dives into the guts of SQL comprehension , let's talk about speed a little bit. Specifically, I want to talk about one of the most annoying slowdowns as your project grows: project parsing.\nWhen you're waiting a few seconds or a few minutes for things to start happening after you invoke dbt, it's because parsing isn't finished yet. But Lukas' SDF demo at last month's webinar didn't have a big wait, so why not?\nThe key technologies behind SQL Comprehension\nYou ever wonder what\u2019s really going on in your database when you fire off a (perfect, efficient, full-of-insight) SQL query to your database?\nOK, probably not \ud83d\ude05. Your personal tastes aside, we\u2019ve been talking a lot about SQL Comprehension tools at dbt Labs in the wake of our acquisition of SDF Labs, and think that the community would benefit if we included them in the conversation too! We recently published a blog that talked about the different levels of SQL Comprehension tools . If you read that, you may have encountered a few new terms you weren\u2019t super familiar with.\nIn this post, we\u2019ll talk about the technologies that underpin SQL Comprehension tools in more detail. Hopefully, you come away with a deeper understanding of and appreciation for the hard work that your computer does to turn your SQL queries into actionable business insights!\nThe Three Levels of SQL Comprehension: What they are and why you need to know about them\nEver since dbt Labs acquired SDF Labs last week , I've been head-down diving into their technology and making sense of it all. The main thing I knew going in was \"SDF understands SQL\". It's a nice pithy quote, but the specifics are fascinating.\nFor the next era of Analytics Engineering to be as transformative as the last, dbt needs to move beyond being a string preprocessor and into fully comprehending SQL. For the first time, SDF provides the technology necessary to make this possible. Today we're going to dig into what SQL comprehension actually means, since it's so critical to what comes next.\nWhy I wish I had a control plane for my renovation\nWhen my wife and I renovated our home, we chose to take on the role of owner-builder. It was a bold (and mostly naive) decision, but we wanted control over every aspect of the project. What we didn\u2019t realize was just how complex and exhausting managing so many moving parts would be.\nWe had to coordinate multiple elements:\nThe architects , who designed the layout, interior, and exterior.\nThe architectural plans , which outlined what the house should look like.\nThe builders , who executed those plans.\nThe inspectors , councils , and energy raters , who checked whether everything met the required standards.\nPutting Your DAG on the internet\nNew in dbt: allow Snowflake Python models to access the internet \u200b\nWith dbt 1.8, dbt released support for Snowflake\u2019s external access integrations further enabling the use of dbt + AI to enrich your data. This allows querying of external APIs within dbt Python models, a functionality that was required for dbt Cloud customer, EQT AB . Learn about why they needed it and how they helped build the feature and get it shipped!\nLLM-powered Analytics Engineering: How we're using AI inside of our dbt project, today, with no new tools.\nCloud Data Platforms make new things possible; dbt helps you put them into production \u200b\nThe original paradigm shift that enabled dbt to exist and be useful was databases going to the cloud.\nAll of a sudden it was possible for more people to do better data work as huge blockers became huge opportunities:\nWe could now dynamically scale compute on-demand, without upgrading to a larger on-prem database.\nWe could now store and query enormous datasets like clickstream data, without pre-aggregating and transforming it.\nToday, the next wave of innovation is happening in AI and LLMs, and it's coming to the cloud data platforms dbt practitioners are already using every day. For one example, Snowflake have just released their Cortex functions to access LLM-powered tools tuned for running common tasks against your existing datasets. In doing so, there are a new set of opportunities available to us:\nOptimizing Materialized Views with dbt\nThis blog post was updated on December 18, 2023 to cover the support of MVs on dbt-bigquery\nand updates on how to test MVs.\nIntroduction \u200b\nThe year was 2020. I was a kitten-only household, and dbt Labs was still Fishtown Analytics. A enterprise customer I was working with, Jetblue, asked me for help running their dbt models every 2 minutes to meet a 5 minute SLA.\nAfter getting over the initial terror, we talked through the use case and soon realized there was a better option. Together with my team, I created lambda views to meet the need.\nFlash forward to 2023. I\u2019m writing this as my giant dog snores next to me (don\u2019t worry the cats have multiplied as well). Jetblue has outgrown lambda views due to performance constraints (a view can only be so performant) and we are at another milestone in dbt\u2019s journey to support streaming. What. a. time.\nToday we are announcing that we now support Materialized Views in dbt. So, what does that mean?\nCreate dbt Documentation and Tests 10x faster with ChatGPT\nWhether you are creating your pipelines into dbt for the first time or just adding a new model once in a while, good documentation and testing should always be a priority for you and your team. Why do we avoid it like the plague then? Because it\u2019s a hassle having to write down each individual field, its description in layman terms and figure out what tests should be performed to ensure the data is fine and dandy. How can we make this process faster and less painful?\nBy now, everyone knows the wonders of the GPT models for code generation and pair programming so this shouldn\u2019t come as a surprise. But ChatGPT really shines at inferring the context of verbosely named fields from database table schemas. So in this post I am going to help you 10x your documentation and testing speed by using ChatGPT to do most of the leg work for you.\nData Vault 2.0 with dbt Cloud\nData Vault 2.0 is a data modeling technique designed to help scale large data warehousing projects. It is a rigid, prescriptive system detailed vigorously in a book that has become the bible for this technique.\nSo why Data Vault? Have you experienced a data warehousing project with 50+ data sources, with 25+ data developers working on the same data platform, or data spanning 5+ years with two or more generations of source systems? If not, it might be hard to initially understand the benefits of Data Vault, and maybe Kimball modelling is better for you. But if you are in any of the situations listed, then this is the article for you!\nMaking dbt Cloud API calls using dbt-cloud-cli\nThis blog explains how to use the dbt-cloud-cli Python library to create a data catalog app with dbt Cloud artifacts. This is different from the dbt Cloud CLI , a tool that allows you to run dbt commands against your dbt Cloud development environment from your local command line.\ndbt-cloud-cli\ndbt Cloud is a hosted service that many organizations use for their dbt deployments. Among other things, it provides an interface for creating and managing deployment jobs. When triggered (e.g., cron schedule, API trigger), the jobs generate various artifacts that contain valuable metadata related to the dbt project and the run results.\ndbt Cloud provides a REST API for managing jobs, run artifacts and other dbt Cloud resources. Data/analytics engineers would often write custom scripts for issuing automated calls to the API using tools cURL or Python Requests .  In some cases, the engineers would go on and copy/rewrite them between projects that need to interact with the API. Now, they have a bunch of scripts on their hands that they need to maintain and develop further if business requirements change. If only there was a dedicated tool for interacting with the dbt Cloud API that abstracts away the complexities of the API calls behind an easy-to-use interface\u2026 Oh wait, there is: the dbt-cloud-cli !\ndbt + Machine Learning: What makes a great baton pass?\nSpecial Thanks: Emilie Schario, Matt Winkler\ndbt has done a great job of building an elegant, common interface between data engineers, analytics engineers, and any data-y role, by uniting our work on SQL. This unification of tools and workflows creates interoperability between what would normally be distinct teams within the data organization.\nI like to call this interoperability a \u201cbaton pass.\u201d Like in a relay race, there are clear handoff points & explicit ownership at all stages of the process. But there\u2019s one baton pass that\u2019s still relatively painful and undefined: the handoff between machine learning (ML) engineers and analytics engineers.\nIn my experience, the initial collaboration workflow between ML engineering & analytics engineering starts off strong but eventually becomes muddy during the maintenance phase. This eventually leads to projects becoming unusable and forgotten.\nIn this article, we\u2019ll explore a real-life baton pass between ML engineering and analytics engineering and highlighting where things went wrong.\nThe Exact GitHub Pull Request Template We Use at dbt Labs\nHaving a GitHub pull request template is one of the most important and frequently overlooked aspects of creating an efficient and scalable dbt-centric analytics workflow. Opening a pull request is the final step of your modeling process - a process which typically involves a lot of complex work!\nFor you, the dbt developer, the pull request (PR for short) serves as a final checkpoint in your modeling process, ensuring that no key elements are missing from your code or project.\nThe Spiritual Alignment of dbt + Airflow\nAirflow and dbt are often framed as either / or:\nYou either build SQL transformations using Airflow\u2019s SQL database operators (like SnowflakeOperator ), or develop them in a dbt project.\nYou either orchestrate dbt models in Airflow, or you deploy them using dbt Cloud.\nIn my experience, these are false dichotomies, that sound great as hot takes but don\u2019t really help us do our jobs as data people."
  },
  {
    "url": "https://docs.getdbt.com/blog/dbt-fusion-engine-path-to-ga",
    "text": "Path to GA: How the dbt Fusion engine rolls out from beta to production\nToday, we announced that the dbt Fusion engine is available in beta .\nIf Fusion works with your project today, great! You're in for a treat \ud83d\ude04\nIf it's your first day using dbt, welcome! You should start on Fusion \u2014 you're in for a treat too.\nToday is Launch Day \u2014\u00a0the first day of a new era: the Age of Fusion. We expect many teams with existing projects will encounter at least one issue that will prevent them from adopting the dbt Fusion engine in production environments. That's ok!\nWe're moving quickly to unblock more teams, and we are committing that by the time Fusion reaches General Availability:\nWe will support Snowflake, Databricks, BigQuery, Redshift\u00a0\u2014\u00a0and likely also Athena, Postgres, Spark, and Trino \u2014 with the new Fusion Adapter pattern .\nWe will have coverage for (basically) all dbt Core functionality. Some things are impractical to replicate outside of Python, or so seldom-used that we'll be more reactive than proactive. On the other hand, many existing dbt Core behaviours will be improved by the unique capabilities of the dbt Fusion engine, such as speed and SQL comprehension. You'll see us talk about this in relevant GitHub issues, many of which we've linked below.\nThe source-available dbt-fusion repository will contain more total functionality than what is available in dbt Core today. ( Read more about this here .)\ndbt-fusion\nThe developer experience will be even speedier and more intuitive.\nThese statements aren't true yet \u2014\u00a0but you can see where we're headed. That's what betas are for, that's the journey we're going on together, and that's why we want to have you all involved.\nWe will be adding functionality rapidly over the coming weeks. In particular, keep an eye out for Databricks, BigQuery and Redshift support (in that order) in the coming weeks.\nThe most popular dbt Labs packages ( dbt_utils , audit_helper , dbt_external_tables , dbt_project_evaluator ) are already compatible with Fusion. Some external packages may not work out of the box, but we plan to work with package maintainers to get them ready & working on Fusion.\ndbt_utils\naudit_helper\ndbt_external_tables\ndbt_project_evaluator\nSo when is Fusion going to be GA? We're targeting later this year for full feature parity, but we're also hoping to approach it asymptotically\u00a0\u2014\u00a0meaning that many existing dbt users ca start adopting Fusion much sooner.\nDuring the beta period, you may run into unanticipated (and anticipated) issues when trying to run your project on Fusion. Please share any issues in the dbt-fusion repository or on Slack in #dbt-fusion-engine , and we'll do our best to to unblock you.\nCan I use Fusion for my dbt project today? \u200b\nMaybe! The biggest first question: \"Is your adapter supported yet?\" (If not, sit tight, we're working fast!) If so, then it depends on the exact matrix of features you currently use in your dbt project.\nYou may be able to start using Fusion immediately, may need to make (mostly automatic) modifications to your project to resolve deprecations, or your project may not yet be parsable at all:\nState Description Workaround Resolvable by Unblocked You can adopt the dbt Fusion engine with no changes to your project Soft blocked Your project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nDescription Workaround Resolvable by Unblocked You can adopt the dbt Fusion engine with no changes to your project Soft blocked Your project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nWorkaround Resolvable by Unblocked You can adopt the dbt Fusion engine with no changes to your project Soft blocked Your project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nResolvable by Unblocked You can adopt the dbt Fusion engine with no changes to your project Soft blocked Your project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nUnblocked You can adopt the dbt Fusion engine with no changes to your project Soft blocked Your project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nYou can adopt the dbt Fusion engine with no changes to your project Soft blocked Your project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nSoft blocked Your project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nSoft blocked Your project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nSoft blocked Your project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nYour project parses successfully but relies on not-yet-implemented functionality Don't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nDon't invoke unsupported functions or build unsupported models dbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\ndbt Labs Hard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nHard blocked by deprecations Your project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nYour project contains functionality deprecated in dbt Core v1.10 Resolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nResolve deprecations with the dbt-autofix script or workflow in dbt Studio You Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nYou Hard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nHard blocked by known parse issues Your project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nYour project contains Python models or uses a not-yet-supported adapter Temporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nTemporarily remove Python models dbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\ndbt Labs Hard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nHard blocked by unknown parse issues Your project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nYour project is probably doing something surprising with Jinja Create an issue, consider modifying impacted code You & dbt Labs\nCreate an issue, consider modifying impacted code You & dbt Labs\nYou & dbt Labs\nWe're continuously removing blockers to Fusion adoption on a rolling basis during this beta period and in the leadup to a broader release. The rest of this post will go deeper into the four thematic criteria we set out above:\nAdapter coverage\nFeature coverage\nSource-available code publishing\nDeveloper experience improvements\nRequirement for GA: Adapter Coverage \u200b\nDatabricks, BigQuery and Redshift \u200b\ndbt Fusion's adapters are now based on the ADBC standard , a modern, high-performance Apache project optimised for columnar analytical databases.\ndbt Labs has developed new ADBC-compatible drivers (and a supporting framework, XDBC ) to complement the existing, stable Snowflake driver.\nTarget release dates: We expect to add support for Databricks , BigQuery , and Redshift (in that order) in the coming weeks.\nWhere possible, Fusion adapters will support the same authentication methods and connection/credential configurations as dbt Core adapters. We've also heard loud & clear feedback from dbt platform customers who have beta-tested the Fusion CLI \u2014\u00a0we want to figure out a way for Fusion CLI to use connection setup (config/creds) from the platform for local runs ( tracking issue ).\nAthena, Postgres, Spark and Trino \u200b\nWe're aiming to support these adapters later in the year, prior to GA. Check each adapter's tracking issue ( Trino , Athena , Spark , and Postgres ) for specific timelines.\nCustom adapters \u200b\nThe short answer: Fusion's new adapter format could be extended to support community development of third-party adapters, but it's not on the near-term roadmap before GA ( tracking issue ).\nThe longer answer: Fusion now downloads necessary drivers (part of the adapter stack) on-demand. This dynamic linking requires the drivers to be signed by dbt Labs, meaning that we need to have a system in place to review contributions of new drivers and ensure their security.\nIn the meantime, if you want to migrate a supported project to the dbt Fusion engine but have a dependency on another project using a custom adapter, you can use a Hybrid project to have dbt Core execute the unsupported part of the pipeline and then publish artifacts for downstream projects to consume.\nRequirement for GA: Feature coverage \u200b\nFeature coverage includes ensuring documented features work as expected, as well as (where possible) supporting undocumented \"accidental\" features.\nMost of the time, even if your project uses an unimplemented feature, you can still take Fusion for a spin. This is because as long as your project parses, you can just skip unsupported models.\nKnown unimplemented features \u200b\nPython models \u200b\nPython models are the one exception to that \"just skip them\" advice. The dbt Fusion engine does not currently support parsing Python models, which means it can not extract refs or configs inside the files. Instead of potentially building models out of DAG order, we've chosen to not support Python models at all for now . They're coming back though - check out the issue for details.\nBreadth of Materialization Support \u200b\nAs of today we support the most common materializations: table , view , incremental , ephemeral for models \u2014\u00a0plus the materializations underlying snapshots, seeds, and tests. Other native strategies (like microbatch incremental models , iceberg tables , materialized views/dynamic tables , or stored test failures ) as well as custom materializations are on the roadmap \u2014 check their respective issues to see when.\ntable\nview\nincremental\nephemeral\nIt's worth reiterating here: Even if you have models that rely on not-yet-supported materialization strategies, you can still try the dbt Fusion engine in the rest of your project. The rest of your DAG will build as normal, but unsupported strategies will raise an error if they are included in scope of dbt build or dbt run .\ndbt build\ndbt run\nTo exclude those nodes, use a command like\ndbt build --exclude config.materialized:my_custom_mat\ndbt build --exclude config.materialized:my_custom_mat\ndbt build --exclude config.incremental_strategy:microbatch\ndbt build --exclude config.incremental_strategy:microbatch\nOther common features \u200b\nDid you know that there are over 400 documented features of dbt? Doug does, because he had to put them all into a Notion database.\nFusion already supports two-thirds of them, and we have a plan for the rest. You can follow along at the dbt-fusion repo , where there are issues to track the outstanding behaviours. There's also a rough set of milestones attached, but those are subject to reordering as more teams start using Fusion and giving feedback.\ndbt-fusion\nSome of the most relevant ones include:\nExposures\nA new stable logging system\nA new local documentation experience that replaces dbt-docs (!)\nProgrammatic invocations\nModel governance (contracts, constraints, access, deprecation_date)\nA grab bag of CLI commands like dbt clone , state:modified.subselector , --empty , ...\ndbt clone\nstate:modified.subselector\n--empty\nIt's worth noting that resolution doesn't necessarily mean identical behaviours. As a couple of examples:\nMany of these behaviours have not been implemented yet because the Fusion engine introduces new capabilities, above all SQL comprehension, that we will leverage to provide a superior experience. A direct port-over of the feature would miss the point.\nOthers (like the events and logging system) are tightly coupled to dbt Core's Python roots \u2014 they're worth a rethink, and not worth shooting for exact 100% conformance\nHere's a point-in-time snapshot of how we expect to tackle the known remaining work. Please refer to the repository's issues page as the source of truth:\nSurprise unimplemented features \u200b\nDid you know that there are also over a bajillion undocumented features of dbt? Since March, we've been validating the new engine's parser against projects orchestrated by the dbt platform, which has flagged hundreds of divergent behaviours and common parse bugs.\nBut we also know there is a long tail of behaviours that will only arise in the wild, and that the easiest way to get to the bottom of them will be to work with users.\nThis work will be ongoing, alongside feature support. When you start using the Fusion engine, please open an issue if you hit an unexpected error \u2014 and please include a basic project that reproduces the error, so we can fix it!\nRequirement for GA: The Source-available dbt-fusion codebase is better than dbt-core for most use cases \u200b\ndbt-fusion\ndbt-core\nBy GA, the dbt-fusion repository will have the necessary (and fully source-available) components to compile a functional engine for the vast majority of dbt Core projects \u2014\u00a0and a faster one at that. That means that you will always have the ability to compile, use, and modify this code itself, without requiring access to the dbt Labs provided binary (although we think you'll probably just want to use the binary, for reasons detailed in the Components of the dbt Fusion engine post).\ndbt-fusion\nSo far, we've released the code necessary to self-compile a dbt binary that can run dbt deps and dbt parse . Throughout the beta period we will continue to prepare more code for use by those who want to view, contribute to, or modify the code for their own purposes, including what's necessary for the rest of the commands to work.\ndbt deps\ndbt parse\nBeyond just the code necessary to produce a complete dbt binary, we've also committed to open-sourcing several of the underlying library components (such as dbt-jinja, dbt-serde-yaml, and the grammars necessary to produce a high-performance SQL parser). Again, check out the Components of the dbt Fusion engine post for the details.\nSome behaviours that worked in dbt Core won't have an equivalent in this new codebase. The most obvious examples are those which depended on the vagaries of Python: arbitrary callbacks on the EventManager (there's no longer an EventManager on which to register a callback!), the experimental plugins system (dynamic loading of binaries works completely differently in Rust and would require signing), or the dbt templater in SQLFluff (which hooked into dbt Core beyond the exposed interfaces - although we plan to build a fast linter ourselves ).\nRequirement for GA: The DX rocks \u200b\nMore speed \u200b\nInvocations powered by the dbt Fusion engine are already significantly faster than the same invocation in dbt Core, but there's more to do here! We know that there is still a lot of low-hanging fruit, and by GA we expect to see tasks like full project compilation complete at least twice as fast for many projects.\nIf you do some benchmarking, we're particularly interested in any situations where Fusion \"pauses\" on a single file for a couple of seconds. Some other things to keep in mind:\nWriting very large manifests is pretty slow, no matter what. Try including --no-write-json . We're wondering whether it makes sense to have a trimmed-down manifest by default. What do you think?\n--no-write-json\nThe dbt compile command involves more work in Fusion than in dbt Core, because it's doing full SQL validation. To compare just the SQL rendering step (the equivalent of dbt Core's compile command), you can try turning off static analysis with the CLI flag --static-analysis off .\ndbt compile\ncompile\n--static-analysis off\nAs a sign of what's possible, take note of the incremental recompilation used to provide real-time feedback in the VS Code extension.\nA more info-dense console output \u200b\nWhile we were preparing for the beta release, we kept the Fusion CLI output intentionally verbose \u2014 it displays everything that's happening, which means errors and warnings can be pushed out of view by other status updates. We're already in the process of clearing this up a bit , and we've got some funny ideas about the possibility of progress bars. However we do it, the goal should be that you see the log lines about things that need attention, and not much more.\nYour idea here \u200b\nWhat feels off when you're using dbt Fusion? Tell us all about it \u2014 if you've got a clear idea for what's wrong and what it should be instead, feel free to jump straight to a GitHub issue. Bonus points if you've got a minimal repro project.\nIf you need to kick an idea around before opening an issue, we'll also be actively checking in on #dbt-fusion-engine (for high-level discussions) and #dbt-fusion-engine-migration (to get into the weeds of a specific bug) on Slack.\nFrom now until Fusion is GA, we will be prioritizing parity with existing framework features, not adding new ones. Once we hit GA, we'll think about whether to transfer existing feature requests from the dbt-core repo to dbt-fusion \u2014 or maybe a third place? \u2014 stay tuned.\ndbt-core\ndbt-fusion\nFollowing along \u200b\nThe path to GA for Fusion is a Community-wide effort. We want to hear from you, work with you, get your ideas and feedback. Whether it is sharing a bug report, an idea for a feature or more high level thoughts and feedback, we're looking to engage with you.\nIn Slack, we're on #dbt-fusion-engine and #dbt-fusion-engine-migration\nThe GitHub repo is https://github.com/dbt-labs/dbt-fusion\nThere are a couple of dozen dbt World Circuit meetups happening globally during June: https://www.meetup.com/pro/dbt/ . (Jeremy will be speaking in Paris, Marseille, and Boston \u2014\u00a0come hang out!)\nWe'll be having regular office hours with a revolving cast of characters from the Developer Experience, Engineering, and Product teams. Dates will be circulated in the #dbt-fusion-engine channel.\nComments"
  },
  {
    "url": "https://docs.getdbt.com/blog/dbt-fusion-engine",
    "text": "Meet the\u202fdbt\u202fFusion Engine: the new Rust-based, industrial-grade engine for dbt\nTL;DR: What You Need to Know \u200b\ndbt\u2019s familiar authoring layer remains unchanged, but the execution engine beneath it is completely new.\nThe new engine is called the dbt Fusion engine \u2014 rewritten from the ground up in Rust based on technology from SDF .  The dbt Fusion engine is substantially faster than dbt Core and has built in SQL comprehension technology to power the next generation of analytics engineering workflows.\nThe dbt Fusion engine is currently in beta. You can try it today if you use Snowflake \u2014 with additional adapters coming starting in early June. Review our path to general availability (GA) and try the quickstart .\nYou do not need to be a dbt Labs customer to use Fusion - dbt Core users can adopt the dbt Fusion engine today for free in your local environment.\nYou can use Fusion with the new dbt VS Code extension , directly via the CLI , or via dbt Studio .\nThis is the beginning of a new era for analytics engineering. For a glimpse into what the Fusion engine is going to enable over the next 1 to 2 years, read this post .\nSince its introduction in 2016, dbt has paved the way for the analytics engineering revolution. Teams worldwide have moved from ad hoc processes running customized SQL scripts into a mature analytics workflow based on the dbt viewpoint . dbt enables data practitioners to work like software engineers , building their analytics code as an asset to ship trusted data products faster.\ndbt came to represent many things:\nA viewpoint on how analytics should be done\nA workflow where data practitioners could put that viewpoint into action\nA framework \u2014 dbt Core \u2014 that powered this workflow comprised of: An authoring layer: The schema, spec, and definitions for a dbt project written in SQL, YML, and Jinja An engine: The tooling via which the authoring layer was built and executed against a data platform, resolving templated code into executable SQL, building your dependency graph, and more.\nAn authoring layer: The schema, spec, and definitions for a dbt project written in SQL, YML, and Jinja\nAn engine: The tooling via which the authoring layer was built and executed against a data platform, resolving templated code into executable SQL, building your dependency graph, and more.\nWhile the authoring layer has continued to evolve nicely, giving dbt developers ever-more functionality to work with, the engine itself, dbt Core, is still built on the same technology and uses the same primary design principles that it started with in 2016. This causes two primary problems that cannot be iteratively solved:\ndbt Core can be slow .  It\u2019s built in Python and for larger dbt projects it can become unworkable. Even for smaller projects, to power a great developer experience, users would need a step change in performance.\nThe dbt engine renders SQL, but it doesn\u2019t comprehend SQL. That means that any functionality relying on specifics of SQL code was impossible to build into dbt.\nAnd so it became clear that for us to power the analytics workloads of tomorrow, we weren't going to get there with incremental improvements \u2014\u00a0we needed to rebuild the dbt engine from scratch . We needed:\nAn engine built for speed.\nAn engine that knows about your code.\nAn engine that powers the next generation of developer experience.\nAnd that engine is Fusion.\nWhat exactly is Fusion? \u200b\nFusion is the new engine for dbt.\nIf the authoring layer is \"what\" your dbt project is supposed to do, then the engine is the \"how.\" That includes:\nRendering Jinja\nBuilding dependency graphs\nCreating artifact files\nCommunicating with databases\nAt first glance, Fusion looks a lot like dbt Core. Your projects are built using the familiar dbt authoring layer. You still write SQL and Jinja. You still type dbt run . (To make it easier to try Fusion, we're also shipping with an optional dbtf alias, as many users have the dbt namespace already specified).\ndbt run\ndbtf\ndbt\nBut underneath that is a layer of technical depth and rigor that is entirely new to dbt, happening at the engine layer.\nFusion:\nIs fully rewritten in Rust, enabling a dramatically faster dbt experience . Fusion does not depend on Python at all. In fact, besides the adapter macros, not a single line of code is shared between dbt Core and the dbt Fusion engine. (For long-time dbt spelunkers, we've described the new structure in a separate post .)\nUnderstands your SQL code. It\u2019s a true SQL compiler and gives dbt a full view on what the code in your dbt project means and how it will propagate across your entire data lineage.\nBased on the technology from SDF , Fusion represents a step change increase in the technical capabilities of dbt.\nAs a result of these capabilities, Fusion can deliver new experiences. Some of these we\u2019re releasing today, like real-time error detection in VS Code and significant cost savings in project execution.  dbt now knows about your code!\nYou probably now know enough now to head on over to the quickstart and get going , but if you want to know little more about what Fusion delivers today, keep reading.\nNear-term benefits of adopting Fusion \u200b\nYou can think of Fusion as the same dbt you know and love, but better and faster, and you're going to see it show up in a lot of places!\nSo how and why should you adopt Fusion for your dbt project?\nJust the new Fusion-powered dbt CLI \u200b\nSignificant performance improvements: Up to 30x faster parsing and 2x quicker full-project compilation, with near-instant recompilation of single files in the VS Code Extension. We expect continued performance gains as part of the path to GA.\nThe new Fusion-powered dbt Fusion CLI + VS Code extension \u200b\nBut the real benefit of Fusion is not just going to be in the CLI itself \u2014 it\u2019s in the ability to build net new product experiences that leverage Fusion\u2019s capabilities. The first of these, unveiled today, is the VS Code extension, powered by dbt Fusion\u2019s SQL Comprehension . This extension could only be built on Fusion:\nIt\u2019s fast \u2014 the VS Code extension recompiles your entire dbt project in the background every time you save any file, as well as identifying errors instantly for the active file. For that to be workable, it needs to happen fast.\nIt understand SQL and functions as a compiler \u2014 it knows what columns exist in your project, what functions you are using and the type signature and output of those functions.\nThere\u2019s a whole host of features in the VS Code extension. Some early favorites:\nWrite code with confidence \u2014 live error detection and function autocomplete. How many time have you hit dbt run only to realize that you typed select * frmo , misspelled a column name or tried to sum the unsummable? No more! With the LSP-powered VS Code extension, you can immediately see when pesky errors sneak into your code. You wouldn't sum a datetime. Similarly \u2014 is it dateadd or date_add ? And which way around do the arguments go again? Just start typing and you'll see contextual prompts and autocomplete.\nWrite code with confidence \u2014 live error detection and function autocomplete.\nHow many time have you hit dbt run only to realize that you typed select * frmo , misspelled a column name or tried to sum the unsummable? No more! With the LSP-powered VS Code extension, you can immediately see when pesky errors sneak into your code. You wouldn't sum a datetime.\nHow many time have you hit dbt run only to realize that you typed select * frmo , misspelled a column name or tried to sum the unsummable? No more! With the LSP-powered VS Code extension, you can immediately see when pesky errors sneak into your code.\ndbt run\nselect * frmo\nSimilarly \u2014 is it dateadd or date_add ? And which way around do the arguments go again? Just start typing and you'll see contextual prompts and autocomplete.\nSimilarly \u2014 is it dateadd or date_add ? And which way around do the arguments go again? Just start typing and you'll see contextual prompts and autocomplete.\ndateadd\ndate_add\nSee how the code you\u2019ve written iteratively progresses to your transformed data: Preview CTEs and viewing compiled code Because the VS Code extension compiles your code every time you save, you can view the compiled code from your project in real time as you\u2019re making edits. This is a real lifesaver when working on complex macros. Writing your code with CTEs allows you to modularly split up the logic in your model. The days when you swap out the final CTE at the end for the name of the CTE you're debugging are no more, now you can just click.\nSee how the code you\u2019ve written iteratively progresses to your transformed data: Preview CTEs and viewing compiled code\nBecause the VS Code extension compiles your code every time you save, you can view the compiled code from your project in real time as you\u2019re making edits. This is a real lifesaver when working on complex macros.\nWriting your code with CTEs allows you to modularly split up the logic in your model. The days when you swap out the final CTE at the end for the name of the CTE you're debugging are no more, now you can just click.\nfinal\nTraverse your project: Go-to-reference and built in lineage Need to find out how an upstream model was defined? Or where all the inputs from the model you\u2019re working on came from? With both the ability to jump to the model and column references and view model and column level lineage, it\u2019s honestly a night and day difference.\nTraverse your project: Go-to-reference and built in lineage\nNeed to find out how an upstream model was defined? Or where all the inputs from the model you\u2019re working on came from? With both the ability to jump to the model and column references and view model and column level lineage, it\u2019s honestly a night and day difference.\nI could go on and on and on \u2014 there\u2019s so much here.\nTaken separately, these range from quality of life improvements to significant changes.\nBut taken together, it actually fundamentally changes the experience of writing your dbt code. There were just so many things that you had to constantly be juggling in the back of your head that are now offloaded to the extension. The sum change to the experience of writing dbt code... is exceptional. I already can\u2019t imagine working without this.\nOf course \u2014 there\u2019s another technology changing the experience of writing dbt (and all) code \u2014 AI. The functionality that Fusion enables dovetails perfectly with AI-assisted coding by allowing you to vet, validate, and comprehend AI-generated code more easily. Moving forward, expect even tighter coupling between Fusion and AI-based coding assistants as the speed and rigor of Fusion will help produce higher quality AI-generated code.\nThe VS Code extension is one of our first product experiences exclusively powered by the dbt Fusion engine. The extension depends on the Language Server, and the Language Server depends on Fusion's SQL comprehension capabilities. We made the decision not to support dbt Core for the VS Code Extension because existing community-built extensions have already built as much as is possible on top of dbt Core's foundation.  To get to this next level of experience, we needed Fusion.\nHow to get started with Fusion \u200b\nThe dbt Fusion engine is currently in beta. We've written a separate post describing the path to Fusion's final release, and how you can see if your project is compatible today.\nWhether or not you can move your existing project to Fusion today, you can jump into the VS Code extension using our quickstart to try get a feeling for what's ahead.\ndbt customers: Over the coming weeks, in projects eligible to start using Fusion, you\u2019ll see a toggle in your account or receive a message from your account team. From there, you can activate Fusion for your environments .\nTo use the VS Code extension: Install the \"dbt\" extension directly from the marketplace for automated setup and head to the quickstart. This will also automatically install the Fusion-powered CLI for you.\nTo use the dbt CLI powered by Fusion: Simply install Fusion\nIf you are looking to migrate an existing project to Fusion, see the migration guide \u2014\u00a0as well as the dbt-autofix helper, which automatically addresses many of the changes needed to migrate to Fusion.\ndbt-autofix\nWhat's Next? \u200b\nToday\u2019s launch is the start. There is much left to do over the short term and long term.\nMoving forward we\u2019re building many net new products and evolutions of our current products that simply wouldn\u2019t have been possible in a pre-Fusion world. This will be particularly impactful for powering AI workflows, both to assist in the creation of high quality dbt projects and serving as the trusted interface to structured data for AI agents.\nWe\u2019re excited to work with the Community on the evolution of Fusion. If you\u2019ve heard talk about the early days of the dbt Community and wished you could have been around for it, you now have the opportunity to make the deep, foundational impact that is often only possible at the start of a new technical innovation cycle.\nSo get involved!\nTry out the Fusion quickstart\nOpen up a GitHub issue in dbt-fusion to report a bug or participate in the path to GA\ndbt-fusion\nJoin us on Slack in #dbt-fusion-engine and share your thoughts or questions\nHead to an in-person dbt Meetup \u2014 we\u2019re hosting the dbt World Circuit \ud83c\udfce\ufe0f\u00a0around the world where you can and come talk to one of us about Fusion!\nComments"
  },
  {
    "url": "https://docs.getdbt.com/docs/fusion/about-fusion",
    "text": "About the dbt Fusion engine beta\ndbt is the industry standard for data transformation. The dbt Fusion engine enables dbt to operate at speed and scale like never before. info The dbt Fusion engine is currently in beta and the related documentation is a work in progress. The information on this page will evolve as features are added and enhanced. Join the conversation in our Community Slack channel #dbt-fusion-engine . The dbt Fusion engine shares the same familiar framework for authoring data transformations as dbt Core , while enabling data developers to work faster and deploy transformation workloads more efficiently. What is Fusion \u200b Fusion is an entirely new piece of software, written in a different programming language (Rust) than dbt Core (Python). Fusion is significantly faster than dbt Core , and it has a native understanding of SQL across multiple engine dialects. Fusion will eventually support the full dbt Core framework, a superset of dbt Core\u2019s capabilities, and the vast majority of existing dbt projects. Fusion contains mixture of source-available, proprietary, and open source code. That means: dbt Labs publishes much of the source code in the dbt-fusion repository , where you can read the code and participate in community discussions. Some Fusion capabilities are exclusively available for paying customers of the cloud-based dbt platform . Refer to supported features for more information. Read more about the licensing for the dbt Fusion engine here . Why use Fusion \u200b As a developer, Fusion can: Immediately catch incorrect SQL in your dbt models Preview inline CTEs for faster debugging Trace model and column definitions across your dbt project All of that and more is available in the dbt extension for VSCode , with Fusion at the foundation. Fusion also enables more-efficient deployments of large DAGs. By tracking which columns are used where, and which source tables have fresh data, Fusion can ensure that models are rebuilt only when they need to process new data. This \"state-aware orchestration\" is a feature of the dbt platform. How to use Fusion \u200b You can: Select Fusion from the dropdown/toggle in the dbt platform Install the dbt extension for VSCode Install the Fusion CLI Go straight to the Quickstart to feel the Fusion as fast as possible. What's next? \u200b dbt Labs launched the dbt Fusion engine as a public beta on May 28, 2025, with plans to reach full feature parity with dbt Core ahead of Fusion's general availability . More information about Fusion \u200b Fusion marks a significant update to dbt. While many of the workflows you've grown accustomed to remain unchanged, there are a lot of new ideas, and a lot of old ones going away. The following is a list of the full scope of our current release of the Fusion engine, including implementation, installation, deprecations, and limitations: About the dbt Fusion engine About the dbt extension New concepts in Fusion Supported features matrix Installing Fusion CLI Installing VS Code extension Fusion release track Quickstart for Fusion Upgrade guide Fusion licensing\ndbt is the industry standard for data transformation. The dbt Fusion engine enables dbt to operate at speed and scale like never before. info The dbt Fusion engine is currently in beta and the related documentation is a work in progress. The information on this page will evolve as features are added and enhanced. Join the conversation in our Community Slack channel #dbt-fusion-engine . The dbt Fusion engine shares the same familiar framework for authoring data transformations as dbt Core , while enabling data developers to work faster and deploy transformation workloads more efficiently. What is Fusion \u200b Fusion is an entirely new piece of software, written in a different programming language (Rust) than dbt Core (Python). Fusion is significantly faster than dbt Core , and it has a native understanding of SQL across multiple engine dialects. Fusion will eventually support the full dbt Core framework, a superset of dbt Core\u2019s capabilities, and the vast majority of existing dbt projects. Fusion contains mixture of source-available, proprietary, and open source code. That means: dbt Labs publishes much of the source code in the dbt-fusion repository , where you can read the code and participate in community discussions. Some Fusion capabilities are exclusively available for paying customers of the cloud-based dbt platform . Refer to supported features for more information. Read more about the licensing for the dbt Fusion engine here . Why use Fusion \u200b As a developer, Fusion can: Immediately catch incorrect SQL in your dbt models Preview inline CTEs for faster debugging Trace model and column definitions across your dbt project All of that and more is available in the dbt extension for VSCode , with Fusion at the foundation. Fusion also enables more-efficient deployments of large DAGs. By tracking which columns are used where, and which source tables have fresh data, Fusion can ensure that models are rebuilt only when they need to process new data. This \"state-aware orchestration\" is a feature of the dbt platform. How to use Fusion \u200b You can: Select Fusion from the dropdown/toggle in the dbt platform Install the dbt extension for VSCode Install the Fusion CLI Go straight to the Quickstart to feel the Fusion as fast as possible. What's next? \u200b dbt Labs launched the dbt Fusion engine as a public beta on May 28, 2025, with plans to reach full feature parity with dbt Core ahead of Fusion's general availability . More information about Fusion \u200b Fusion marks a significant update to dbt. While many of the workflows you've grown accustomed to remain unchanged, there are a lot of new ideas, and a lot of old ones going away. The following is a list of the full scope of our current release of the Fusion engine, including implementation, installation, deprecations, and limitations: About the dbt Fusion engine About the dbt extension New concepts in Fusion Supported features matrix Installing Fusion CLI Installing VS Code extension Fusion release track Quickstart for Fusion Upgrade guide Fusion licensing\nThe dbt Fusion engine is currently in beta and the related documentation is a work in progress. The information on this page will evolve as features are added and enhanced. Join the conversation in our Community Slack channel #dbt-fusion-engine .\n#dbt-fusion-engine\nThe dbt Fusion engine shares the same familiar framework for authoring data transformations as dbt Core , while enabling data developers to work faster and deploy transformation workloads more efficiently.\nWhat is Fusion \u200b\nFusion is an entirely new piece of software, written in a different programming language (Rust) than dbt Core (Python). Fusion is significantly faster than dbt Core , and it has a native understanding of SQL across multiple engine dialects. Fusion will eventually support the full dbt Core framework, a superset of dbt Core\u2019s capabilities, and the vast majority of existing dbt projects.\nFusion contains mixture of source-available, proprietary, and open source code. That means:\ndbt Labs publishes much of the source code in the dbt-fusion repository , where you can read the code and participate in community discussions.\ndbt-fusion\nSome Fusion capabilities are exclusively available for paying customers of the cloud-based dbt platform . Refer to supported features for more information.\nRead more about the licensing for the dbt Fusion engine here .\nWhy use Fusion \u200b\nAs a developer, Fusion can:\nImmediately catch incorrect SQL in your dbt models\nPreview inline CTEs for faster debugging\nTrace model and column definitions across your dbt project\nAll of that and more is available in the dbt extension for VSCode , with Fusion at the foundation.\nFusion also enables more-efficient deployments of large DAGs. By tracking which columns are used where, and which source tables have fresh data, Fusion can ensure that models are rebuilt only when they need to process new data. This \"state-aware orchestration\" is a feature of the dbt platform.\nHow to use Fusion \u200b\nYou can:\nSelect Fusion from the dropdown/toggle in the dbt platform\nInstall the dbt extension for VSCode\nInstall the Fusion CLI\nGo straight to the Quickstart to feel the Fusion as fast as possible.\nWhat's next? \u200b\ndbt Labs launched the dbt Fusion engine as a public beta on May 28, 2025, with plans to reach full feature parity with dbt Core ahead of Fusion's general availability .\nMore information about Fusion \u200b\nFusion marks a significant update to dbt. While many of the workflows you've grown accustomed to remain unchanged, there are a lot of new ideas, and a lot of old ones going away. The following is a list of the full scope of our current release of the Fusion engine, including implementation, installation, deprecations, and limitations:\nAbout the dbt Fusion engine\nAbout the dbt extension\nNew concepts in Fusion\nSupported features matrix\nInstalling Fusion CLI\nInstalling VS Code extension\nFusion release track\nQuickstart for Fusion\nUpgrade guide\nFusion licensing\nWhat is Fusion Why use Fusion How to use Fusion What's next? More information about Fusion\nWhy use Fusion How to use Fusion What's next? More information about Fusion\nHow to use Fusion\nWhat's next? More information about Fusion\nMore information about Fusion"
  },
  {
    "url": "https://docs.getdbt.com/docs/get-started-dbt",
    "text": "dbt Quickstarts\nBegin your dbt journey by trying one of our quickstarts, which provides a step-by-step guide to help you set up dbt or dbt Core with a variety of data platforms .\nthe dbt platform \u200b\ndbt is a scalable solution that enables you to develop, test, deploy, and explore data products using a single, fully managed software service. It enables teams with diverse skills to build reliable data products at any scale, with capabilities including:\nDevelopment experiences tailored to multiple personas (in-browser Studio IDE or locally with the dbt CLI )\nOut-of-the-box CI/CD workflows\nThe Semantic Layer for consistent metrics that can be delivered to any endpoint\nDomain ownership of data with multi-project Mesh setups\nCatalog for collaborative data discovery and understanding\nLearn more about dbt features and start your free trial today.\nQuickstart for dbt and Amazon Athena\nQuickstart for dbt and Azure Synapse Analytics\nQuickstart for dbt and BigQuery\nQuickstart for dbt and Databricks\nQuickstart for dbt and Microsoft Fabric\nQuickstart for dbt and Redshift\nQuickstart for dbt and Snowflake\nQuickstart for dbt and Starburst Galaxy\nQuickstart for dbt and Teradata\ndbt Core \u200b\ndbt Core is a command-line open-source tool that enables data practitioners to transform data using analytics engineering best practices. It suits individuals and small technical teams who prefer manual setup and customization, supports community adapters, and open-source standards.\ndbt Core from a manual install\nQuickstart for dbt Core using DuckDB\nRelated docs \u200b\nExpand your dbt knowledge and expertise with these additional resources:\nJoin the monthly demos to see dbt in action and ask questions.\ndbt AWS marketplace contains information on how to deploy dbt on AWS, user reviews, and more.\nBest practices contains information on how dbt Labs approaches building projects through our current viewpoints on structure, style, and setup.\ndbt Learn offers free online courses that cover dbt fundamentals, advanced topics, and more.\nJoin the dbt Community to learn how other data practitioners globally are using dbt, share your own experiences, and get help with your dbt projects."
  },
  {
    "url": "https://docs.getdbt.com/docs/supported-data-platforms",
    "text": "Supported data platforms\ndbt connects to and runs SQL against your database, warehouse, lake, or query engine. These SQL-speaking platforms are collectively referred to as data platforms . dbt connects with data platforms by using a dedicated adapter plugin for each. Plugins are built as Python modules that dbt Core discovers if they are installed on your system. Refer to the Build, test, document, and promote adapters guide for details.\nYou can connect to adapters and data platforms natively in dbt or install them manually using dbt Core .\nYou can also further customize how dbt works with your specific data platform via configuration: see Configuring Postgres for an example.\nTypes of Adapters \u200b\nThere are two types of adapters available today:\nTrusted \u2014 Trusted adapters are those where the adapter maintainers have decided to participate in the Trusted Adapter Program and have made a commitment to meeting those requirements. For adapters supported in dbt , maintainers have undergone an additional rigorous process that covers contractual requirements for development, documentation, user experience, and maintenance.\nCommunity \u2014 Community adapters are open-source and maintained by community members. These adapters are not part of the Trusted Adapter Program and could have usage inconsistencies.\nDoes it work?\nDoes anyone \"own\" the code, or is anyone liable for ensuring it works?\nDo bugs get fixed quickly?\nDoes it stay up-to-date with new dbt Core features?\nIs the usage substantial enough to self-sustain?\nDo other known projects depend on this library?"
  },
  {
    "url": "https://docs.getdbt.com/docs/cloud/about-cloud/dbt-cloud-features",
    "text": "The dbt platform features\ndbt is the fastest and most reliable way to deploy dbt. Develop, test, schedule, document, and investigate data models all in one browser-based UI.\nIn addition to providing a hosted architecture for running dbt across your organization, dbt comes equipped with turnkey support for scheduling jobs, CI/CD, hosting documentation, monitoring and alerting, an integrated development environment ( Studio IDE ), and allows you to develop and run dbt commands from your local command line interface (CLI) or code editor.\ndbt 's flexible plans and features make it well-suited for data teams of any size \u2014 sign up for your free 14-day trial !\ndbt CLI\ndbt Studio IDE\ndbt Canvas\ndbt Copilot*\nManage environments\nSchedule and run dbt jobs\nNotifications\nRun visibility\nHost & share documentation\nSupports GitHub, GitLab, AzureDevOps\nEnable Continuous Integration\nSecurity\nVisualize and orchestrate exposures*\ndbt Semantic Layer*\nDiscovery API*\ndbt Catalog*\ndbt Insights*\n*These features are available on selected plans .\nRelated docs \u200b\ndbt plans and pricing\nQuickstart guides\nStudio IDE"
  },
  {
    "url": "https://docs.getdbt.com/docs/about-setup",
    "text": "About dbt setup\ndbt compiles and runs your analytics code against your data platform, enabling you and your team to collaborate on a single source of truth for metrics, insights, and business definitions. There are two options for deploying dbt:\ndbt runs dbt Core in a hosted (single or multi-tenant) environment with a browser-based interface. The intuitive user interface aids you in setting up the various components. dbt comes equipped with turnkey support for scheduling jobs, CI/CD, hosting documentation, monitoring, and alerting. It also offers an integrated development environment ( Studio IDE ) and allows you to develop and run dbt commands from your local command line (CLI) or code editor.\ndbt Core is an open-source command line tool that can be installed locally in your environment, and communication with databases is facilitated through adapters.\nIf you're not sure which is the right solution for you, read our What is dbt? and our dbt features articles to help you decide. If you still have questions, don't hesitate to contact us .\nTo begin configuring dbt now, select the option that is right for you.\ndbt platform setup\ndbt Core setup"
  },
  {
    "url": "https://docs.getdbt.com/docs/cloud/about-develop-dbt",
    "text": "About developing in dbt\nDevelop dbt projects using the dbt platform, a faster and more reliable way to deploy dbt and manage your project in a single, web-based UI. You can develop in your browser using a dbt -powered command line interface (CLI), an integrated development environment ( Studio IDE ), or Canvas . dbt CLI Allows you to develop and run dbt commands from your local command line or code editor against your dbt development environment. dbt Studio IDE Develop directly in your browser, making dbt project development efficient by compiling code into SQL and managing project changes seamlessly using an intuitive user interface. dbt Canvas Develop with Canvas, a seamless drag-and-drop experience that helps analysts quickly create and visualize dbt models in dbt. To get started, you'll need a dbt account and a developer seat. For a more comprehensive guide about developing in dbt, refer to the quickstart guides .\nYou can develop in your browser using a dbt -powered command line interface (CLI), an integrated development environment ( Studio IDE ), or Canvas .\ndbt CLI\ndbt Studio IDE\ndbt Canvas\nTo get started, you'll need a dbt account and a developer seat. For a more comprehensive guide about developing in dbt, refer to the quickstart guides ."
  },
  {
    "url": "https://docs.getdbt.com/docs/build/projects",
    "text": "About dbt projects\nA dbt project informs dbt about the context of your project and how to transform your data (build your data sets). By design, dbt enforces the top-level structure of a dbt project such as the dbt_project.yml file, the models directory, the snapshots directory, and so on. Within the directories of the top-level, you can organize your project in any way that meets the needs of your organization and data pipeline.\ndbt_project.yml\nmodels\nsnapshots\nAt a minimum, all a project needs is the dbt_project.yml project configuration file. dbt supports a number of different resources, so a project may also include:\ndbt_project.yml\nResource Description models Each model lives in a single file and contains logic that either transforms raw data into a dataset that is ready for analytics or, more often, is an intermediate step in such a transformation. snapshots A way to capture the state of your mutable tables so you can refer to it later. seeds CSV files with static data that you can load into your data platform with dbt. data tests SQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nDescription models Each model lives in a single file and contains logic that either transforms raw data into a dataset that is ready for analytics or, more often, is an intermediate step in such a transformation. snapshots A way to capture the state of your mutable tables so you can refer to it later. seeds CSV files with static data that you can load into your data platform with dbt. data tests SQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nmodels Each model lives in a single file and contains logic that either transforms raw data into a dataset that is ready for analytics or, more often, is an intermediate step in such a transformation. snapshots A way to capture the state of your mutable tables so you can refer to it later. seeds CSV files with static data that you can load into your data platform with dbt. data tests SQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nEach model lives in a single file and contains logic that either transforms raw data into a dataset that is ready for analytics or, more often, is an intermediate step in such a transformation. snapshots A way to capture the state of your mutable tables so you can refer to it later. seeds CSV files with static data that you can load into your data platform with dbt. data tests SQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nsnapshots A way to capture the state of your mutable tables so you can refer to it later. seeds CSV files with static data that you can load into your data platform with dbt. data tests SQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nA way to capture the state of your mutable tables so you can refer to it later. seeds CSV files with static data that you can load into your data platform with dbt. data tests SQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nseeds CSV files with static data that you can load into your data platform with dbt. data tests SQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nCSV files with static data that you can load into your data platform with dbt. data tests SQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\ndata tests SQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nSQL queries that you can write to test the models and resources in your project. macros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nmacros Blocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nBlocks of code that you can reuse multiple times. docs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\ndocs Docs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nDocs for your project that you can build. sources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nsources A way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nA way to name and describe the data loaded into your warehouse by your Extract and Load tools. exposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nexposures A way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nA way to define and describe a downstream use of your project. metrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nmetrics A way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nA way for you to define metrics for your project. groups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\ngroups Groups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nGroups enable collaborative node organization in restricted collections. analysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nanalysis A way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nA way to organize analytical SQL queries in your project such as the general ledger from your QuickBooks. semantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nsemantic models Semantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nSemantic models define the foundational data relationships in MetricFlow and the Semantic Layer , enabling you to query metrics using a semantic graph. saved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nsaved queries Saved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nSaved queries organize reusable queries by grouping metrics, dimensions, and filters into nodes visible in the dbt DAG.\nWhen building out the structure of your project, you should consider these impacts on your organization's workflow:\nHow would people run dbt commands \u2014 Selecting a path\nHow would people navigate within the project \u2014 Whether as developers in the Studio IDE or stakeholders from the docs\nHow would people configure the models \u2014 Some bulk configurations are easier done at the directory level so people don\u2019t have to remember to do everything in a config block with each new model\nProject configuration \u200b\nEvery dbt project includes a project configuration file called dbt_project.yml . It defines the directory of the dbt project and other project configurations.\ndbt_project.yml\nEdit dbt_project.yml to set up common project configurations such as:\ndbt_project.yml\nYAML key Value description name Your project\u2019s name in snake case version Version of your project require-dbt-version Restrict your project to only work with a range of dbt Core versions profile The profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nValue description name Your project\u2019s name in snake case version Version of your project require-dbt-version Restrict your project to only work with a range of dbt Core versions profile The profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nname Your project\u2019s name in snake case version Version of your project require-dbt-version Restrict your project to only work with a range of dbt Core versions profile The profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nYour project\u2019s name in snake case version Version of your project require-dbt-version Restrict your project to only work with a range of dbt Core versions profile The profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nversion Version of your project require-dbt-version Restrict your project to only work with a range of dbt Core versions profile The profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nVersion of your project require-dbt-version Restrict your project to only work with a range of dbt Core versions profile The profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nrequire-dbt-version Restrict your project to only work with a range of dbt Core versions profile The profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nRestrict your project to only work with a range of dbt Core versions profile The profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nprofile The profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nThe profile dbt uses to connect to your data platform model-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nmodel-paths Directories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nDirectories to where your model and source files live seed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nseed-paths Directories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nDirectories to where your seed files live test-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\ntest-paths Directories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nDirectories to where your test files live analysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nanalysis-paths Directories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nDirectories to where your analyses live macro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nmacro-paths Directories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nDirectories to where your macros live snapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nsnapshot-paths Directories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nDirectories to where your snapshots live docs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\ndocs-paths Directories to where your docs blocks live vars Project variables you want to use for data compilation\nDirectories to where your docs blocks live vars Project variables you want to use for data compilation\nvars Project variables you want to use for data compilation\nProject variables you want to use for data compilation\nFor complete details on project configurations, see dbt_project.yml .\nProject subdirectories \u200b\nYou can use the Project subdirectory option in dbt to specify a subdirectory in your git repository that dbt should use as the root directory for your project. This is helpful when you have multiple dbt projects in one repository or when you want to organize your dbt project files into subdirectories for easier management.\nTo use the Project subdirectory option in dbt , follow these steps:\nClick your account name in the bottom left and select Your profile .\nClick your account name in the bottom left and select Your profile .\nUnder Projects , select the project you want to configure as a project subdirectory.\nUnder Projects , select the project you want to configure as a project subdirectory.\nSelect Edit on the lower right-hand corner of the page.\nSelect Edit on the lower right-hand corner of the page.\nIn the Project subdirectory field, add the name of the subdirectory. For example, if your dbt project files are located in a subdirectory called <repository>/finance , you would enter finance as the subdirectory. You can also reference nested subdirectories. For example, if your dbt project files are located in <repository>/teams/finance , you would enter teams/finance as the subdirectory. Note : You do not need a leading or trailing / in the Project subdirectory field.\nIn the Project subdirectory field, add the name of the subdirectory. For example, if your dbt project files are located in a subdirectory called <repository>/finance , you would enter finance as the subdirectory.\n<repository>/finance\nfinance\nYou can also reference nested subdirectories. For example, if your dbt project files are located in <repository>/teams/finance , you would enter teams/finance as the subdirectory. Note : You do not need a leading or trailing / in the Project subdirectory field.\n<repository>/teams/finance\nteams/finance\n/\nClick Save when you've finished.\nClick Save when you've finished.\nAfter configuring the Project subdirectory option, dbt will use it as the root directory for your dbt project. This means that dbt commands, such as dbt run or dbt test , will operate on files within the specified subdirectory. If there is no dbt_project.yml file in the Project subdirectory, you will be prompted to initialize the dbt project.\ndbt run\ndbt test\ndbt_project.yml\nSome plans support only one dbt project, while Enterprise-tier plans allow multiple projects and cross-project references with Mesh .\nNew projects \u200b\nYou can create new projects and share them with other people by making them available on a hosted git repository like GitHub, GitLab, and BitBucket.\nAfter you set up a connection with your data platform, you can initialize your new project in dbt and start developing. Or, run dbt init from the command line to set up your new project.\nDuring project initialization, dbt creates sample model files in your project directory to help you start developing quickly.\nSample projects \u200b\nIf you want to explore dbt projects more in-depth, you can clone dbt Lab\u2019s Jaffle shop on GitHub. It's a runnable project that contains sample configurations and helpful notes.\nIf you want to see what a mature, production project looks like, check out the GitLab Data Team public repo .\nRelated docs \u200b\nBest practices: How we structure our dbt projects\nQuickstarts for dbt\nQuickstart for dbt Core\nProject configuration Project subdirectories New projects Sample projects Related docs\nProject subdirectories New projects Sample projects Related docs\nNew projects Sample projects Related docs\nSample projects Related docs\nRelated docs"
  },
  {
    "url": "https://docs.getdbt.com/docs/mesh/about-mesh",
    "text": "About dbt Mesh\nOrganizations of all sizes rely upon dbt to manage their data transformations, from small startups to large enterprises. At scale, it can be challenging to coordinate all the organizational and technical requirements demanded by your stakeholders within the scope of a single dbt project.\nTo date, there also hasn't been a first-class way to effectively manage the dependencies, governance, and workflows between multiple dbt projects.\nThat's where Mesh comes in - empowering data teams to work independently and collaboratively ; sharing data, code, and best practices without sacrificing security or autonomy.\nMesh is not a single product - it is a pattern enabled by a convergence of several features in dbt:\nCross-project references - this is the foundational feature that enables the multi-project deployments. {{ ref() }} s now work across dbt projects on Enterprise and Enterprise+ plans.\n{{ ref() }}\nCatalog - dbt 's metadata-powered documentation platform, complete with full, cross-project lineage.\nGovernance - dbt's governance features allow you to manage access to your dbt models both within and across projects. Groups - With groups, you can organize nodes in your dbt DAG that share a logical connection (for example, by functional area) and assign an owner to the entire group. Access - access configs allow you to control who can reference models. Model Versions - when coordinating across projects and teams, we recommend treating your data models as stable APIs. Model versioning is the mechanism to allow graceful adoption and deprecation of models as they evolve. Model Contracts - data contracts set explicit expectations on the shape of the data to ensure data changes upstream of dbt or within a project's logic don't break downstream consumers' data products.\nGroups - With groups, you can organize nodes in your dbt DAG that share a logical connection (for example, by functional area) and assign an owner to the entire group.\nAccess - access configs allow you to control who can reference models.\nModel Versions - when coordinating across projects and teams, we recommend treating your data models as stable APIs. Model versioning is the mechanism to allow graceful adoption and deprecation of models as they evolve.\nModel Contracts - data contracts set explicit expectations on the shape of the data to ensure data changes upstream of dbt or within a project's logic don't break downstream consumers' data products.\nWhen is the right time to use dbt Mesh? \u200b\nThe multi-project architecture helps organizations with mature, complex transformation workflows in dbt increase the flexibility and performance of their dbt projects. If you're already using dbt and your project has started to experience any of the following, you're likely ready to start exploring this paradigm:\nThe number of models in your project is degrading performance and slowing down development.\nTeams have developed separate workflows and need to decouple development from each other.\nTeams are experiencing communication challenges , and the reliability of some of your data products has started to deteriorate.\nSecurity and governance requirements are increasing and would benefit from increased isolation.\ndbt is designed to coordinate the features above and simplify the complexity to solve for these problems.\nIf you're just starting your dbt journey, don't worry about building a multi-project architecture right away. You can incrementally adopt the features as you scale. The collection of features work effectively as independent tools. Familiarizing yourself with the tooling and features that make up a multi-project architecture, and how they can apply to your organization will help you make better decisions as you grow.\nFor additional information, refer to the Mesh FAQs .\nWhen is the right time to use dbt Mesh?"
  },
  {
    "url": "https://docs.getdbt.com/docs/deploy/deployments",
    "text": "Deploy dbt\nUse dbt 's capabilities to seamlessly run a dbt job in production or staging environments. Rather than run dbt commands manually from the command line, you can leverage the dbt 's in-app scheduling to automate how and when you execute dbt. The dbt platform offers the easiest and most reliable way to run your dbt project in production. Effortlessly promote high quality code from development to production and build fresh data assets that your business intelligence tools and end users query to make business decisions. Deploying with dbt lets you: Keep production data fresh on a timely basis Ensure CI and production pipelines are efficient Identify the root cause of failures in deployment environments Maintain high-quality code and data in production Gain visibility into the health of deployment jobs, models, and tests Uses exports to write saved queries in your data platform for reliable and fast metric reporting Visualize and orchestrate downstream exposures to understand how models are used in downstream tools and proactively refresh the underlying data sources during scheduled dbt jobs. Enterprise Enterprise + Use dbt 's Git repository caching to protect against third-party outages and improve job run reliability. Enterprise Enterprise + Use Hybrid projects to upload dbt artifacts into the dbt platform for central visibility, cross-project referencing, and easier collaboration. beta Enterprise + Before continuing, make sure you understand dbt's approach to deployment environments . Learn how to use dbt 's features to help your team ship timely and quality production data more easily. Deploy with dbt \u200b Job scheduler The job scheduler is the backbone of running jobs in the dbt platform, bringing power and simplicity to building data pipelines in both continuous integration and production environments. Deploy jobs Create and schedule jobs for the job scheduler to run. Runs on a schedule, by API, or after another job completes. State-aware orchestration Intelligently determines which models to build by detecting changes in code or data at each job run. Continuous integration Set up CI checks so you can build and test any modified code in a staging environment when you open PRs and push new commits to your dbt repository. Continuous deployment Set up merge jobs to ensure the latest code changes are always in production when pull requests are merged to your Git repository. Job commands Configure which dbt commands to execute when running a dbt job. Monitor jobs and alerts \u200b Visualize and orchestrate exposures Learn how to use dbt to automatically generate downstream exposures from dashboards and proactively refresh the underlying data sources during scheduled dbt jobs. Artifacts dbt generates and saves artifacts for your project, which it uses to power features like creating docs for your project and reporting the freshness of your sources. Job notifications Receive email or Slack channel notifications when a job run succeeds, fails, or is canceled so you can respond quickly and begin remediation if necessary. Model notifications Receive email notifications in real time about issues encountered by your models and tests while a job is running. Run visibility View the history of your runs and the model timing dashboard to help identify where improvements can be made to the scheduled jobs. Retry jobs Rerun your errored jobs from start or the failure point. Source freshness Enable snapshots to capture the freshness of your data sources and configure how frequent these snapshots should be taken. This can help you determine whether your source data freshness is meeting your SLAs. Webhooks Create outbound webhooks to send events about your dbt jobs' statuses to other systems in your organization. Hybrid projects beta Enterprise \u200b Hybrid projects Use Hybrid projects to upload dbt Core artifacts into the dbt platform for central visibility, cross-project referencing, and easier collaboration. Related docs \u200b Use exports to materialize saved queries Integrate with other orchestration tools\nUse dbt 's capabilities to seamlessly run a dbt job in production or staging environments. Rather than run dbt commands manually from the command line, you can leverage the dbt 's in-app scheduling to automate how and when you execute dbt. The dbt platform offers the easiest and most reliable way to run your dbt project in production. Effortlessly promote high quality code from development to production and build fresh data assets that your business intelligence tools and end users query to make business decisions. Deploying with dbt lets you: Keep production data fresh on a timely basis Ensure CI and production pipelines are efficient Identify the root cause of failures in deployment environments Maintain high-quality code and data in production Gain visibility into the health of deployment jobs, models, and tests Uses exports to write saved queries in your data platform for reliable and fast metric reporting Visualize and orchestrate downstream exposures to understand how models are used in downstream tools and proactively refresh the underlying data sources during scheduled dbt jobs. Enterprise Enterprise + Use dbt 's Git repository caching to protect against third-party outages and improve job run reliability. Enterprise Enterprise + Use Hybrid projects to upload dbt artifacts into the dbt platform for central visibility, cross-project referencing, and easier collaboration. beta Enterprise + Before continuing, make sure you understand dbt's approach to deployment environments . Learn how to use dbt 's features to help your team ship timely and quality production data more easily. Deploy with dbt \u200b Job scheduler The job scheduler is the backbone of running jobs in the dbt platform, bringing power and simplicity to building data pipelines in both continuous integration and production environments. Deploy jobs Create and schedule jobs for the job scheduler to run. Runs on a schedule, by API, or after another job completes. State-aware orchestration Intelligently determines which models to build by detecting changes in code or data at each job run. Continuous integration Set up CI checks so you can build and test any modified code in a staging environment when you open PRs and push new commits to your dbt repository. Continuous deployment Set up merge jobs to ensure the latest code changes are always in production when pull requests are merged to your Git repository. Job commands Configure which dbt commands to execute when running a dbt job. Monitor jobs and alerts \u200b Visualize and orchestrate exposures Learn how to use dbt to automatically generate downstream exposures from dashboards and proactively refresh the underlying data sources during scheduled dbt jobs. Artifacts dbt generates and saves artifacts for your project, which it uses to power features like creating docs for your project and reporting the freshness of your sources. Job notifications Receive email or Slack channel notifications when a job run succeeds, fails, or is canceled so you can respond quickly and begin remediation if necessary. Model notifications Receive email notifications in real time about issues encountered by your models and tests while a job is running. Run visibility View the history of your runs and the model timing dashboard to help identify where improvements can be made to the scheduled jobs. Retry jobs Rerun your errored jobs from start or the failure point. Source freshness Enable snapshots to capture the freshness of your data sources and configure how frequent these snapshots should be taken. This can help you determine whether your source data freshness is meeting your SLAs. Webhooks Create outbound webhooks to send events about your dbt jobs' statuses to other systems in your organization. Hybrid projects beta Enterprise \u200b Hybrid projects Use Hybrid projects to upload dbt Core artifacts into the dbt platform for central visibility, cross-project referencing, and easier collaboration. Related docs \u200b Use exports to materialize saved queries Integrate with other orchestration tools\nThe dbt platform offers the easiest and most reliable way to run your dbt project in production. Effortlessly promote high quality code from development to production and build fresh data assets that your business intelligence tools and end users query to make business decisions. Deploying with dbt lets you:\nKeep production data fresh on a timely basis\nEnsure CI and production pipelines are efficient\nIdentify the root cause of failures in deployment environments\nMaintain high-quality code and data in production\nGain visibility into the health of deployment jobs, models, and tests\nUses exports to write saved queries in your data platform for reliable and fast metric reporting\nVisualize and orchestrate downstream exposures to understand how models are used in downstream tools and proactively refresh the underlying data sources during scheduled dbt jobs. Enterprise Enterprise +\nUse dbt 's Git repository caching to protect against third-party outages and improve job run reliability. Enterprise Enterprise +\nUse Hybrid projects to upload dbt artifacts into the dbt platform for central visibility, cross-project referencing, and easier collaboration. beta Enterprise +\nBefore continuing, make sure you understand dbt's approach to deployment environments .\nLearn how to use dbt 's features to help your team ship timely and quality production data more easily.\nDeploy with dbt \u200b\nJob scheduler\nDeploy jobs\nState-aware orchestration\nContinuous integration\nContinuous deployment\nJob commands\nMonitor jobs and alerts \u200b\nVisualize and orchestrate exposures\nArtifacts\nJob notifications\nModel notifications\nRun visibility\nRetry jobs\nSource freshness\nWebhooks\nHybrid projects beta Enterprise \u200b\nHybrid projects\nRelated docs \u200b\nUse exports to materialize saved queries\nIntegrate with other orchestration tools"
  },
  {
    "url": "https://docs.getdbt.com/docs/explore/explore-your-data",
    "text": "Explore your data\ndbt provides a variety of tools for you to explore your data, models, and other resources. Many of the features you'd traditionally use your data warehouse services to explore are at your fingertips in your dbt account.\ndbt Catalog\ndbt Insights\nDocumentation\nSome features are only available on selected plans .\nRelated docs \u200b\ndbt plans and pricing\nQuickstart guides\nReference material"
  },
  {
    "url": "https://docs.getdbt.com/docs/use-dbt-semantic-layer/dbt-sl",
    "text": "dbt Semantic Layer Starter Enterprise Enterprise +\nThe dbt Semantic Layer eliminates duplicate coding by allowing data teams to define metrics on top of existing models and automatically handling data joins. The dbt Semantic Layer, powered by MetricFlow , simplifies the process of defining and using critical business metrics, like revenue in the modeling layer (your dbt project). By centralizing metric definitions, data teams can ensure consistent self-service access to these metrics in downstream data tools and applications. Moving metric definitions out of the BI layer and into the modeling layer allows data teams to feel confident that different business units are working from the same metric definitions, regardless of their tool of choice. If a metric definition changes in dbt, it\u2019s refreshed everywhere it\u2019s invoked and creates consistency across all applications. To ensure secure access control, the Semantic Layer implements robust access permissions mechanisms. Refer to the Semantic Layer FAQs or Why we need a universal semantic layer blog post to learn more. Get started with the dbt Semantic Layer \u200b To define and query metrics with the dbt Semantic Layer , you must be on a dbt Starter or Enterprise-tier account. Suitable for both Multi-tenant and Single-tenant accounts. Note: Single-tenant accounts should contact their account representative for necessary setup and enablement. This page points to various resources available to help you understand, configure, deploy, and integrate the Semantic Layer . The following sections contain links to specific pages that explain each aspect in detail. Use these links to navigate directly to the information you need, whether you're setting up the Semantic Layer for the first time, deploying metrics, or integrating with downstream tools. Refer to the following resources to get started with the Semantic Layer : Quickstart with the Semantic Layer \u2014 Build and define metrics, set up the Semantic Layer , and query them using our first-class integrations. Semantic Layer FAQs \u2014 Discover answers to frequently asked questions about the Semantic Layer , such as availability, integrations, and more. Configure the dbt Semantic Layer \u200b The following resources provide information on how to configure the Semantic Layer : Set up the Semantic Layer \u2014 Learn how to set up the Semantic Layer in dbt using intuitive navigation. Architecture \u2014 Explore the powerful components that make up the Semantic Layer . Deploy metrics \u200b This section provides information on how to deploy the Semantic Layer and materialize your metrics: Deploy your Semantic Layer \u2014 Run a dbt job to deploy the Semantic Layer and materialize your metrics. Write queries with exports \u2014 Use exports to write commonly used queries directly within your data platform, on a schedule. Cache common queries \u2014 Leverage result caching and declarative caching for common queries to speed up performance and reduce query computation. Consume metrics and integrate \u200b Consume metrics and integrate the Semantic Layer with downstream tools and applications: Consume metrics \u2014 Query and consume metrics in downstream tools and applications using the Semantic Layer . Available integrations \u2014 Review a wide range of partners you can integrate and query with the Semantic Layer . Semantic Layer APIs \u2014 Use the Semantic Layer APIs to query metrics in downstream tools for consistent, reliable data metrics.\nThe dbt Semantic Layer eliminates duplicate coding by allowing data teams to define metrics on top of existing models and automatically handling data joins. The dbt Semantic Layer, powered by MetricFlow , simplifies the process of defining and using critical business metrics, like revenue in the modeling layer (your dbt project). By centralizing metric definitions, data teams can ensure consistent self-service access to these metrics in downstream data tools and applications. Moving metric definitions out of the BI layer and into the modeling layer allows data teams to feel confident that different business units are working from the same metric definitions, regardless of their tool of choice. If a metric definition changes in dbt, it\u2019s refreshed everywhere it\u2019s invoked and creates consistency across all applications. To ensure secure access control, the Semantic Layer implements robust access permissions mechanisms. Refer to the Semantic Layer FAQs or Why we need a universal semantic layer blog post to learn more. Get started with the dbt Semantic Layer \u200b To define and query metrics with the dbt Semantic Layer , you must be on a dbt Starter or Enterprise-tier account. Suitable for both Multi-tenant and Single-tenant accounts. Note: Single-tenant accounts should contact their account representative for necessary setup and enablement. This page points to various resources available to help you understand, configure, deploy, and integrate the Semantic Layer . The following sections contain links to specific pages that explain each aspect in detail. Use these links to navigate directly to the information you need, whether you're setting up the Semantic Layer for the first time, deploying metrics, or integrating with downstream tools. Refer to the following resources to get started with the Semantic Layer : Quickstart with the Semantic Layer \u2014 Build and define metrics, set up the Semantic Layer , and query them using our first-class integrations. Semantic Layer FAQs \u2014 Discover answers to frequently asked questions about the Semantic Layer , such as availability, integrations, and more. Configure the dbt Semantic Layer \u200b The following resources provide information on how to configure the Semantic Layer : Set up the Semantic Layer \u2014 Learn how to set up the Semantic Layer in dbt using intuitive navigation. Architecture \u2014 Explore the powerful components that make up the Semantic Layer . Deploy metrics \u200b This section provides information on how to deploy the Semantic Layer and materialize your metrics: Deploy your Semantic Layer \u2014 Run a dbt job to deploy the Semantic Layer and materialize your metrics. Write queries with exports \u2014 Use exports to write commonly used queries directly within your data platform, on a schedule. Cache common queries \u2014 Leverage result caching and declarative caching for common queries to speed up performance and reduce query computation. Consume metrics and integrate \u200b Consume metrics and integrate the Semantic Layer with downstream tools and applications: Consume metrics \u2014 Query and consume metrics in downstream tools and applications using the Semantic Layer . Available integrations \u2014 Review a wide range of partners you can integrate and query with the Semantic Layer . Semantic Layer APIs \u2014 Use the Semantic Layer APIs to query metrics in downstream tools for consistent, reliable data metrics.\nThe dbt Semantic Layer, powered by MetricFlow , simplifies the process of defining and using critical business metrics, like revenue in the modeling layer (your dbt project). By centralizing metric definitions, data teams can ensure consistent self-service access to these metrics in downstream data tools and applications.\nrevenue\nMoving metric definitions out of the BI layer and into the modeling layer allows data teams to feel confident that different business units are working from the same metric definitions, regardless of their tool of choice. If a metric definition changes in dbt, it\u2019s refreshed everywhere it\u2019s invoked and creates consistency across all applications. To ensure secure access control, the Semantic Layer implements robust access permissions mechanisms.\nRefer to the Semantic Layer FAQs or Why we need a universal semantic layer blog post to learn more.\nGet started with the dbt Semantic Layer \u200b\nThis page points to various resources available to help you understand, configure, deploy, and integrate the Semantic Layer . The following sections contain links to specific pages that explain each aspect in detail. Use these links to navigate directly to the information you need, whether you're setting up the Semantic Layer for the first time, deploying metrics, or integrating with downstream tools.\nRefer to the following resources to get started with the Semantic Layer :\nQuickstart with the Semantic Layer \u2014 Build and define metrics, set up the Semantic Layer , and query them using our first-class integrations.\nSemantic Layer FAQs \u2014 Discover answers to frequently asked questions about the Semantic Layer , such as availability, integrations, and more.\nConfigure the dbt Semantic Layer \u200b\nThe following resources provide information on how to configure the Semantic Layer :\nSet up the Semantic Layer \u2014 Learn how to set up the Semantic Layer in dbt using intuitive navigation.\nArchitecture \u2014 Explore the powerful components that make up the Semantic Layer .\nDeploy metrics \u200b\nThis section provides information on how to deploy the Semantic Layer and materialize your metrics:\nDeploy your Semantic Layer \u2014 Run a dbt job to deploy the Semantic Layer and materialize your metrics.\nWrite queries with exports \u2014 Use exports to write commonly used queries directly within your data platform, on a schedule.\nCache common queries \u2014 Leverage result caching and declarative caching for common queries to speed up performance and reduce query computation.\nConsume metrics and integrate \u200b\nConsume metrics and integrate the Semantic Layer with downstream tools and applications:\nConsume metrics \u2014 Query and consume metrics in downstream tools and applications using the Semantic Layer .\nAvailable integrations \u2014 Review a wide range of partners you can integrate and query with the Semantic Layer .\nSemantic Layer APIs \u2014 Use the Semantic Layer APIs to query metrics in downstream tools for consistent, reliable data metrics.\nGet started with the dbt Semantic Layer Configure the dbt Semantic Layer Deploy metrics Consume metrics and integrate\nConfigure the dbt Semantic Layer Deploy metrics Consume metrics and integrate\nDeploy metrics Consume metrics and integrate\nConsume metrics and integrate"
  },
  {
    "url": "https://docs.getdbt.com/docs/cloud/dbt-copilot",
    "text": "About dbt Copilot Starter Enterprise Enterprise +\nCopilot is a powerful, AI-powered assistant fully integrated into your dbt experience\u2014designed to accelerate your analytics workflows. Copilot embeds AI-driven assistance across every stage of the analytics development life cycle (ADLC) and harnesses rich metadata\u2014capturing relationships, lineage, and context  \u2014 so you can deliver refined, trusted data products at speed. With automatic code generation and using natural language prompts, Copilot can generate code , documentation , tests , metrics , and semantic models for you with the click of a button in the Studio IDE , Canvas , and Insights . tip Copilot is available on Starter, Enterprise, and Enterprise+ accounts. Book a demo to see how AI-driven development can streamline your workflow. Example of using dbt Copilot to generate documentation in the IDE How dbt Copilot works \u200b Copilot enhances efficiency by automating repetitive tasks while ensuring data privacy and security. It works as follows: Access Copilot through: The Studio IDE to generate documentation, tests, semantic models. The Canvas (beta) to generate SQL code using natural language prompts. Enterprise Enterprise + The Insights to generate SQL queries for analysis using natural language prompts. Enterprise Enterprise + Copilot gathers metadata (like column names, model SQL, documentation) but never accesses row-level warehouse data. The metadata and user prompts are sent to the AI provider (in this case, OpenAI) through API calls for processing. The AI-generated content is returned to dbt for you to review, edit, and save within your project files. Copilot does not use warehouse data to train AI models. No sensitive data persists on dbt Labs' systems, except for usage data. Client data, including any personal or sensitive data inserted into the query by the user, is deleted within 30 days by OpenAI. Copilot uses a best practice style guide to ensure consistency across teams. Copilot is optimized for OpenAI's gpt-3.x , gpt-4o , gpt-4.1-[mini|nano] , and gpt-4.5 (deprecated by OpenAI) models. Other models, like o1 and o2 , are not supported and will not work with Copilot . tip Copilot accelerates, but doesn\u2019t replace, your analytics engineer. It helps deliver better data products faster, but always review AI-generated content, as it may be incorrect.\nCopilot is a powerful, AI-powered assistant fully integrated into your dbt experience\u2014designed to accelerate your analytics workflows. Copilot embeds AI-driven assistance across every stage of the analytics development life cycle (ADLC) and harnesses rich metadata\u2014capturing relationships, lineage, and context  \u2014 so you can deliver refined, trusted data products at speed. With automatic code generation and using natural language prompts, Copilot can generate code , documentation , tests , metrics , and semantic models for you with the click of a button in the Studio IDE , Canvas , and Insights . tip Copilot is available on Starter, Enterprise, and Enterprise+ accounts. Book a demo to see how AI-driven development can streamline your workflow. Example of using dbt Copilot to generate documentation in the IDE How dbt Copilot works \u200b Copilot enhances efficiency by automating repetitive tasks while ensuring data privacy and security. It works as follows: Access Copilot through: The Studio IDE to generate documentation, tests, semantic models. The Canvas (beta) to generate SQL code using natural language prompts. Enterprise Enterprise + The Insights to generate SQL queries for analysis using natural language prompts. Enterprise Enterprise + Copilot gathers metadata (like column names, model SQL, documentation) but never accesses row-level warehouse data. The metadata and user prompts are sent to the AI provider (in this case, OpenAI) through API calls for processing. The AI-generated content is returned to dbt for you to review, edit, and save within your project files. Copilot does not use warehouse data to train AI models. No sensitive data persists on dbt Labs' systems, except for usage data. Client data, including any personal or sensitive data inserted into the query by the user, is deleted within 30 days by OpenAI. Copilot uses a best practice style guide to ensure consistency across teams. Copilot is optimized for OpenAI's gpt-3.x , gpt-4o , gpt-4.1-[mini|nano] , and gpt-4.5 (deprecated by OpenAI) models. Other models, like o1 and o2 , are not supported and will not work with Copilot . tip Copilot accelerates, but doesn\u2019t replace, your analytics engineer. It helps deliver better data products faster, but always review AI-generated content, as it may be incorrect.\nCopilot embeds AI-driven assistance across every stage of the analytics development life cycle (ADLC) and harnesses rich metadata\u2014capturing relationships, lineage, and context  \u2014 so you can deliver refined, trusted data products at speed.\nWith automatic code generation and using natural language prompts, Copilot can generate code , documentation , tests , metrics , and semantic models for you with the click of a button in the Studio IDE , Canvas , and Insights .\nCopilot is available on Starter, Enterprise, and Enterprise+ accounts. Book a demo to see how AI-driven development can streamline your workflow.\nHow dbt Copilot works \u200b\nCopilot enhances efficiency by automating repetitive tasks while ensuring data privacy and security. It works as follows:\nAccess Copilot through: The Studio IDE to generate documentation, tests, semantic models. The Canvas (beta) to generate SQL code using natural language prompts. Enterprise Enterprise + The Insights to generate SQL queries for analysis using natural language prompts. Enterprise Enterprise +\nThe Studio IDE to generate documentation, tests, semantic models.\nThe Canvas (beta) to generate SQL code using natural language prompts. Enterprise Enterprise +\nThe Insights to generate SQL queries for analysis using natural language prompts. Enterprise Enterprise +\nCopilot gathers metadata (like column names, model SQL, documentation) but never accesses row-level warehouse data.\nThe metadata and user prompts are sent to the AI provider (in this case, OpenAI) through API calls for processing.\nThe AI-generated content is returned to dbt for you to review, edit, and save within your project files.\nCopilot does not use warehouse data to train AI models.\nNo sensitive data persists on dbt Labs' systems, except for usage data.\nClient data, including any personal or sensitive data inserted into the query by the user, is deleted within 30 days by OpenAI.\nCopilot uses a best practice style guide to ensure consistency across teams.\nCopilot is optimized for OpenAI's gpt-3.x , gpt-4o , gpt-4.1-[mini|nano] , and gpt-4.5 (deprecated by OpenAI) models. Other models, like o1 and o2 , are not supported and will not work with Copilot .\ngpt-3.x\ngpt-4o\ngpt-4.1-[mini|nano]\ngpt-4.5\no1\no2\nCopilot accelerates, but doesn\u2019t replace, your analytics engineer. It helps deliver better data products faster, but always review AI-generated content, as it may be incorrect.\nHow dbt Copilot works"
  },
  {
    "url": "https://docs.getdbt.com/docs/cloud-integrations/overview",
    "text": "About dbt integrations\nMany data applications integrate with dbt , enabling you to leverage the power of dbt for a variety of use cases and workflows.\nIntegrations with dbt \u200b\nVisualize and orchestrate downstream exposures\ndbt Snowflake Native App (preview)\ndbt Semantic layer integrations"
  },
  {
    "url": "https://docs.getdbt.com/docs/dbt-versions/core",
    "text": "About dbt Core versions\ndbt Core releases follow semantic versioning guidelines. For more on how we use semantic versions, see How dbt Core uses semantic versioning .\nDid you know that you can always be working with the latest features and functionality? With dbt , you can get early access to new functionality before it becomes available in dbt Core and without the need of managing your own version upgrades. Refer to the \"Latest\" Release Track setting for details.\nWith dbt , you can get early access to new functionality before it becomes available in dbt Core and without the need of managing your own version upgrades. Refer to the \"Latest\" Release Track setting for details.\ndbt Labs provides different support levels for different versions, which may include new features, bug fixes, or security patches:\nActive \u2014 We will patch regressions, new bugs, and include fixes for older bugs / quality-of-life improvements. We implement these changes when we have high confidence that they're narrowly scoped and won't cause unintended side effects.\nCritical \u2014 Newer minor versions transition the previous minor version into \"Critical Support\" with limited \"security\" releases for critical security and installation fixes.\nEnd of Life \u2014 Minor versions that have reached EOL no longer receive new patch releases.\nDeprecated \u2014 dbt Core versions older than v1.0 are no longer maintained by dbt Labs, nor supported in dbt platform .\nLatest releases \u200b\ndbt Core Initial release Support level and end date v1.10 Jun 16, 2025 Active Support \u2014 Jun 15, 2026 v1.9 Dec 9, 2024 Active Support \u2014 Dec 8, 2025 v1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nInitial release Support level and end date v1.10 Jun 16, 2025 Active Support \u2014 Jun 15, 2026 v1.9 Dec 9, 2024 Active Support \u2014 Dec 8, 2025 v1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nSupport level and end date v1.10 Jun 16, 2025 Active Support \u2014 Jun 15, 2026 v1.9 Dec 9, 2024 Active Support \u2014 Dec 8, 2025 v1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.10 Jun 16, 2025 Active Support \u2014 Jun 15, 2026 v1.9 Dec 9, 2024 Active Support \u2014 Dec 8, 2025 v1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nJun 16, 2025 Active Support \u2014 Jun 15, 2026 v1.9 Dec 9, 2024 Active Support \u2014 Dec 8, 2025 v1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nActive Support \u2014 Jun 15, 2026 v1.9 Dec 9, 2024 Active Support \u2014 Dec 8, 2025 v1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.9 Dec 9, 2024 Active Support \u2014 Dec 8, 2025 v1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nDec 9, 2024 Active Support \u2014 Dec 8, 2025 v1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nActive Support \u2014 Dec 8, 2025 v1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.8 May 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nMay 9, 2024 End of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nEnd of Life \u26a0\ufe0f v1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.7 Nov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nNov 2, 2023 End of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nEnd of Life \u26a0\ufe0f v1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.6 Jul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nJul 31, 2023 End of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nEnd of Life \u26a0\ufe0f v1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.5 Apr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nApr 27, 2023 End of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nEnd of Life \u26a0\ufe0f v1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.4 Jan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nJan 25, 2023 End of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nEnd of Life \u26a0\ufe0f v1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.3 Oct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nOct 12, 2022 End of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nEnd of Life \u26a0\ufe0f v1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.2 Jul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nJul 26, 2022 Deprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nDeprecated \u26d4\ufe0f v1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.1 Apr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nApr 28, 2022 Deprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nDeprecated \u26d4\ufe0f v1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv1.0 Dec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nDec 3, 2021 Deprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nDeprecated \u26d4\ufe0f v0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\nv0.X \u26d4\ufe0f (Various dates) Deprecated \u26d4\ufe0f\n(Various dates) Deprecated \u26d4\ufe0f\nDeprecated \u26d4\ufe0f\nAll functionality in dbt Core since the v1.7 release is available in dbt release tracks , which provide automated upgrades at a cadence appropriate for your team.\n1 Release tracks are required for the Developer and Starter plans on dbt . Accounts using older dbt versions will be migrated to the \"Latest\" release track.\nFor customers of dbt : dbt Labs strongly recommends migrating environments on older and unsupported versions to release tracks or a supported version. In 2025, dbt Labs will remove the oldest dbt Core versions from availability in dbt platform , starting with v1.0 -- v1.2.\nFurther reading \u200b\nTo learn how you can use dbt Core versions in dbt , see Choosing a dbt Core version .\nTo learn about installing dbt Core , see \" How to install dbt Core .\"\nTo restrict your project to only work with a range of dbt Core versions, or use the currently running dbt Core version, see require-dbt-version and dbt_version .\nrequire-dbt-version\ndbt_version\nVersion support prior to v1.0 \u200b\nAll dbt Core versions released prior to 1.0 and their version-specific documentation have been deprecated. If upgrading to a currently supported version, reference our best practices for upgrading\nEOL version support \u200b\nAll dbt Core minor versions that have reached end-of-life (EOL) will have no new patch releases. This means they will no longer receive any fixes, including for known bugs that have been identified. Fixes for those bugs will instead be made in newer minor versions that are still under active support.\nWe recommend upgrading to a newer version in dbt or dbt Core to continue receiving support.\nAll dbt Core v1.0 and later are available in dbt until further notice. In the future, we intend to align dbt availability with dbt Core ongoing support. You will receive plenty of advance notice before any changes take place.\nCurrent version support \u200b\nMinor versions \u200b\nMinor versions include new features and capabilities. They will be supported for one year from their initial release date. dbt Labs is committed to this 12-month support timeframe. Our mechanism for continuing to support a minor version is by releasing new patches: small, targeted bug fixes. Whenever we refer to a minor version, such as v1.0, we always mean its latest available patch release (v1.0.x).\nWhile a minor version is officially supported:\nYou can use it in dbt . For more on dbt versioning, see Choosing a dbt version .\nYou can select it from the version dropdown on this website, to see documentation that is accurate for use with that minor version.\nOngoing patches \u200b\nDuring the 12-month support window, we will continue to release new patch versions that include fixes.\nActive Support: In the first few months after a minor version's initial release, we will patch it with \"bugfix\" releases. These will include fixes for regressions and net-new bugs that were present in the minor version's original release.\nCritical Support: When a newer minor version is available, we will transition the previous minor version into \"Critical Support.\" Subsequent patches to that older minor version will be \"security\" releases only, limited to critical fixes related to security and installation.\nAfter a minor version reaches the end of its critical support period, one year after its initial release, no new patches will be released.\nFuture versions \u200b\nFor the latest information about upcoming releases, including planned release dates and which features and fixes might be included, consult the dbt-core repository milestones and product roadmaps .\ndbt-core\nBest practices for upgrading \u200b\nBecause of our new version practice, we've outlined best practices and expectations for dbt users to upgrade as we continue to release new versions of dbt Core .\nUpgrading to new patch versions \u200b\nWe expect users to upgrade to patches as soon as they're available. When we refer to a \"minor version\" of dbt Core, such as v1.0, we are always referring to the latest available patch release for that minor version. We encourage you to structure your development and production environments so that you can always install the latest patches of dbt-core and any adapter plugins. (Note that patch numbers may be different between dbt-core and plugins. See below for an explanation.)\ndbt-core\nUpgrading to new minor versions \u200b\nDuring the official support period, minor versions will remain available in dbt and the version dropdown on the docs site. While we do not expect users to immediately upgrade to newer minor versions as soon as they're available, there will always be some features and fixes only available for users of the latest minor version.\nTrying prereleases \u200b\nAll dbt Core versions are available as prereleases before the final release. \"Release candidates\" are available for testing, in production-like environments, two weeks before the final release. For minor versions, we also aim to release one or more \"betas,\" which include new features and invite community feedback, 4+ weeks before the final release. It is in your interest to help us test prereleases\u2014we need your help!\nHow dbt Core uses semantic versioning \u200b\nLike many software projects, dbt Core releases follow semantic versioning , which defines three types of version releases.\nMajor versions: To date, dbt Core has had one major version release: v1.0.0. When v2.0.0 is released, it will introduce new features, and functionality that has been announced for deprecation will stop working.\nMinor versions , also called \"feature\" releases, include a mix of new features, behind-the-scenes improvements, and changes to existing capabilities that are backwards compatible with previous minor versions. They will not break code in your project that relies on documented functionality.\nPatch versions , also called \"bugfix\" or \"security\" releases, include fixes only . These fixes could be needed to restore previous (documented) behavior, fix obvious shortcomings of new features, or offer critical fixes for security or installation issues. We are judicious about which fixes are included in patch releases, to minimize the surface area of changes.\nWe are committed to avoiding breaking changes in minor versions for end users of dbt. There are two types of breaking changes that may be included in minor versions:\nChanges to the Python interface for adapter plugins. These changes are relevant only to adapter maintainers, and they will be clearly communicated in documentation and release notes. For more information, refer to Build, test, document, and promote adapters guide.\nChanges to metadata interfaces, including artifacts and logging , signalled by a version bump. Those version upgrades may require you to update external code that depends on these interfaces, or to coordinate upgrades between dbt orchestrations that share metadata, such as state-powered selection .\nHow we version adapter plugins \u200b\nWhen you use dbt, you use a combination of dbt-core and an adapter plugin specific to your database. You can see the current list in Supported Data Platforms . Both dbt-core and dbt adapter plugins follow semantic versioning.\ndbt-core\ndbt-core\ndbt-core and adapter plugins use the dbt-adapters interface to coordinate new features and behind-the-scenes changes. New adapter features are defined in dbt-adapters (which dbt-core will use). These features are opt-in, meaning they only impact adapters that explicitly implement them. This allows us to independently release adapters, dbt-adapters , and dbt-core without creating a broken experience for users.\ndbt-core\ndbt-adapters\ndbt-adapters\ndbt-core\ndbt-adapters\ndbt-core\nUnlike dbt-core versions before 1.8, the minor and patch version numbers might not match between dbt-core and the adapter plugin(s) you've installed.\ndbt-core\ndbt-core\nFor example, you might find you're using dbt-core==1.8.0 with dbt-snowflake==1.9.0 . Even though these don't have the same minor version, they can still work together as they both work with dbt-adapters==1.8.0 . Patch releases can contain important bug or security fixes so it\u2019s critical to stay up to date.\ndbt-core==1.8.0\ndbt-snowflake==1.9.0\ndbt-adapters==1.8.0\nYou can use the dbt --version command to see which versions you have installed:\ndbt --version\n$ dbt --version Core: - installed: 1.8.0 - latest:    1.8.0 - Up to date! Plugins: - snowflake: 1.9.0 - Up to date!\n$ dbt --version Core: - installed: 1.8.0 - latest:    1.8.0 - Up to date! Plugins: - snowflake: 1.9.0 - Up to date!\nYou can see which version of the registered adapter that's being invoked in the logs . Below is an example of the message in the logs/dbt.log file:\nlogs/dbt.log\n[0m13:13:48.572182 [info ] [MainThread]: Registered adapter: snowflake=1.9.0\n[0m13:13:48.572182 [info ] [MainThread]: Registered adapter: snowflake=1.9.0\nIt's likely that newer patches have become available since then, so it's always important to check and make sure you're up to date!\nFurther reading Version support prior to v1.0 EOL version support Current version support Minor versions Ongoing patches Future versions Best practices for upgrading Upgrading to new patch versions Upgrading to new minor versions Trying prereleases How dbt Core uses semantic versioning How we version adapter plugins\nVersion support prior to v1.0 EOL version support Current version support Minor versions Ongoing patches Future versions Best practices for upgrading Upgrading to new patch versions Upgrading to new minor versions Trying prereleases How dbt Core uses semantic versioning How we version adapter plugins\nEOL version support Current version support Minor versions Ongoing patches Future versions Best practices for upgrading Upgrading to new patch versions Upgrading to new minor versions Trying prereleases How dbt Core uses semantic versioning How we version adapter plugins\nCurrent version support Minor versions Ongoing patches Future versions Best practices for upgrading Upgrading to new patch versions Upgrading to new minor versions Trying prereleases How dbt Core uses semantic versioning How we version adapter plugins\nMinor versions Ongoing patches Future versions\nOngoing patches Future versions\nFuture versions\nBest practices for upgrading Upgrading to new patch versions Upgrading to new minor versions Trying prereleases How dbt Core uses semantic versioning How we version adapter plugins\nUpgrading to new patch versions Upgrading to new minor versions Trying prereleases\nUpgrading to new minor versions Trying prereleases\nTrying prereleases\nHow dbt Core uses semantic versioning How we version adapter plugins\nHow we version adapter plugins"
  },
  {
    "url": "https://docs.getdbt.com/docs/dbt-support",
    "text": "dbt support\nSupport for dbt is available to all users through the following channels:\nDedicated dbt Support team ( dbt users).\nThe Community Forum .\ndbt Community slack .\ndbt Core support \u200b\nIf you're developing on the command line (CLI) and have questions or need some help \u2014 reach out to the helpful dbt community through the Community Forum or dbt Community slack .\ndbt platform support \u200b\nThe global dbt Support team is available to dbt customers by email or by clicking Create a support ticket through the dbt navigation.\nCreate a support ticket \u200b\nTo create a support ticket in dbt :\nIn the dbt navigation, click on Help & Guides .\nClick Create a support ticket .\nFill out the form and click Create Ticket .\nA dbt Support team member will respond to your ticket through email.\nAsk dbt Support Assistant \u200b\ndbt Support Assistant is an AI widget that provides instant, AI-generated responses to common questions. This feature is available to dbt users and can help answer troubleshooting questions, give a synopsis of features and functionality, or link to relevant documentation.\nThe dbt Support Assistant AI widget is separate from Copilot , a powerful AI engine that helps with code generation to accelerate your analytics workflows. The dbt Support Assistant focuses on answering documentation and troubleshooting-related questions. Enabling or disabling AI features in dbt won't affect the dbt Support Assistant's availability.\nWe recommend validating information received in AI responses for any scenario using our documentation. Please contact support to report incorrect information provided by the Support Assistant.\nSupport plans and resources \u200b\nWe want to help you work through implementing and utilizing dbt platform at your organization. Have a question you can't find an answer to in our docs or the Community Forum ? Our Support team is here to dbt help you!\ndbt help\nEnterprise and Enterprise+ plans \u2014 Priority support , optional premium plans, enhanced SLAs, implementation assistance, dedicated management, and dbt Labs security reviews depending on price point.\nDeveloper and Starter plans \u2014 24x5 support (no service level agreement (SLA); contact Sales for Enterprise plan inquiries).\nSupport team help \u2014 Assistance with common dbt questions , like project setup, login issues, error understanding, setup private packages, link to a new GitHub account, how to generate a har file , and so on.\nResource guide \u2014 Check the guide for effective help-seeking strategies.\nTypes of dbt cloud-based platform related questions our Support team can assist you with, regardless of your dbt plan: How do I... set up a dbt project? set up a private package in dbt? configure custom branches on git repos? link dbt to a new GitHub account? Help! I can't... log in. access logs. update user groups. I need help understanding... why this run failed. why I am getting this error message in dbt ? why my CI jobs are not kicking off as expected.\nset up a dbt project?\nset up a private package in dbt?\nconfigure custom branches on git repos?\nlink dbt to a new GitHub account? Help! I can't...\nlog in.\naccess logs.\nupdate user groups. I need help understanding...\nwhy this run failed.\nwhy I am getting this error message in dbt ?\nwhy my CI jobs are not kicking off as expected.\ndbt Enterprise accounts \u200b\nBasic assistance with dbt project troubleshooting.\nHelp with errors and issues in macros, models, and dbt Labs' packages.\nFor strategic advice, best practices, or expansion conversations, consult your Account team.\nFor customers on a dbt Enterprise-tier plan, we also offer basic assistance in troubleshooting issues with your dbt project:\nSomething isn't working the way I would expect it to... in a macro I created... in an incremental model I'm building... in one of dbt Labs' packages like dbt_utils or audit_helper...\nin a macro I created...\nin an incremental model I'm building...\nin one of dbt Labs' packages like dbt_utils or audit_helper...\nI need help understanding and troubleshooting this error... Server error: Compilation Error in rpc request (from remote system)   'dbt_utils' is undefined SQL compilation error: syntax error line 1 at position 38 unexpected '<EOF>'. Compilation Error Error reading name_of_folder/name_of_file.yml - Runtime Error Syntax   error near line 9\nServer error: Compilation Error in rpc request (from remote system)   'dbt_utils' is undefined\nServer error: Compilation Error in rpc request (from remote system)   'dbt_utils' is undefined\nSQL compilation error: syntax error line 1 at position 38 unexpected '<EOF>'.\nSQL compilation error: syntax error line 1 at position 38 unexpected '<EOF>'.\nCompilation Error Error reading name_of_folder/name_of_file.yml - Runtime Error Syntax   error near line 9\nCompilation Error Error reading name_of_folder/name_of_file.yml - Runtime Error Syntax   error near line 9\nTypes of questions you should ask your Account team:\nHow should we think about setting up our dbt projects, environments, and jobs based on our company structure and needs?\nI want to expand my account! How do I add more people and train them?\nHere is our data road map for the next year - can we talk through how dbt fits into it and what features we may not be utilizing that can help us achieve our goals?\nIt is time for our contract renewal, what options do I have?\nSeverity level for Enterprise support \u200b\nSupport tickets are assigned a severity level based on the impact of the issue on your business. The severity level is assigned by dbt Labs, and the level assigned determines the priority level of support you will receive. For specific ticket response time or other questions that relate to your Enterprise or Enterprise+ account\u2019s SLA, please refer to your Enterprise contract.\nSeverity Level Description Severity Level 1 Any Error which makes the use or continued use of the Subscription or material features impossible; Subscription is not operational, with no alternative available. Severity Level 2 Feature failure, without a workaround, but Subscription is operational. Severity Level 3 Feature failure, but a workaround exists. Severity Level 4 Error with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nDescription Severity Level 1 Any Error which makes the use or continued use of the Subscription or material features impossible; Subscription is not operational, with no alternative available. Severity Level 2 Feature failure, without a workaround, but Subscription is operational. Severity Level 3 Feature failure, but a workaround exists. Severity Level 4 Error with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nSeverity Level 1 Any Error which makes the use or continued use of the Subscription or material features impossible; Subscription is not operational, with no alternative available. Severity Level 2 Feature failure, without a workaround, but Subscription is operational. Severity Level 3 Feature failure, but a workaround exists. Severity Level 4 Error with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nAny Error which makes the use or continued use of the Subscription or material features impossible; Subscription is not operational, with no alternative available. Severity Level 2 Feature failure, without a workaround, but Subscription is operational. Severity Level 3 Feature failure, but a workaround exists. Severity Level 4 Error with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nSeverity Level 2 Feature failure, without a workaround, but Subscription is operational. Severity Level 3 Feature failure, but a workaround exists. Severity Level 4 Error with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nFeature failure, without a workaround, but Subscription is operational. Severity Level 3 Feature failure, but a workaround exists. Severity Level 4 Error with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nSeverity Level 3 Feature failure, but a workaround exists. Severity Level 4 Error with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nFeature failure, but a workaround exists. Severity Level 4 Error with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nSeverity Level 4 Error with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nError with low-to-no impact on Client\u2019s access to or use of the Subscription, or Client has a general question or feature enhancement request.\nLeave feedback \u200b\nLeave feedback or submit a feature request for dbt or dbt Core .\nShare feedback or feature request for the dbt platform \u200b\nIn the dbt navigation, click Leave feedback .\nIn the Leave feedback pop up, fill out the form.\nUpload any relevant files to the feedback form (optional).\nConfirm if you'd like dbt Labs to contact you about the feedback (optional).\nClick Send Feedback .\nShare feedback or feature request for dbt Core \u200b\nCreate a GitHub issue here .\nExternal help \u200b\nFor SQL writing, project performance review, or project building, refer to dbt Preferred Consulting Providers and dbt Labs' Services.\nFor help writing SQL, reviewing the overall performance of your project, or want someone to actually help build your dbt project, refer to the following pages:\nList of dbt Consulting Partners .\ndbt Labs' Services .\ndbt Core support dbt platform support Create a support ticket Ask dbt Support Assistant Support plans and resources dbt Enterprise accounts Severity level for Enterprise support Leave feedback External help\ndbt platform support Create a support ticket Ask dbt Support Assistant Support plans and resources dbt Enterprise accounts Severity level for Enterprise support Leave feedback External help\nCreate a support ticket Ask dbt Support Assistant Support plans and resources\nAsk dbt Support Assistant Support plans and resources\nSupport plans and resources\ndbt Enterprise accounts Severity level for Enterprise support Leave feedback External help\nSeverity level for Enterprise support\nLeave feedback External help\nExternal help"
  },
  {
    "url": "https://docs.getdbt.com/docs/cloud/cost-management",
    "text": "About cost management in dbt preview Enterprise Enterprise +\nThe cost management dashboard in dbt gives you valuable insight into how your dbt projects impact your data warehouse costs. They will help you optimize your warehouse spending by visualizing how features, including models, tests, snapshots, and other resources, influence costs over time so that you can take action, report to stakeholders, and optimize development workflows.\nCurrently, only Snowflake is supported.\nThis document will cover setup in Snowflake, dbt , and how to use the cost management dashboard to view your insights.\nPrerequisites \u200b\nThe cost management dashboard and features are currently only available to customers in the US on AWS. Support for more regions and providers is being rolled out over the coming months.\nTo configure the cost management tools, you must have the following:\nProper permission set to configure connections in dbt (such as account admin or project creator).\nProper privileges in Snowflake to create a user and assign them database access.\nA supported data warehouse. Note: Only Snowflake is supported at this time. More warehouses coming soon!\nA dbt account on the Enterprise or Enterprise+ plan .\nSupport for dbt Core and the dbt Fusion engine is coming soon! Select features will be introduced in v1.10 , with many more to come in future versions.\nSet up in Snowflake \u200b\nYou must configure metadata credentials for each unique Snowflake account you want the cost management tool to monitor. To configure the proper access in Snowflake:\nIdentify an existing or new (recommended) service user in your Snowflake account. We recommend configuring a new user for this service, for example, dbt_cost_user , for more flexible customization.\ndbt_cost_user\nGrant the user read permissions to the ORGANIZATION_USAGE and ACCOUNT_USAGE schemas. (Optional) You can scope this down to the specific tables in the warehouse if preferred using a Snowflake database role assigned the following access: ACCOUNT_USAGE.QUERY_HISTORY ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY ACCOUNT_USAGE.ACCESS_HISTORY ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY ORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY\nread\nORGANIZATION_USAGE\nACCOUNT_USAGE\n(Optional) You can scope this down to the specific tables in the warehouse if preferred using a Snowflake database role assigned the following access: ACCOUNT_USAGE.QUERY_HISTORY ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY ACCOUNT_USAGE.ACCESS_HISTORY ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY ORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY\nACCOUNT_USAGE.QUERY_HISTORY\nACCOUNT_USAGE.QUERY_HISTORY\nACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY\nACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY\nACCOUNT_USAGE.ACCESS_HISTORY\nACCOUNT_USAGE.ACCESS_HISTORY\nACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\nACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\nORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY\nORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY\nTo create a user dbt_cost_user and a role dbt_cost_management using SQL and assign the required permissions over specific tables, you'd execute something that looks like the following example:\ndbt_cost_user\ndbt_cost_management\nCREATE USER dbt_cost_user PASSWORD = 'A_SECURE_PASSWORD' DEFAULT_ROLE = dbt_cost_management MUST_CHANGE_PASSWORD = FALSE ; CREATE ROLE dbt_cost_management ; GRANT ROLE dbt_cost_management TO USER dbt_cost_user ; GRANT USAGE ON DATABASE SNOWFLAKE TO ROLE dbt_cost_management ; GRANT USAGE ON SCHEMA SNOWFLAKE . ACCOUNT_USAGE TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ACCOUNT_USAGE . QUERY_HISTORY TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ACCOUNT_USAGE . ACCESS_HISTORY TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY TO ROLE dbt_cost_management ; GRANT USAGE ON SCHEMA SNOWFLAKE . ORGANIZATION_USAGE TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ORGANIZATION_USAGE . USAGE_IN_CURRENCY_DAILY TO ROLE dbt_cost_management ;\nCREATE USER dbt_cost_user PASSWORD = 'A_SECURE_PASSWORD' DEFAULT_ROLE = dbt_cost_management MUST_CHANGE_PASSWORD = FALSE ; CREATE ROLE dbt_cost_management ; GRANT ROLE dbt_cost_management TO USER dbt_cost_user ; GRANT USAGE ON DATABASE SNOWFLAKE TO ROLE dbt_cost_management ; GRANT USAGE ON SCHEMA SNOWFLAKE . ACCOUNT_USAGE TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ACCOUNT_USAGE . QUERY_HISTORY TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ACCOUNT_USAGE . ACCESS_HISTORY TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY TO ROLE dbt_cost_management ; GRANT USAGE ON SCHEMA SNOWFLAKE . ORGANIZATION_USAGE TO ROLE dbt_cost_management ; GRANT SELECT ON VIEW SNOWFLAKE . ORGANIZATION_USAGE . USAGE_IN_CURRENCY_DAILY TO ROLE dbt_cost_management ;\nFor broader, account-wide access, you could assign IMPORTED PRIVILEGES to the user:\nIMPORTED PRIVILEGES\nCREATE USER dbt_cost_user PASSWORD = 'A_SECURE_PASSWORD' DEFAULT_ROLE = dbt_cost_management MUST_CHANGE_PASSWORD = FALSE ; CREATE ROLE dbt_cost_management ; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE dbt_cost_management ; GRANT ROLE dbt_cost_management TO USER dbt_cost_user ;\nCREATE USER dbt_cost_user PASSWORD = 'A_SECURE_PASSWORD' DEFAULT_ROLE = dbt_cost_management MUST_CHANGE_PASSWORD = FALSE ; CREATE ROLE dbt_cost_management ; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE dbt_cost_management ; GRANT ROLE dbt_cost_management TO USER dbt_cost_user ;\nIf you prefer, you can also configure the user for key pair authentication instead of using a username and password with dbt .\nYou must repeat the user creation process in each Snowflake warehouse you want to monitor.\nOnce the user is created and assigned proper privileges, it's time to configure the connection in dbt .\nSet up in dbt \u200b\nConfiguring the cost management features requires both a connection and a user component:\nConnection setup : Set up the credentials used to access the data warehouse information. Use the connection associated with your main account identifier (name or ID). Only one unique connection per warehouse needs to have the credentials configured. If you have multiple connections that reference the same account identifier, you only need to add platform metadata credentials to one of them.\nProvision user access : Add new permissions to users and/or groups to regulate access to the dashboard.\nConnection setup \u200b\nTo configure the metadata connection in dbt:\nNavigate to Account Settings and click Connections .\nNavigate to Account Settings and click Connections .\nClick the connection associated with the data warehouse(s) you configured in the Snowflake setup. Do not click Edit . This is for the broader settings and will prevent the metadata section from being altered.\nClick the connection associated with the data warehouse(s) you configured in the Snowflake setup. Do not click Edit . This is for the broader settings and will prevent the metadata section from being altered.\nScroll down to the Platform metadata credentials and click Add credentials .\nScroll down to the Platform metadata credentials and click Add credentials .\nSet the appropriate Auth method (username and password or key pair) and fill out all the fields provided.\nSet the appropriate Auth method (username and password or key pair) and fill out all the fields provided.\nIn the Features section, click the box to enable Cost Management . Fill out the fields with the appropriate information.\nIn the Features section, click the box to enable Cost Management .\nClick Save .\nClick Save .\nRepeat this process for each dbt warehouse connection you want to monitor.\nRepeat this process for each dbt warehouse connection you want to monitor.\nAfter the setup, it will be a few hours before the initial sync completes and information begins to populate the dashboard.\nProvision user access \u200b\nSince the dashboard contains sensitive financial information, we're introducing two new permission sets to help you regulate access: Cost Management Admin and Cost Mangement Viewer .\nCost Management Admin\nCost Mangement Viewer\nThe Cost Management Viewer role is especially useful to organizations who want to grant viewer access to the dashboard without the elevated permissions associated with admin roles.\nCost Management Viewer\nFor example, let's say you have a group of developers in charge of cost observability and insights you wish to grant access to view the dashboard:\nNavigate to your Account settings and open Groups and licenses .\nClick the group from your list and click Edit .\nFrom Accounts and permissions click Add permission .\nSelect the Cost Management Viewer permission from the dropdown and click Save .\nBy assigning these permission set to the users or groups you want to have access to the dashboard you can avoid granting broader access with the other roles.\nCost management dashboard \u200b\nThe cost management dashboard can be accessed anywhere in dbt from the left-side menu. Once enabled, Cost management will be an option below the Account home feature at the top of the sidebar. Users who don't have the proper permissions will not see the option.\nUsers with the following permission sets will be able to access the cost management dashboard:\nAccount Admin\nAccount Viewer\nNew: Cost Management Viewer\nNew: Cost Management Admin\nOnce the information syncs, you will see the results by selecting the Cost management dashboard option from the left-side menu.\nHover over the Last refreshed... date to see a list of your configured connections and their status. View your connection status.\nAdjust the period you want to monitor. Adjust the period you want to view.\nMetrics \u200b\nThere are metrics that will be available to view and measure your costs as you navigate the dashboard. As you filter your dashboard, you will have access to a list view that enables you to sort by these metrics. The following metrics are available in the cost management dashboard:\nExecution queries: The total number of queries run within the data warehouse by executing dbt resources (model builds, tests, etc.).\nConsumption queries: The total number of queries of given resource across all usage in the warehouse (includes BI/analytics tools, query consoles, etc.)\nExecution costs: The total warehouse cost associated with the resource(s) being executed in dbt runs.\nDuration (resource view only): The total duration of queries that executed dbt resources over the time period.\nYou can sort the list views by these metrics to see how resources are impacting individual areas and have quick views into your highest cost areas\nOverview \u200b\nThe Overview dashboard is the first display you'll see. It gives you general information about your costs:\nView trends in your warehouse costs for the selected time. Hover over the graph to view the difference in spend period-over-period. View trends over time.\nThe top tiles display: Warehouse spend over the selected period. Realized savings (coming soon). See your total spending.\nWarehouse spend over the selected period.\nRealized savings (coming soon).\nThe bar chart breaks down costs of dbt execution by project. You can click on the individual bars to view more information. View your spending over time by project and interact with the data to view more.\nYou'll be brought to the Discover tab when you click on a bar or project. Here, you can view more detailed information about your spending.\nDiscover \u200b\nThe Discover tab takes you to a pane where you can really start getting granular with your cost analysis. It is an interactive page that allows you to break down costs by project, resource, date, and various combinations of those. You'll be able to monitor specific notes in your project lineage to see which resources are impacting your spending the most and view the metadata specific to those resources.\nThere are multiple options for filtering the cost data views with two starting points\nResources\nResources\nEnvironments Filter the Discover view by resource types.\nEnvironments\nResource view \u200b\nWhen you filter by resources, you get valuable insights into how your projects\u2019 resources impact warehouse costs. Use the dropdown menu or click the colored squares to add or remove resource types from the bar graph and list view.\nFilter information by following resource types: Model Test Operation Snapshot Seed Source\nModel\nTest\nOperation\nSnapshot\nSeed\nSource\nFilter the graph view by project and/or resource type.\nView a detailed breakdown of your resources and the costs associated. You can filter by resource name and/or type and sort by each column. Filter and view detailed breakdowns of your resources.\nClick into a resource to view its lineage and how much each node impacts your costs. You can even open the resource in dbt Explorer from this view to better understand your metadata! View the resources lineage and monitor node costs.\nEnvironment view \u200b\nWhen you filter by environment, select a project to view more detailed information about how each environment type impacts your warehouse costs.\nThe list view will mark your production environment with a PROD icon.\nPROD\nClick a colored square next to an environment name to add or remove it from the bar graph view.\nHover over a bar to view the cost breakdown for each environment. The bar graph breaks down costs by environment.\nSort the list view by any of the available fields. Click an item in the list view for detailed bar graph breakdowns of cost, query execution count, and consumption count. View individual environments and how they impact your costs.\nResource details \u200b\nWhen you click on a model or other resource from the Discover tab, you are provided with detailed information about the models consumption and execution:\nThe first section is a description and information about the resource, including size and consumption queries over the last 30 days.\nLineage: A DAG view of different lenses with data consumption metrics: Resource type: View the resource type lineage for your model including other models, snapshots, seeds, and metrics. Execution costs: View the execution costs for each resource in your models lineage. Consumption query history: View the warehouse consumption metrics for the different resources in your models lineage.\nResource type: View the resource type lineage for your model including other models, snapshots, seeds, and metrics.\nExecution costs: View the execution costs for each resource in your models lineage.\nConsumption query history: View the warehouse consumption metrics for the different resources in your models lineage.\nCost: A graph of the resources consumption costs over the previous 30 day period.\nQuery execution count: A graph of the total number of queries the resource has run against the warehouse over the previous 30 day period.\nConsumption queries: A graph of the queries used to gather analytical data from the warehouse.\nKnown limitations \u200b\nThe following are some of the known limitations and caveats for the cost management dashboard:\nThe dashboard doesn't currently reflect the costs of development environments.\nThere may be discrepancies in cost comparison between the dashboard and the data platform UI, as they may reflect different numbers depending on the time period or range selected.\nThe cost metric may not perfectly reflect queries with very small durations, which may also skew the average.\nThe consumption metric includes all queries of a given model in the warehouse, beyond just analytics use cases, so it is best for relative comparison between resources.\nThe consumption metric relies on mapping the dbt model to its tables in the warehouse, so it may be imprecise depending on how the mapping changes\nA dbt run results in multiple executions (run steps) issuing queries, which makes it less intuitive to reason about, so in the future, moving toward more run-centric metrics (grouping/aggregating)\nCore costs are dependent on using dbt v1.10 or higher to associate queries with dbt workloads.\nSnowflake can take up to 72 hours to report accurate cost data, so the past three days may undercount until the data is updated.\nPrerequisites Set up in Snowflake Set up in dbt Connection setup Provision user access Cost management dashboard Metrics Overview Discover Resource details Known limitations\nSet up in Snowflake Set up in dbt Connection setup Provision user access Cost management dashboard Metrics Overview Discover Resource details Known limitations\nSet up in dbt Connection setup Provision user access Cost management dashboard Metrics Overview Discover Resource details Known limitations\nConnection setup Provision user access\nProvision user access\nCost management dashboard Metrics Overview Discover Resource details Known limitations\nMetrics Overview Discover\nOverview Discover\nDiscover\nResource details Known limitations\nKnown limitations"
  },
  {
    "url": "https://docs.getdbt.com/docs/faqs",
    "text": "Frequently asked questions\nOur Frequently Asked Questions (FAQs) section is a space where you can find an answer to some questions we get asked a lot (but that we\u2019re happy to answer!). If you have a question or are still stuck on something, just reach out to us by emailing support@getdbt.com or clicking on the chat widget, and we\u2019ll do our best to help out.\n\ud83d\uddc3\ufe0f Accounts\n11 items\n\ud83d\uddc3\ufe0f dbt Core\n3 items\n\ud83d\uddc3\ufe0f Documentation\n6 items\n\ud83d\uddc3\ufe0f Environments\n6 items\n\ud83d\uddc3\ufe0f Git\n9 items\n\ud83d\uddc3\ufe0f Jinja\n3 items\n\ud83d\uddc3\ufe0f Models\n12 items\n\ud83d\uddc3\ufe0f Projects\n24 items\n\ud83d\uddc3\ufe0f Project_ref\n2 items\n\ud83d\uddc3\ufe0f Runs\n8 items\n\ud83d\uddc3\ufe0f Seeds\n8 items\n\ud83d\uddc3\ufe0f Snapshots\n4 items\n\ud83d\uddc3\ufe0f Tests\n9 items\n\ud83d\uddc3\ufe0f Troubleshooting\n22 items\n\ud83d\uddc3\ufe0f Warehouse\n8 items"
  },
  {
    "url": "https://docs.getdbt.com/community/resources/viewpoint",
    "text": "The dbt Viewpoint\nIn 2015-2016, a team of folks at RJMetrics had the opportunity to observe, and participate in, a significant evolution of the analytics ecosystem. The seeds of dbt were conceived in this environment, and the viewpoint below was written to reflect what we had learned and how we believed the world should be different. dbt is our attempt to address the workflow challenges we observed, and as such, this viewpoint is the most foundational statement of the dbt project's goals. The remainder of this document is largely unedited from the original post .\nThe remainder of this document is largely unedited from the original post .\nAnalytics today \u200b\nThe center of gravity in mature analytics organizations has shifted away from proprietary, end-to-end tools towards more composable solutions made up of:\ndata integration scripts and/or tools,\nhigh-performance analytic databases,\nSQL, R, and/or Python, and\nvisualization tools.\nThis change has unlocked significant possibility, but analytics teams (ours included) have still faced challenges in consistently delivering high-quality, low-latency analytics.\nWe believe that analytics teams have a workflow problem. Too often, analysts operate in isolation, and this creates suboptimal outcomes. Knowledge is siloed. We too often rewrite analyses that a colleague had already written. We fail to grasp the nuances of datasets that we\u2019re less familiar with. We differ in our calculations of a shared metric.\nWe have convinced ourselves after hundreds of conversations that these conditions are by and large the status quo for even sophisticated analytics teams. As a result, organizations suffer from reduced decision speed and reduced decision quality.\nAnalytics doesn\u2019t have to be this way. In fact, the playbook for solving these problems already exists\u200a\u2014\u200aon our software engineering teams. The same techniques that software engineering teams use to collaborate on the rapid creation of quality applications can apply to analytics. We believe it\u2019s time to build an open set of tools and processes to make that happen.\nAnalytics is collaborative \u200b\nWe believe a mature analytics team\u2019s techniques and workflow should have the following collaboration features:\nVersion Control \u200b\nAnalytic code\u200a\u2014\u200awhether it\u2019s Python, SQL, Java, or anything else\u200a\u2014\u200ashould be version controlled. Analysis changes as data and businesses evolve, and it\u2019s important to know who changed what, when.\nQuality Assurance \u200b\nBad data can lead to bad analyses, and bad analyses can lead to bad decisions. Any code that generates data or analysis should be reviewed and tested.\nDocumentation \u200b\nYour analysis is a software application, and, like every other software application, people are going to have questions about how to use it. Even though it might seem simple, in reality the \u201cRevenue\u201d line you\u2019re showing could mean dozens of things. Your code should come packaged with a basic description of how it should be interpreted, and your team should be able to add to that documentation as additional questions arise.\nModularity \u200b\nIf you build a series of analyses about your company\u2019s revenue, and your colleague does as well, you should use the same input data. Copy-paste is not a good approach here\u200a\u2014\u200aif the definition of the underlying set changes, it will need to be updated everywhere it was used. Instead, think of the schema of a data set as its public interface. Create tables, views , or other data sets that expose a consistent schema and can be modified if business logic changes.\nAnalytic code is an asset \u200b\nThe code, processes, and tooling required to produce that analysis are core organizational investments. We believe a mature analytics organization\u2019s workflow should have the following characteristics so as to protect and grow that investment:\nEnvironments \u200b\nAnalytics requires multiple environments. Analysts need the freedom to work without impacting users, while users need service level guarantees so that they can trust the data they rely on to do their jobs.\nService level guarantees \u200b\nAnalytics teams should stand behind the accuracy of all analysis that has been promoted to production. Errors should be treated with the same level of urgency as bugs in a production product. Any code being retired from production should go through a deprecation process.\nDesign for maintainability \u200b\nMost of the cost involved in software development is in the maintenance phase. Because of this, software engineers write code with an eye towards maintainability. Analytic code, however, is often fragile. Changes in underlying data break most analytic code in ways that are hard to predict and to fix.\nAnalytic code should be written with an eye towards maintainability. Future changes to the schema and data should be anticipated and code should be written to minimize the corresponding impact.\nAnalytics workflows require automated tools \u200b\nFrequently, much of an analytic workflow is manual. Piping data from source to destination, from stage to stage, can eat up a majority of an analyst\u2019s time. Software engineers build extensive tooling to support the manual portions of their jobs. In order to implement the analytics workflows we are suggesting, similar tools will be required.\nHere\u2019s one example of an automated workflow:\nmodels and analysis are downloaded from multiple source control repositories,\ncode is configured for the given environment,\ncode is tested, and\ncode is deployed.\nWorkflows like this should be built to execute with a single command.\nAnalytics today Analytics is collaborative Version Control Quality Assurance Documentation Modularity Analytic code is an asset Environments Service level guarantees Design for maintainability Analytics workflows require automated tools\nAnalytics is collaborative Version Control Quality Assurance Documentation Modularity Analytic code is an asset Environments Service level guarantees Design for maintainability Analytics workflows require automated tools\nVersion Control Quality Assurance Documentation Modularity\nQuality Assurance Documentation Modularity\nDocumentation Modularity\nModularity\nAnalytic code is an asset Environments Service level guarantees Design for maintainability Analytics workflows require automated tools\nEnvironments Service level guarantees Design for maintainability\nService level guarantees Design for maintainability\nDesign for maintainability\nAnalytics workflows require automated tools"
  },
  {
    "url": "https://docs.getdbt.com/docs/fusion/install-fusion",
    "text": "About Fusion installation beta\nThe dbt Fusion engine is currently in beta and the related documentation is a work in progress. The information on this page will evolve as features are added and enhanced. Join the conversation in our Community Slack channel #dbt-fusion-engine .\n#dbt-fusion-engine\nThis guide walks you through installing Fusion locally, including important prerequisites, step-by-step installation instructions, troubleshooting common issues, and configuration guidance.\nPrerequisites \u200b\nBefore installing Fusion, ensure:\nYou have administrative privileges to install software on your local machine.\nYou have administrative privileges to install software on your local machine.\nYou are familiar with command-line interfaces (Terminal on macOS/Linux, PowerShell on Windows).\nYou are familiar with command-line interfaces (Terminal on macOS/Linux, PowerShell on Windows).\nYou are using a supported data warehouse and authentication method. Snowflake Username/password Native OAuth External OAuth Key pair MFA Databricks Service Account / User Token Native Oauth\nYou are using a supported data warehouse and authentication method.\nUsername/password\nNative OAuth\nExternal OAuth\nKey pair\nMFA\nService Account / User Token\nNative Oauth\nYou are using a supported OS and architecture: \ud83d\udfe2 - Supported \ud83d\udfe1 - Not yet supported - Support expected by 2025-07-18 Operating System X86-64 ARM macOS \ud83d\udfe2 \ud83d\udfe2 Linux \ud83d\udfe2 \ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\nYou are using a supported OS and architecture:\n\ud83d\udfe2 - Supported \ud83d\udfe1 - Not yet supported - Support expected by 2025-07-18\nOperating System X86-64 ARM macOS \ud83d\udfe2 \ud83d\udfe2 Linux \ud83d\udfe2 \ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\nX86-64 ARM macOS \ud83d\udfe2 \ud83d\udfe2 Linux \ud83d\udfe2 \ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\nARM macOS \ud83d\udfe2 \ud83d\udfe2 Linux \ud83d\udfe2 \ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\nmacOS \ud83d\udfe2 \ud83d\udfe2 Linux \ud83d\udfe2 \ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\n\ud83d\udfe2 \ud83d\udfe2 Linux \ud83d\udfe2 \ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\n\ud83d\udfe2 Linux \ud83d\udfe2 \ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\nLinux \ud83d\udfe2 \ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\n\ud83d\udfe2 \ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\n\ud83d\udfe1 Windows \ud83d\udfe1 \ud83d\udfe1\nWindows \ud83d\udfe1 \ud83d\udfe1\n\ud83d\udfe1 \ud83d\udfe1\n\ud83d\udfe1\nInstall Fusion \u200b\nFusion can be installed via the command line from our official CDN:\nmacOS/Linux: Using curl\ncurl\nWindows: Using irm\nirm\nmacOS & Linux installation \u200b\nRun the following command in the terminal:\ncurl -fsSL https://public.cdn.getdbt.com/fs/install/install.sh | sh -s -- --update\ncurl -fsSL https://public.cdn.getdbt.com/fs/install/install.sh | sh -s -- --update\nTo use dbtf immediately after installation, reload your shell so that the new $PATH is recognized:\ndbtf\n$PATH\nexec $SHELL\nexec $SHELL\nOr, close and reopen your Terminal window. This will load the updated environment settings into the new session.\nWindows installation (PowerShell) \u200b\nRun the following command in PowerShell:\nirm https://public.cdn.getdbt.com/fs/install/install.ps1 | iex\nirm https://public.cdn.getdbt.com/fs/install/install.ps1 | iex\nTo use dbtf immediately after installation, reload your shell so that the new Path is recognized:\ndbtf\nPath\nStart-Process powershell\nStart-Process powershell\nOr, close and reopen PowerShell. This will load the updated environment settings into the new session.\nVerify the installation \u200b\nAfter installation, open a new command-line window and verify that Fusion is installed correctly by checking the version. You can run these commands using dbt , or use dbtf as an unambiguous alias for Fusion, if you have another dbt CLI installed on your machine.\ndbt\ndbtf\ndbtf --version\ndbtf --version\nFusion will be installed in the following locations:\nmacOS & Linux: $HOME/.local/bin/dbt\n$HOME/.local/bin/dbt\nWindows: C:\\Users\\<YourUsername>\\.local\\bin\\dbt.exe\nC:\\Users\\<YourUsername>\\.local\\bin\\dbt.exe\nThis location is automatically added to your path to easily execute the dbtf command, but it requires reloading your shell.\ndbtf\nUpdate Fusion \u200b\nThe following command will update to the latest version of Fusion and adapter code:\ndbtf system update\ndbtf system update\nUninstall \u200b\nThis command will uninstall the Fusion binary from your system (but aliases will remain wherever they are installed, for example ~/.zshrc ):\n~/.zshrc\ndbtf system uninstall\ndbtf system uninstall\nAdapter installation \u200b\nThe Fusion install automatically includes the Snowflake adapter. Other adapters will be available at a later date.\nTroubleshooting \u200b\nCommon issues and resolutions:\ndbt command not found: Ensure installation location is correctly added to your $PATH .\n$PATH\nVersion conflicts: Verify no existing dbt Core or dbt Cloud CLI versions are installed (or active) that could conflict with Fusion.\nInstallation permissions: Confirm your user has appropriate permissions to install software locally.\nFrequently asked questions \u200b\nCan I revert to my previous dbt installation? Yes. If you want to test Fusion without affecting your existing workflows, consider isolating or managing your installation via separate environments or virtual machines.\nCan I revert to my previous dbt installation?\nYes. If you want to test Fusion without affecting your existing workflows, consider isolating or managing your installation via separate environments or virtual machines.\nMore information about Fusion \u200b\nFusion marks a significant update to dbt. While many of the workflows you've grown accustomed to remain unchanged, there are a lot of new ideas, and a lot of old ones going away. The following is a list of the full scope of our current release of the Fusion engine, including implementation, installation, deprecations, and limitations:\nAbout the dbt Fusion engine\nAbout the dbt extension\nNew concepts in Fusion\nSupported features matrix\nInstalling Fusion CLI\nInstalling VS Code extension\nFusion release track\nQuickstart for Fusion\nUpgrade guide\nFusion licensing\nPrerequisites Install Fusion macOS & Linux installation Windows installation (PowerShell) Verify the installation Update Fusion Uninstall Adapter installation Troubleshooting Frequently asked questions More information about Fusion\nInstall Fusion macOS & Linux installation Windows installation (PowerShell) Verify the installation Update Fusion Uninstall Adapter installation Troubleshooting Frequently asked questions More information about Fusion\nmacOS & Linux installation Windows installation (PowerShell) Verify the installation Update Fusion Uninstall Adapter installation\nWindows installation (PowerShell) Verify the installation Update Fusion Uninstall Adapter installation\nVerify the installation Update Fusion Uninstall Adapter installation\nUpdate Fusion Uninstall Adapter installation\nUninstall Adapter installation\nAdapter installation\nTroubleshooting Frequently asked questions More information about Fusion\nFrequently asked questions More information about Fusion\nMore information about Fusion"
  },
  {
    "url": "https://docs.getdbt.com/docs/core/about-core-setup",
    "text": "About dbt Core setup\ndbt Core is an open-source tool that enables data teams to transform data using analytics engineering best practices. You can install dbt locally in your environment and use dbt Core on the command line. It can communicate with databases through adapters.\nThis section of our docs will guide you through various settings to get started:\nConnecting to a data platform\nHow to run your dbt projects\nIf you're interested in using a command line interface to develop dbt projects in dbt , the dbt CLI lets you run dbt commands locally. The dbt CLI is tailored for dbt 's infrastructure and integrates with all its features .\nIf you need a more detailed first-time setup guide for specific data platforms, read our quickstart guides ."
  },
  {
    "url": "https://docs.getdbt.com/docs/core/installation-overview",
    "text": "About dbt Core and installation\ndbt Core is an open sourced project where you can develop from the command line and run your dbt project.\nTo use dbt Core , your workflow generally looks like:\nBuild your dbt project in a code editor \u2014 popular choices include VSCode and Atom.\nBuild your dbt project in a code editor \u2014 popular choices include VSCode and Atom.\nRun your project from the command line \u2014 macOS ships with a default Terminal program, however you can also use iTerm or the command line prompt within a code editor to execute dbt commands.\nRun your project from the command line \u2014 macOS ships with a default Terminal program, however you can also use iTerm or the command line prompt within a code editor to execute dbt commands.\nWe've written a guide for our recommended setup when running dbt projects using dbt Core .\nIf you're using the command line, we recommend learning some basics of your terminal to help you work more effectively. In particular, it's important to understand cd , ls and pwd to be able to navigate through the directory structure of your computer easily.\ncd\nls\npwd\nInstall dbt Core \u200b\nYou can install dbt Core on the command line by using one of these methods:\nUse pip to install dbt (recommended)\nUse a Docker image to install dbt\nInstall dbt from source\nYou can also develop locally using the dbt CLI . The dbt CLI and dbt Core are both command line tools that let you run dbt commands. The key distinction is the dbt CLI is tailored for dbt 's infrastructure and integrates with all its features .\nUpgrading dbt Core \u200b\ndbt provides a number of resources for understanding general best practices while upgrading your dbt project as well as detailed migration guides highlighting the changes required for each minor and major release .\nUpgrade pip\npip\nAbout dbt data platforms and adapters \u200b\ndbt works with a number of different data platforms (databases, query engines, and other SQL-speaking technologies). It does this by using a dedicated adapter for each. When you install dbt Core , you'll also want to install the specific adapter for your database. For more details, see Supported Data Platforms .\nMost command-line tools, including dbt, have a --help flag that you can use to show available commands and arguments. For example, you can use the --help flag with dbt in two ways: \u2014 dbt --help : Lists the commands available for dbt \u2014 dbt run --help : Lists the flags available for the run command\n--help\n--help\ndbt --help\ndbt run --help\nrun\nInstall dbt Core Upgrading dbt Core About dbt data platforms and adapters\nUpgrading dbt Core About dbt data platforms and adapters\nAbout dbt data platforms and adapters"
  },
  {
    "url": "https://docs.getdbt.com/guides/codespace?step=1",
    "text": "Quickstart for dbt Core using DuckDB\nIntroduction \u200b\nIn this quickstart guide, you'll learn how to use dbt Core with DuckDB, enabling you to get set up quickly and efficiently. DuckDB is an open-source database management system which is designed for analytical workloads. It is designed to provide fast and easy access to large datasets, making it well-suited for data analytics tasks. This guide will demonstrate how to: Create a virtual development environment using a template provided by dbt Labs. We will set up a fully functional dbt environment with an operational and executable project. The codespace automatically connects to the DuckDB database and loads a year's worth of data from our fictional Jaffle Shop caf\u00e9, which sells food and beverages in several US cities. Run through the steps outlined in the jaffle_shop_duck_db repository, but if you want to dig into the underlying code further, refer to the README for the Jaffle Shop template. Run any dbt command from the environment\u2019s terminal. Generate a larger dataset for the Jaffle Shop caf\u00e9 (for example, five years of data instead of just one). You can learn more through high-quality dbt Learn courses and workshops . Related content \u200b DuckDB setup Create a GitHub repository Build your first models Test and document your project Prerequisites \u200b When using DuckDB with dbt Core , you'll need to use the dbt command-line interface (CLI). Currently, DuckDB is not supported in dbt . It's important that you know some basics of the terminal. In particular, you should understand cd , ls , and pwd to navigate through the directory structure of your computer easily. You have a GitHub account . Set up DuckDB for dbt Core \u200b This section will provide a step-by-step guide for setting up DuckDB for use in local (Mac and Windows) environments and web browsers. In the repository, there's a requirements.txt file which is used to install dbt Core, DuckDB, and all other necessary dependencies. You can check this file to see what will be installed on your machine. It's typically located in the root directory of your project alongside other key files like dbt_project.yml . Otherwise, we will show you how in later steps. Below is an example of the requirements.txt file alongside other key files like dbt_project.yml : /my_dbt_project/ \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 models/ \u2502   \u251c\u2500\u2500 my_model.sql \u251c\u2500\u2500 tests/ \u2502   \u251c\u2500\u2500 my_test.sql \u2514\u2500\u2500 requirements.txt For more information, refer to the DuckDB setup . Local Web browser First, clone the Jaffle Shop git repository by running the following command in your terminal: git clone https://github.com/dbt-labs/jaffle_shop_duckdb.git Change into the docs-duckdb directory from the command line: cd jaffle_shop_duckdb Install dbt Core and DuckDB in a virtual environment. Example for Mac python3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate Example for Windows python -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat Example for Windows PowerShell python -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1 Ensure your profile is setup correctly from the command line by running the following dbt commands . dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project dbt docs generate \u2014 generates your project's documentation. dbt docs serve \u2014 starts a webserver on port 8080 to serve your documentation locally and opens the documentation site in your default browser. For complete details, refer to the dbt command reference . Here's what a successful output will look like: (venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28 To query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data). Go to the jaffle-shop-template repository after you log in to your GitHub account. Click Use this template at the top of the page and choose Create new repository . Click Create repository from template when you\u2019re done setting the options for your new repository. Click Code (at the top of the new repository\u2019s page). Under the Codespaces tab,  choose Create codespace on main . Depending on how you've configured your computer's settings, this either opens a new browser tab with the Codespace development environment with VSCode running in it or opens a new VSCode window with the codespace in it. Wait for the codespace to finish building by waiting for the postCreateCommand command to complete; this can take several minutes: Wait for postCreateCommand to complete When this command completes, you can start using the codespace development environment. The terminal the command ran in will close and you will get a prompt in a brand new terminal. At the terminal's prompt, you can execute any dbt command you want. For example: /workspaces/test ( main ) $ dbt build You can also use the duckcli to write SQL against the warehouse from the command line or build reports in the Evidence project provided in the reports directory. For complete information, refer to the dbt command reference . Common commands are: dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project Generate a larger data set \u200b If you'd like to work with a larger selection of Jaffle Shop data, you can generate an arbitrary number of years of fictitious data from within your codespace. Install the Python package called jafgen . At the terminal's prompt, run: python -m pip install jafgen When installation is done, run: jafgen [ number of years to generate ] # e.g. jafgen 6 Replace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter. As you increase the number of years, it takes exponentially more time to generate the data because the Jaffle Shop stores grow in size and number. For a good balance of data size and time to build, dbt Labs suggests a maximum of 6 years. Next steps \u200b Now that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nThis guide will demonstrate how to: Create a virtual development environment using a template provided by dbt Labs. We will set up a fully functional dbt environment with an operational and executable project. The codespace automatically connects to the DuckDB database and loads a year's worth of data from our fictional Jaffle Shop caf\u00e9, which sells food and beverages in several US cities. Run through the steps outlined in the jaffle_shop_duck_db repository, but if you want to dig into the underlying code further, refer to the README for the Jaffle Shop template. Run any dbt command from the environment\u2019s terminal. Generate a larger dataset for the Jaffle Shop caf\u00e9 (for example, five years of data instead of just one). You can learn more through high-quality dbt Learn courses and workshops . Related content \u200b DuckDB setup Create a GitHub repository Build your first models Test and document your project Prerequisites \u200b When using DuckDB with dbt Core , you'll need to use the dbt command-line interface (CLI). Currently, DuckDB is not supported in dbt . It's important that you know some basics of the terminal. In particular, you should understand cd , ls , and pwd to navigate through the directory structure of your computer easily. You have a GitHub account . Set up DuckDB for dbt Core \u200b This section will provide a step-by-step guide for setting up DuckDB for use in local (Mac and Windows) environments and web browsers. In the repository, there's a requirements.txt file which is used to install dbt Core, DuckDB, and all other necessary dependencies. You can check this file to see what will be installed on your machine. It's typically located in the root directory of your project alongside other key files like dbt_project.yml . Otherwise, we will show you how in later steps. Below is an example of the requirements.txt file alongside other key files like dbt_project.yml : /my_dbt_project/ \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 models/ \u2502   \u251c\u2500\u2500 my_model.sql \u251c\u2500\u2500 tests/ \u2502   \u251c\u2500\u2500 my_test.sql \u2514\u2500\u2500 requirements.txt For more information, refer to the DuckDB setup . Local Web browser First, clone the Jaffle Shop git repository by running the following command in your terminal: git clone https://github.com/dbt-labs/jaffle_shop_duckdb.git Change into the docs-duckdb directory from the command line: cd jaffle_shop_duckdb Install dbt Core and DuckDB in a virtual environment. Example for Mac python3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate Example for Windows python -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat Example for Windows PowerShell python -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1 Ensure your profile is setup correctly from the command line by running the following dbt commands . dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project dbt docs generate \u2014 generates your project's documentation. dbt docs serve \u2014 starts a webserver on port 8080 to serve your documentation locally and opens the documentation site in your default browser. For complete details, refer to the dbt command reference . Here's what a successful output will look like: (venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28 To query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data). Go to the jaffle-shop-template repository after you log in to your GitHub account. Click Use this template at the top of the page and choose Create new repository . Click Create repository from template when you\u2019re done setting the options for your new repository. Click Code (at the top of the new repository\u2019s page). Under the Codespaces tab,  choose Create codespace on main . Depending on how you've configured your computer's settings, this either opens a new browser tab with the Codespace development environment with VSCode running in it or opens a new VSCode window with the codespace in it. Wait for the codespace to finish building by waiting for the postCreateCommand command to complete; this can take several minutes: Wait for postCreateCommand to complete When this command completes, you can start using the codespace development environment. The terminal the command ran in will close and you will get a prompt in a brand new terminal. At the terminal's prompt, you can execute any dbt command you want. For example: /workspaces/test ( main ) $ dbt build You can also use the duckcli to write SQL against the warehouse from the command line or build reports in the Evidence project provided in the reports directory. For complete information, refer to the dbt command reference . Common commands are: dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project Generate a larger data set \u200b If you'd like to work with a larger selection of Jaffle Shop data, you can generate an arbitrary number of years of fictitious data from within your codespace. Install the Python package called jafgen . At the terminal's prompt, run: python -m pip install jafgen When installation is done, run: jafgen [ number of years to generate ] # e.g. jafgen 6 Replace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter. As you increase the number of years, it takes exponentially more time to generate the data because the Jaffle Shop stores grow in size and number. For a good balance of data size and time to build, dbt Labs suggests a maximum of 6 years. Next steps \u200b Now that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nCreate a virtual development environment using a template provided by dbt Labs.\nWe will set up a fully functional dbt environment with an operational and executable project. The codespace automatically connects to the DuckDB database and loads a year's worth of data from our fictional Jaffle Shop caf\u00e9, which sells food and beverages in several US cities.\nRun through the steps outlined in the jaffle_shop_duck_db repository, but if you want to dig into the underlying code further, refer to the README for the Jaffle Shop template.\njaffle_shop_duck_db\nRun any dbt command from the environment\u2019s terminal.\nGenerate a larger dataset for the Jaffle Shop caf\u00e9 (for example, five years of data instead of just one).\nYou can learn more through high-quality dbt Learn courses and workshops . Related content \u200b DuckDB setup Create a GitHub repository Build your first models Test and document your project Prerequisites \u200b When using DuckDB with dbt Core , you'll need to use the dbt command-line interface (CLI). Currently, DuckDB is not supported in dbt . It's important that you know some basics of the terminal. In particular, you should understand cd , ls , and pwd to navigate through the directory structure of your computer easily. You have a GitHub account . Set up DuckDB for dbt Core \u200b This section will provide a step-by-step guide for setting up DuckDB for use in local (Mac and Windows) environments and web browsers. In the repository, there's a requirements.txt file which is used to install dbt Core, DuckDB, and all other necessary dependencies. You can check this file to see what will be installed on your machine. It's typically located in the root directory of your project alongside other key files like dbt_project.yml . Otherwise, we will show you how in later steps. Below is an example of the requirements.txt file alongside other key files like dbt_project.yml : /my_dbt_project/ \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 models/ \u2502   \u251c\u2500\u2500 my_model.sql \u251c\u2500\u2500 tests/ \u2502   \u251c\u2500\u2500 my_test.sql \u2514\u2500\u2500 requirements.txt For more information, refer to the DuckDB setup . Local Web browser First, clone the Jaffle Shop git repository by running the following command in your terminal: git clone https://github.com/dbt-labs/jaffle_shop_duckdb.git Change into the docs-duckdb directory from the command line: cd jaffle_shop_duckdb Install dbt Core and DuckDB in a virtual environment. Example for Mac python3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate Example for Windows python -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat Example for Windows PowerShell python -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1 Ensure your profile is setup correctly from the command line by running the following dbt commands . dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project dbt docs generate \u2014 generates your project's documentation. dbt docs serve \u2014 starts a webserver on port 8080 to serve your documentation locally and opens the documentation site in your default browser. For complete details, refer to the dbt command reference . Here's what a successful output will look like: (venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28 To query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data). Go to the jaffle-shop-template repository after you log in to your GitHub account. Click Use this template at the top of the page and choose Create new repository . Click Create repository from template when you\u2019re done setting the options for your new repository. Click Code (at the top of the new repository\u2019s page). Under the Codespaces tab,  choose Create codespace on main . Depending on how you've configured your computer's settings, this either opens a new browser tab with the Codespace development environment with VSCode running in it or opens a new VSCode window with the codespace in it. Wait for the codespace to finish building by waiting for the postCreateCommand command to complete; this can take several minutes: Wait for postCreateCommand to complete When this command completes, you can start using the codespace development environment. The terminal the command ran in will close and you will get a prompt in a brand new terminal. At the terminal's prompt, you can execute any dbt command you want. For example: /workspaces/test ( main ) $ dbt build You can also use the duckcli to write SQL against the warehouse from the command line or build reports in the Evidence project provided in the reports directory. For complete information, refer to the dbt command reference . Common commands are: dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project Generate a larger data set \u200b If you'd like to work with a larger selection of Jaffle Shop data, you can generate an arbitrary number of years of fictitious data from within your codespace. Install the Python package called jafgen . At the terminal's prompt, run: python -m pip install jafgen When installation is done, run: jafgen [ number of years to generate ] # e.g. jafgen 6 Replace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter. As you increase the number of years, it takes exponentially more time to generate the data because the Jaffle Shop stores grow in size and number. For a good balance of data size and time to build, dbt Labs suggests a maximum of 6 years. Next steps \u200b Now that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nRelated content \u200b\nDuckDB setup\nCreate a GitHub repository\nBuild your first models\nTest and document your project\nPrerequisites \u200b\nWhen using DuckDB with dbt Core , you'll need to use the dbt command-line interface (CLI). Currently, DuckDB is not supported in dbt .\nIt's important that you know some basics of the terminal. In particular, you should understand cd , ls , and pwd to navigate through the directory structure of your computer easily.\ncd\nls\npwd\nYou have a GitHub account .\nSet up DuckDB for dbt Core \u200b\nThis section will provide a step-by-step guide for setting up DuckDB for use in local (Mac and Windows) environments and web browsers. In the repository, there's a requirements.txt file which is used to install dbt Core, DuckDB, and all other necessary dependencies. You can check this file to see what will be installed on your machine. It's typically located in the root directory of your project alongside other key files like dbt_project.yml . Otherwise, we will show you how in later steps. Below is an example of the requirements.txt file alongside other key files like dbt_project.yml : /my_dbt_project/ \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 models/ \u2502   \u251c\u2500\u2500 my_model.sql \u251c\u2500\u2500 tests/ \u2502   \u251c\u2500\u2500 my_test.sql \u2514\u2500\u2500 requirements.txt For more information, refer to the DuckDB setup . Local Web browser First, clone the Jaffle Shop git repository by running the following command in your terminal: git clone https://github.com/dbt-labs/jaffle_shop_duckdb.git Change into the docs-duckdb directory from the command line: cd jaffle_shop_duckdb Install dbt Core and DuckDB in a virtual environment. Example for Mac python3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate Example for Windows python -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat Example for Windows PowerShell python -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1 Ensure your profile is setup correctly from the command line by running the following dbt commands . dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project dbt docs generate \u2014 generates your project's documentation. dbt docs serve \u2014 starts a webserver on port 8080 to serve your documentation locally and opens the documentation site in your default browser. For complete details, refer to the dbt command reference . Here's what a successful output will look like: (venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28 To query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data). Go to the jaffle-shop-template repository after you log in to your GitHub account. Click Use this template at the top of the page and choose Create new repository . Click Create repository from template when you\u2019re done setting the options for your new repository. Click Code (at the top of the new repository\u2019s page). Under the Codespaces tab,  choose Create codespace on main . Depending on how you've configured your computer's settings, this either opens a new browser tab with the Codespace development environment with VSCode running in it or opens a new VSCode window with the codespace in it. Wait for the codespace to finish building by waiting for the postCreateCommand command to complete; this can take several minutes: Wait for postCreateCommand to complete When this command completes, you can start using the codespace development environment. The terminal the command ran in will close and you will get a prompt in a brand new terminal. At the terminal's prompt, you can execute any dbt command you want. For example: /workspaces/test ( main ) $ dbt build You can also use the duckcli to write SQL against the warehouse from the command line or build reports in the Evidence project provided in the reports directory. For complete information, refer to the dbt command reference . Common commands are: dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project Generate a larger data set \u200b If you'd like to work with a larger selection of Jaffle Shop data, you can generate an arbitrary number of years of fictitious data from within your codespace. Install the Python package called jafgen . At the terminal's prompt, run: python -m pip install jafgen When installation is done, run: jafgen [ number of years to generate ] # e.g. jafgen 6 Replace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter. As you increase the number of years, it takes exponentially more time to generate the data because the Jaffle Shop stores grow in size and number. For a good balance of data size and time to build, dbt Labs suggests a maximum of 6 years. Next steps \u200b Now that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nIn the repository, there's a requirements.txt file which is used to install dbt Core, DuckDB, and all other necessary dependencies. You can check this file to see what will be installed on your machine. It's typically located in the root directory of your project alongside other key files like dbt_project.yml . Otherwise, we will show you how in later steps. Below is an example of the requirements.txt file alongside other key files like dbt_project.yml : /my_dbt_project/ \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 models/ \u2502   \u251c\u2500\u2500 my_model.sql \u251c\u2500\u2500 tests/ \u2502   \u251c\u2500\u2500 my_test.sql \u2514\u2500\u2500 requirements.txt For more information, refer to the DuckDB setup . Local Web browser First, clone the Jaffle Shop git repository by running the following command in your terminal: git clone https://github.com/dbt-labs/jaffle_shop_duckdb.git Change into the docs-duckdb directory from the command line: cd jaffle_shop_duckdb Install dbt Core and DuckDB in a virtual environment. Example for Mac python3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate Example for Windows python -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat Example for Windows PowerShell python -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1 Ensure your profile is setup correctly from the command line by running the following dbt commands . dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project dbt docs generate \u2014 generates your project's documentation. dbt docs serve \u2014 starts a webserver on port 8080 to serve your documentation locally and opens the documentation site in your default browser. For complete details, refer to the dbt command reference . Here's what a successful output will look like: (venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28 To query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data). Go to the jaffle-shop-template repository after you log in to your GitHub account. Click Use this template at the top of the page and choose Create new repository . Click Create repository from template when you\u2019re done setting the options for your new repository. Click Code (at the top of the new repository\u2019s page). Under the Codespaces tab,  choose Create codespace on main . Depending on how you've configured your computer's settings, this either opens a new browser tab with the Codespace development environment with VSCode running in it or opens a new VSCode window with the codespace in it. Wait for the codespace to finish building by waiting for the postCreateCommand command to complete; this can take several minutes: Wait for postCreateCommand to complete When this command completes, you can start using the codespace development environment. The terminal the command ran in will close and you will get a prompt in a brand new terminal. At the terminal's prompt, you can execute any dbt command you want. For example: /workspaces/test ( main ) $ dbt build You can also use the duckcli to write SQL against the warehouse from the command line or build reports in the Evidence project provided in the reports directory. For complete information, refer to the dbt command reference . Common commands are: dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project Generate a larger data set \u200b If you'd like to work with a larger selection of Jaffle Shop data, you can generate an arbitrary number of years of fictitious data from within your codespace. Install the Python package called jafgen . At the terminal's prompt, run: python -m pip install jafgen When installation is done, run: jafgen [ number of years to generate ] # e.g. jafgen 6 Replace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter. As you increase the number of years, it takes exponentially more time to generate the data because the Jaffle Shop stores grow in size and number. For a good balance of data size and time to build, dbt Labs suggests a maximum of 6 years. Next steps \u200b Now that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nrequirements.txt\ndbt_project.yml\nBelow is an example of the requirements.txt file alongside other key files like dbt_project.yml : /my_dbt_project/ \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 models/ \u2502   \u251c\u2500\u2500 my_model.sql \u251c\u2500\u2500 tests/ \u2502   \u251c\u2500\u2500 my_test.sql \u2514\u2500\u2500 requirements.txt For more information, refer to the DuckDB setup . Local Web browser First, clone the Jaffle Shop git repository by running the following command in your terminal: git clone https://github.com/dbt-labs/jaffle_shop_duckdb.git Change into the docs-duckdb directory from the command line: cd jaffle_shop_duckdb Install dbt Core and DuckDB in a virtual environment. Example for Mac python3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate Example for Windows python -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat Example for Windows PowerShell python -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1 Ensure your profile is setup correctly from the command line by running the following dbt commands . dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project dbt docs generate \u2014 generates your project's documentation. dbt docs serve \u2014 starts a webserver on port 8080 to serve your documentation locally and opens the documentation site in your default browser. For complete details, refer to the dbt command reference . Here's what a successful output will look like: (venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28 To query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data). Go to the jaffle-shop-template repository after you log in to your GitHub account. Click Use this template at the top of the page and choose Create new repository . Click Create repository from template when you\u2019re done setting the options for your new repository. Click Code (at the top of the new repository\u2019s page). Under the Codespaces tab,  choose Create codespace on main . Depending on how you've configured your computer's settings, this either opens a new browser tab with the Codespace development environment with VSCode running in it or opens a new VSCode window with the codespace in it. Wait for the codespace to finish building by waiting for the postCreateCommand command to complete; this can take several minutes: Wait for postCreateCommand to complete When this command completes, you can start using the codespace development environment. The terminal the command ran in will close and you will get a prompt in a brand new terminal. At the terminal's prompt, you can execute any dbt command you want. For example: /workspaces/test ( main ) $ dbt build You can also use the duckcli to write SQL against the warehouse from the command line or build reports in the Evidence project provided in the reports directory. For complete information, refer to the dbt command reference . Common commands are: dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project Generate a larger data set \u200b If you'd like to work with a larger selection of Jaffle Shop data, you can generate an arbitrary number of years of fictitious data from within your codespace. Install the Python package called jafgen . At the terminal's prompt, run: python -m pip install jafgen When installation is done, run: jafgen [ number of years to generate ] # e.g. jafgen 6 Replace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter. As you increase the number of years, it takes exponentially more time to generate the data because the Jaffle Shop stores grow in size and number. For a good balance of data size and time to build, dbt Labs suggests a maximum of 6 years. Next steps \u200b Now that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nrequirements.txt\ndbt_project.yml\n/my_dbt_project/ \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 models/ \u2502   \u251c\u2500\u2500 my_model.sql \u251c\u2500\u2500 tests/ \u2502   \u251c\u2500\u2500 my_test.sql \u2514\u2500\u2500 requirements.txt\n/my_dbt_project/ \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 models/ \u2502   \u251c\u2500\u2500 my_model.sql \u251c\u2500\u2500 tests/ \u2502   \u251c\u2500\u2500 my_test.sql \u2514\u2500\u2500 requirements.txt\nFor more information, refer to the DuckDB setup . Local Web browser First, clone the Jaffle Shop git repository by running the following command in your terminal: git clone https://github.com/dbt-labs/jaffle_shop_duckdb.git Change into the docs-duckdb directory from the command line: cd jaffle_shop_duckdb Install dbt Core and DuckDB in a virtual environment. Example for Mac python3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate Example for Windows python -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat Example for Windows PowerShell python -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1 Ensure your profile is setup correctly from the command line by running the following dbt commands . dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project dbt docs generate \u2014 generates your project's documentation. dbt docs serve \u2014 starts a webserver on port 8080 to serve your documentation locally and opens the documentation site in your default browser. For complete details, refer to the dbt command reference . Here's what a successful output will look like: (venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28 To query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data). Go to the jaffle-shop-template repository after you log in to your GitHub account. Click Use this template at the top of the page and choose Create new repository . Click Create repository from template when you\u2019re done setting the options for your new repository. Click Code (at the top of the new repository\u2019s page). Under the Codespaces tab,  choose Create codespace on main . Depending on how you've configured your computer's settings, this either opens a new browser tab with the Codespace development environment with VSCode running in it or opens a new VSCode window with the codespace in it. Wait for the codespace to finish building by waiting for the postCreateCommand command to complete; this can take several minutes: Wait for postCreateCommand to complete When this command completes, you can start using the codespace development environment. The terminal the command ran in will close and you will get a prompt in a brand new terminal. At the terminal's prompt, you can execute any dbt command you want. For example: /workspaces/test ( main ) $ dbt build You can also use the duckcli to write SQL against the warehouse from the command line or build reports in the Evidence project provided in the reports directory. For complete information, refer to the dbt command reference . Common commands are: dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project Generate a larger data set \u200b If you'd like to work with a larger selection of Jaffle Shop data, you can generate an arbitrary number of years of fictitious data from within your codespace. Install the Python package called jafgen . At the terminal's prompt, run: python -m pip install jafgen When installation is done, run: jafgen [ number of years to generate ] # e.g. jafgen 6 Replace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter. As you increase the number of years, it takes exponentially more time to generate the data because the Jaffle Shop stores grow in size and number. For a good balance of data size and time to build, dbt Labs suggests a maximum of 6 years. Next steps \u200b Now that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nLocal Web browser\nWeb browser\nFirst, clone the Jaffle Shop git repository by running the following command in your terminal: git clone https://github.com/dbt-labs/jaffle_shop_duckdb.git\nFirst, clone the Jaffle Shop git repository by running the following command in your terminal:\ngit clone https://github.com/dbt-labs/jaffle_shop_duckdb.git\ngit clone https://github.com/dbt-labs/jaffle_shop_duckdb.git\nChange into the docs-duckdb directory from the command line: cd jaffle_shop_duckdb\nChange into the docs-duckdb directory from the command line:\ncd jaffle_shop_duckdb\ncd jaffle_shop_duckdb\nInstall dbt Core and DuckDB in a virtual environment. Example for Mac python3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate Example for Windows python -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat Example for Windows PowerShell python -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1\nInstall dbt Core and DuckDB in a virtual environment.\npython3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate\npython3 -m venv venv source venv/bin/activate python3 -m pip install --upgrade pip python3 -m pip install -r requirements.txt source venv/bin/activate\npython -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat\npython -m venv venv venv \\ Scripts \\ activate.bat python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ activate.bat\npython -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1\npython -m venv venv venv \\ Scripts \\ Activate.ps1 python -m pip install --upgrade pip python -m pip install -r requirements.txt venv \\ Scripts \\ Activate.ps1\nEnsure your profile is setup correctly from the command line by running the following dbt commands . dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project dbt docs generate \u2014 generates your project's documentation. dbt docs serve \u2014 starts a webserver on port 8080 to serve your documentation locally and opens the documentation site in your default browser.\nEnsure your profile is setup correctly from the command line by running the following dbt commands .\ndbt compile \u2014 generates executable SQL from your project source files\ndbt run \u2014 compiles and runs your project\ndbt test \u2014 compiles and tests your project\ndbt build \u2014 compiles, runs, and tests your project\ndbt docs generate \u2014 generates your project's documentation.\ndbt docs serve \u2014 starts a webserver on port 8080 to serve your documentation locally and opens the documentation site in your default browser.\nFor complete details, refer to the dbt command reference . Here's what a successful output will look like: (venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28 To query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data).\nHere's what a successful output will look like: (venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28 To query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data).\n(venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28\n(venv) \u279c  jaffle_shop_duckdb git:(duckdb) dbt build 15:10:12  Running with dbt=1.8.1 15:10:13  Registered adapter: duckdb=1.8.1 15:10:13  Found 5 models, 3 seeds, 20 data tests, 416 macros 15:10:13 15:10:14  Concurrency: 24 threads (target='dev') 15:10:14 15:10:14  1 of 28 START seed file main.raw_customers ..................................... [RUN] 15:10:14  2 of 28 START seed file main.raw_orders ........................................ [RUN] 15:10:14  3 of 28 START seed file main.raw_payments ...................................... [RUN] .... 15:10:15  27 of 28 PASS relationships_orders_customer_id__customer_id__ref_customers_ .... [PASS in 0.32s] 15:10:15 15:10:15  Finished running 3 seeds, 3 view models, 20 data tests, 2 table models in 0 hours 0 minutes and 1.52 seconds (1.52s). 15:10:15 15:10:15  Completed successfully 15:10:15 15:10:15  Done. PASS=28 WARN=0 ERROR=0 SKIP=0 TOTAL=28\nTo query data, some useful commands you can run from the command line: dbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal. dbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is. note The steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter. Troubleshoot \u200b Could not set lock on file error IO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable This is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data).\ndbt show --select \"raw_orders\" \u2014 run a query against the data warehouse and preview the results in the terminal.\ndbt show --select \"raw_orders\"\ndbt source \u2014 provides subcommands such as dbt source freshness that are useful when working with source data. dbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is.\ndbt source\ndbt source freshness\ndbt source freshness \u2014 checks the freshness\u00a0(how up to date) a specific source table is.\ndbt source freshness\nThe steps will fail if you decide to run this project in your data warehouse (outside of this DuckDB demo). You will need to reconfigure the project files for your warehouse. Definitely consider this if you are using a community-contributed adapter.\nTroubleshoot \u200b\nIO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable\nIO Error: Could not set lock on file \"jaffle_shop.duckdb\": Resource temporarily unavailable\nThis is a known issue in DuckDB. Try disconnecting from any sessions that are locking the database. If you are using DBeaver, this means shutting down DBeaver (disconnecting doesn't always work). As a last resort, deleting the database file will get you back in action ( but you will lose all your data).\nAs a last resort, deleting the database file will get you back in action ( but you will lose all your data).\nGo to the jaffle-shop-template repository after you log in to your GitHub account.\nGo to the jaffle-shop-template repository after you log in to your GitHub account.\njaffle-shop-template\nClick Use this template at the top of the page and choose Create new repository .\nClick Use this template at the top of the page and choose Create new repository .\nClick Create repository from template when you\u2019re done setting the options for your new repository.\nClick Create repository from template when you\u2019re done setting the options for your new repository.\nClick Code (at the top of the new repository\u2019s page). Under the Codespaces tab,  choose Create codespace on main . Depending on how you've configured your computer's settings, this either opens a new browser tab with the Codespace development environment with VSCode running in it or opens a new VSCode window with the codespace in it.\nClick Code (at the top of the new repository\u2019s page). Under the Codespaces tab,  choose Create codespace on main . Depending on how you've configured your computer's settings, this either opens a new browser tab with the Codespace development environment with VSCode running in it or opens a new VSCode window with the codespace in it.\nWait for the codespace to finish building by waiting for the postCreateCommand command to complete; this can take several minutes: Wait for postCreateCommand to complete When this command completes, you can start using the codespace development environment. The terminal the command ran in will close and you will get a prompt in a brand new terminal.\nWait for the codespace to finish building by waiting for the postCreateCommand command to complete; this can take several minutes:\npostCreateCommand\nWhen this command completes, you can start using the codespace development environment. The terminal the command ran in will close and you will get a prompt in a brand new terminal.\nAt the terminal's prompt, you can execute any dbt command you want. For example: /workspaces/test ( main ) $ dbt build You can also use the duckcli to write SQL against the warehouse from the command line or build reports in the Evidence project provided in the reports directory. For complete information, refer to the dbt command reference . Common commands are: dbt compile \u2014 generates executable SQL from your project source files dbt run \u2014 compiles and runs your project dbt test \u2014 compiles and tests your project dbt build \u2014 compiles, runs, and tests your project\nAt the terminal's prompt, you can execute any dbt command you want. For example:\n/workspaces/test ( main ) $ dbt build\n/workspaces/test ( main ) $ dbt build\nYou can also use the duckcli to write SQL against the warehouse from the command line or build reports in the Evidence project provided in the reports directory.\nreports\nFor complete information, refer to the dbt command reference . Common commands are:\ndbt compile \u2014 generates executable SQL from your project source files\ndbt run \u2014 compiles and runs your project\ndbt test \u2014 compiles and tests your project\ndbt build \u2014 compiles, runs, and tests your project\nGenerate a larger data set \u200b\nIf you'd like to work with a larger selection of Jaffle Shop data, you can generate an arbitrary number of years of fictitious data from within your codespace. Install the Python package called jafgen . At the terminal's prompt, run: python -m pip install jafgen When installation is done, run: jafgen [ number of years to generate ] # e.g. jafgen 6 Replace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter. As you increase the number of years, it takes exponentially more time to generate the data because the Jaffle Shop stores grow in size and number. For a good balance of data size and time to build, dbt Labs suggests a maximum of 6 years. Next steps \u200b Now that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nInstall the Python package called jafgen . At the terminal's prompt, run: python -m pip install jafgen\nInstall the Python package called jafgen . At the terminal's prompt, run:\npython -m pip install jafgen\npython -m pip install jafgen\nWhen installation is done, run: jafgen [ number of years to generate ] # e.g. jafgen 6 Replace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter.\nWhen installation is done, run:\njafgen [ number of years to generate ] # e.g. jafgen 6\njafgen [ number of years to generate ] # e.g. jafgen 6\nReplace NUMBER_OF_YEARS with the number of years you want to simulate. For example, to generate data for 6 years, you would run: jafgen --years 6 . This command builds the CSV files and stores them in the jaffle-data folder, and is automatically sourced based on the sources.yml file and the dbt-duckdb adapter.\nNUMBER_OF_YEARS\njafgen --years 6\njaffle-data\nsources.yml\nAs you increase the number of years, it takes exponentially more time to generate the data because the Jaffle Shop stores grow in size and number. For a good balance of data size and time to build, dbt Labs suggests a maximum of 6 years. Next steps \u200b Now that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nNext steps \u200b\nNow that you have dbt Core , DuckDB, and the Jaffle Shop data up and running, you can explore dbt's capabilities. Refer to these materials to get a better understanding of dbt projects and commands: The About projects page guides you through the structure of a dbt project and its components. dbt command reference explains the various commands available and what they do. dbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert. Once you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today! Check out the other quickstart guides to begin integrating into your existing data warehouse. Additionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nThe About projects page guides you through the structure of a dbt project and its components.\ndbt command reference explains the various commands available and what they do.\ndbt Labs courses offer a variety of beginner, intermediate, and advanced learning modules designed to help you become a dbt expert.\nOnce you see the potential of dbt and what it can do for your organization, sign up for a free trial of dbt . It's the fastest and easiest way to deploy dbt today!\nCheck out the other quickstart guides to begin integrating into your existing data warehouse.\nAdditionally, with your new understanding of the basics of using DuckDB, consider optimizing your setup by documenting your project , commit your changes and, schedule a job . Document your project \u200b To document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nDocument your project \u200b\nTo document your dbt projects with DuckDB, follow the steps: Use the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files Run the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser. Enhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files. Commit your changes \u200b Commit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nUse the dbt docs generate command to compile information about your dbt project and warehouse into manifest.json and catalog.json files\ndbt docs generate\nmanifest.json\ncatalog.json\nRun the dbt docs serve command to create a local website using the generated .json files. This allows you to view your project's documentation in a web browser.\ndbt docs serve\n.json\nEnhance your documentation by adding descriptions to models, columns, and sources using the description key in your YAML files.\ndescription\nCommit your changes \u200b\nCommit your changes to ensure the repository is up to date with the latest code. In the GitHub repository you created for your project, run the following commands in the terminal: git add git commit -m \"Your commit message\" git push Go back to your GitHub repository to verify your new files have been added. Schedule a job \u200b Ensure dbt Core is installed and configured to connect to your DuckDB instance. Create a dbt project and define your models , seeds , and tests . Use a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals. Write a script that runs your dbt commands, such as dbt run , dbt test and more so. Use your chosen scheduler to run the script at your desired frequency. Congratulations on making it through the guide \ud83c\udf89!\nIn the GitHub repository you created for your project, run the following commands in the terminal:\ngit add git commit -m \"Your commit message\" git push\ngit add git commit -m \"Your commit message\" git push\nGo back to your GitHub repository to verify your new files have been added.\nSchedule a job \u200b\nEnsure dbt Core is installed and configured to connect to your DuckDB instance.\nCreate a dbt project and define your models , seeds , and tests .\nmodels\nseeds\ntests\nUse a scheduler such Prefect to schedule your dbt runs. You can create a DAG (Directed Acyclic Graph) that triggers dbt commands at specified intervals.\nWrite a script that runs your dbt commands, such as dbt run , dbt test and more so.\ndbt run\ndbt test\nUse your chosen scheduler to run the script at your desired frequency.\nCongratulations on making it through the guide \ud83c\udf89!"
  }
]